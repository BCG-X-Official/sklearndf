
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>sklearn.preprocessing._data &#8212; sklearndf  documentation</title>
    
  <link rel="stylesheet" href="../../../_static/css/index.f658d18f9b420779cfdf24aa0a7e2d77.css">

    
  <link rel="stylesheet"
    href="../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../../../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../../../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/basic.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/gamma.css" />
    
  <link rel="preload" as="script" href="../../../_static/js/index.d3f166471bb80abb5163.js">

    <script id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/js/gamma.js"></script>
    <script src="../../../_static/js/versions.js"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main">
<div class="container-xl">

    <a class="navbar-brand" href="../../../index.html">
    
      <img src="../../../_static/gamma_sklearndf_logo.png" class="logo" alt="logo" />
    
    </a>
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-menu" aria-controls="navbar-menu" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar-menu" class="col-lg-9 collapse navbar-collapse">
      <ul id="navbar-main-elements" class="navbar-nav mr-auto">
        
        
        <li class="nav-item ">
            <a class="nav-link" href="../../../getting_started/getting_started.html">Getting started</a>
        </li>
        
        <li class="nav-item ">
            <a class="nav-link" href="../../../apidoc/sklearndf.html">API reference</a>
        </li>
        
        <li class="nav-item ">
            <a class="nav-link" href="../../../tutorials.html">Tutorials</a>
        </li>
        
        <li class="nav-item ">
            <a class="nav-link" href="../../../contribution_guide.html">Development Guidelines</a>
        </li>
        
        <li class="nav-item ">
            <a class="nav-link" href="../../../faqs.html">FAQ</a>
        </li>
        
        <li class="nav-item ">
            <a class="nav-link" href="../../../release_notes.html">Release Notes</a>
        </li>
        
        
      </ul>


      

      <ul class="navbar-nav">
        
        
      </ul>
    </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
          <div class="col-12 col-md-3 bd-sidebar"><form class="bd-search d-flex align-items-center" action="../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">

    <div class="bd-toc-item active">
    
  
    <ul class="nav bd-sidenav">
        
        
        
        
        
        
        
        
        
        
        
        
        
      </ul>
  
  </nav>
          </div>
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
              

<nav id="bd-toc-nav">
    <ul class="nav section-nav flex-column">
    
    </ul>
</nav>


              
          </div>
          

          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <h1>Source code for sklearn.preprocessing._data</h1><div class="highlight"><pre>
<span></span><span class="c1"># Authors: Alexandre Gramfort &lt;alexandre.gramfort@inria.fr&gt;</span>
<span class="c1">#          Mathieu Blondel &lt;mathieu@mblondel.org&gt;</span>
<span class="c1">#          Olivier Grisel &lt;olivier.grisel@ensta.org&gt;</span>
<span class="c1">#          Andreas Mueller &lt;amueller@ais.uni-bonn.de&gt;</span>
<span class="c1">#          Eric Martin &lt;eric@ericmart.in&gt;</span>
<span class="c1">#          Giorgio Patrini &lt;giorgio.patrini@anu.edu.au&gt;</span>
<span class="c1">#          Eric Chang &lt;ericchang2017@u.northwestern.edu&gt;</span>
<span class="c1"># License: BSD 3 clause</span>


<span class="kn">from</span> <span class="nn">itertools</span> <span class="kn">import</span> <span class="n">chain</span><span class="p">,</span> <span class="n">combinations</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">from</span> <span class="nn">itertools</span> <span class="kn">import</span> <span class="n">combinations_with_replacement</span> <span class="k">as</span> <span class="n">combinations_w_r</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">sparse</span>
<span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">stats</span>
<span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">optimize</span>
<span class="kn">from</span> <span class="nn">scipy.special</span> <span class="kn">import</span> <span class="n">boxcox</span>

<span class="kn">from</span> <span class="nn">..base</span> <span class="kn">import</span> <span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">TransformerMixin</span>
<span class="kn">from</span> <span class="nn">..utils</span> <span class="kn">import</span> <span class="n">check_array</span>
<span class="kn">from</span> <span class="nn">..utils.deprecation</span> <span class="kn">import</span> <span class="n">deprecated</span>
<span class="kn">from</span> <span class="nn">..utils.extmath</span> <span class="kn">import</span> <span class="n">row_norms</span>
<span class="kn">from</span> <span class="nn">..utils.extmath</span> <span class="kn">import</span> <span class="p">(</span><span class="n">_incremental_mean_and_var</span><span class="p">,</span>
                             <span class="n">_incremental_weighted_mean_and_var</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">..utils.sparsefuncs_fast</span> <span class="kn">import</span> <span class="p">(</span><span class="n">inplace_csr_row_normalize_l1</span><span class="p">,</span>
                                      <span class="n">inplace_csr_row_normalize_l2</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">..utils.sparsefuncs</span> <span class="kn">import</span> <span class="p">(</span><span class="n">inplace_column_scale</span><span class="p">,</span>
                                 <span class="n">mean_variance_axis</span><span class="p">,</span> <span class="n">incr_mean_variance_axis</span><span class="p">,</span>
                                 <span class="n">min_max_axis</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">..utils.validation</span> <span class="kn">import</span> <span class="p">(</span><span class="n">check_is_fitted</span><span class="p">,</span> <span class="n">check_random_state</span><span class="p">,</span>
                                <span class="n">_check_sample_weight</span><span class="p">,</span>
                                <span class="n">FLOAT_DTYPES</span><span class="p">,</span> <span class="n">_deprecate_positional_args</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">._csr_polynomial_expansion</span> <span class="kn">import</span> <span class="n">_csr_polynomial_expansion</span>

<span class="kn">from</span> <span class="nn">._encoders</span> <span class="kn">import</span> <span class="n">OneHotEncoder</span>

<span class="n">BOUNDS_THRESHOLD</span> <span class="o">=</span> <span class="mf">1e-7</span>

<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s1">&#39;Binarizer&#39;</span><span class="p">,</span>
    <span class="s1">&#39;KernelCenterer&#39;</span><span class="p">,</span>
    <span class="s1">&#39;MinMaxScaler&#39;</span><span class="p">,</span>
    <span class="s1">&#39;MaxAbsScaler&#39;</span><span class="p">,</span>
    <span class="s1">&#39;Normalizer&#39;</span><span class="p">,</span>
    <span class="s1">&#39;OneHotEncoder&#39;</span><span class="p">,</span>
    <span class="s1">&#39;RobustScaler&#39;</span><span class="p">,</span>
    <span class="s1">&#39;StandardScaler&#39;</span><span class="p">,</span>
    <span class="s1">&#39;QuantileTransformer&#39;</span><span class="p">,</span>
    <span class="s1">&#39;PowerTransformer&#39;</span><span class="p">,</span>
    <span class="s1">&#39;add_dummy_feature&#39;</span><span class="p">,</span>
    <span class="s1">&#39;binarize&#39;</span><span class="p">,</span>
    <span class="s1">&#39;normalize&#39;</span><span class="p">,</span>
    <span class="s1">&#39;scale&#39;</span><span class="p">,</span>
    <span class="s1">&#39;robust_scale&#39;</span><span class="p">,</span>
    <span class="s1">&#39;maxabs_scale&#39;</span><span class="p">,</span>
    <span class="s1">&#39;minmax_scale&#39;</span><span class="p">,</span>
    <span class="s1">&#39;quantile_transform&#39;</span><span class="p">,</span>
    <span class="s1">&#39;power_transform&#39;</span><span class="p">,</span>
<span class="p">]</span>


<span class="k">def</span> <span class="nf">_handle_zeros_in_scale</span><span class="p">(</span><span class="n">scale</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Makes sure that whenever scale is zero, we handle it correctly.</span>

<span class="sd">    This happens in most scalers when we have constant features.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># if we are fitting on 1D arrays, scale might be a scalar</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">isscalar</span><span class="p">(</span><span class="n">scale</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">scale</span> <span class="o">==</span> <span class="mf">.0</span><span class="p">:</span>
            <span class="n">scale</span> <span class="o">=</span> <span class="mf">1.</span>
        <span class="k">return</span> <span class="n">scale</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">scale</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">copy</span><span class="p">:</span>
            <span class="c1"># New array to avoid side-effects</span>
            <span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="n">scale</span><span class="p">[</span><span class="n">scale</span> <span class="o">==</span> <span class="mf">0.0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span>
        <span class="k">return</span> <span class="n">scale</span>


<span class="nd">@_deprecate_positional_args</span>
<span class="k">def</span> <span class="nf">scale</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">with_mean</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">with_std</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Standardize a dataset along any axis.</span>

<span class="sd">    Center to the mean and component wise scale to unit variance.</span>

<span class="sd">    Read more in the :ref:`User Guide &lt;preprocessing_scaler&gt;`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">        The data to center and scale.</span>

<span class="sd">    axis : int, default=0</span>
<span class="sd">        axis used to compute the means and standard deviations along. If 0,</span>
<span class="sd">        independently standardize each feature, otherwise (if 1) standardize</span>
<span class="sd">        each sample.</span>

<span class="sd">    with_mean : bool, default=True</span>
<span class="sd">        If True, center the data before scaling.</span>

<span class="sd">    with_std : bool, default=True</span>
<span class="sd">        If True, scale the data to unit variance (or equivalently,</span>
<span class="sd">        unit standard deviation).</span>

<span class="sd">    copy : bool, default=True</span>
<span class="sd">        set to False to perform inplace row normalization and avoid a</span>
<span class="sd">        copy (if the input is already a numpy array or a scipy.sparse</span>
<span class="sd">        CSC matrix and if axis is 1).</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    X_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">        The transformed data.</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    This implementation will refuse to center scipy.sparse matrices</span>
<span class="sd">    since it would make them non-sparse and would potentially crash the</span>
<span class="sd">    program with memory exhaustion problems.</span>

<span class="sd">    Instead the caller is expected to either set explicitly</span>
<span class="sd">    `with_mean=False` (in that case, only variance scaling will be</span>
<span class="sd">    performed on the features of the CSC matrix) or to call `X.toarray()`</span>
<span class="sd">    if he/she expects the materialized dense array to fit in memory.</span>

<span class="sd">    To avoid memory copy the caller should pass a CSC matrix.</span>

<span class="sd">    NaNs are treated as missing values: disregarded to compute the statistics,</span>
<span class="sd">    and maintained during the data transformation.</span>

<span class="sd">    We use a biased estimator for the standard deviation, equivalent to</span>
<span class="sd">    `numpy.std(x, ddof=0)`. Note that the choice of `ddof` is unlikely to</span>
<span class="sd">    affect model performance.</span>

<span class="sd">    For a comparison of the different scalers, transformers, and normalizers,</span>
<span class="sd">    see :ref:`examples/preprocessing/plot_all_scaling.py</span>
<span class="sd">    &lt;sphx_glr_auto_examples_preprocessing_plot_all_scaling.py&gt;`.</span>

<span class="sd">    .. warning:: Risk of data leak</span>

<span class="sd">        Do not use :func:`~sklearn.preprocessing.scale` unless you know</span>
<span class="sd">        what you are doing. A common mistake is to apply it to the entire data</span>
<span class="sd">        *before* splitting into training and test sets. This will bias the</span>
<span class="sd">        model evaluation because information would have leaked from the test</span>
<span class="sd">        set to the training set.</span>
<span class="sd">        In general, we recommend using</span>
<span class="sd">        :class:`~sklearn.preprocessing.StandardScaler` within a</span>
<span class="sd">        :ref:`Pipeline &lt;pipeline&gt;` in order to prevent most risks of data</span>
<span class="sd">        leaking: `pipe = make_pipeline(StandardScaler(), LogisticRegression())`.</span>

<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    StandardScaler : Performs scaling to unit variance using the Transformer</span>
<span class="sd">        API (e.g. as part of a preprocessing</span>
<span class="sd">        :class:`~sklearn.pipeline.Pipeline`).</span>

<span class="sd">    &quot;&quot;&quot;</span>  <span class="c1"># noqa</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">accept_sparse</span><span class="o">=</span><span class="s1">&#39;csc&#39;</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="n">copy</span><span class="p">,</span> <span class="n">ensure_2d</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                    <span class="n">estimator</span><span class="o">=</span><span class="s1">&#39;the scale function&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">FLOAT_DTYPES</span><span class="p">,</span>
                    <span class="n">force_all_finite</span><span class="o">=</span><span class="s1">&#39;allow-nan&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">sparse</span><span class="o">.</span><span class="n">issparse</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">with_mean</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Cannot center sparse matrices: pass `with_mean=False` instead&quot;</span>
                <span class="s2">&quot; See docstring for motivation and alternatives.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">axis</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Can only scale sparse matrix on axis=0, &quot;</span>
                             <span class="s2">&quot; got axis=</span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">axis</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">with_std</span><span class="p">:</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">var</span> <span class="o">=</span> <span class="n">mean_variance_axis</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">var</span> <span class="o">=</span> <span class="n">_handle_zeros_in_scale</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="n">inplace_column_scale</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">var</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">with_mean</span><span class="p">:</span>
            <span class="n">mean_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">with_std</span><span class="p">:</span>
            <span class="n">scale_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nanstd</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>
        <span class="c1"># Xr is a view on the original array that enables easy use of</span>
        <span class="c1"># broadcasting on the axis in which we are interested in</span>
        <span class="n">Xr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">rollaxis</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">with_mean</span><span class="p">:</span>
            <span class="n">Xr</span> <span class="o">-=</span> <span class="n">mean_</span>
            <span class="n">mean_1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">Xr</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="c1"># Verify that mean_1 is &#39;close to zero&#39;. If X contains very</span>
            <span class="c1"># large values, mean_1 can also be very large, due to a lack of</span>
            <span class="c1"># precision of mean_. In this case, a pre-scaling of the</span>
            <span class="c1"># concerned feature is efficient, for instance by its mean or</span>
            <span class="c1"># maximum.</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">mean_1</span><span class="p">,</span> <span class="mi">0</span><span class="p">):</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Numerical issues were encountered &quot;</span>
                              <span class="s2">&quot;when centering the data &quot;</span>
                              <span class="s2">&quot;and might not be solved. Dataset may &quot;</span>
                              <span class="s2">&quot;contain too large values. You may need &quot;</span>
                              <span class="s2">&quot;to prescale your features.&quot;</span><span class="p">)</span>
                <span class="n">Xr</span> <span class="o">-=</span> <span class="n">mean_1</span>
        <span class="k">if</span> <span class="n">with_std</span><span class="p">:</span>
            <span class="n">scale_</span> <span class="o">=</span> <span class="n">_handle_zeros_in_scale</span><span class="p">(</span><span class="n">scale_</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="n">Xr</span> <span class="o">/=</span> <span class="n">scale_</span>
            <span class="k">if</span> <span class="n">with_mean</span><span class="p">:</span>
                <span class="n">mean_2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">Xr</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
                <span class="c1"># If mean_2 is not &#39;close to zero&#39;, it comes from the fact that</span>
                <span class="c1"># scale_ is very small so that mean_2 = mean_1/scale_ &gt; 0, even</span>
                <span class="c1"># if mean_1 was close to zero. The problem is thus essentially</span>
                <span class="c1"># due to the lack of precision of mean_. A solution is then to</span>
                <span class="c1"># subtract the mean again:</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">mean_2</span><span class="p">,</span> <span class="mi">0</span><span class="p">):</span>
                    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Numerical issues were encountered &quot;</span>
                                  <span class="s2">&quot;when scaling the data &quot;</span>
                                  <span class="s2">&quot;and might not be solved. The standard &quot;</span>
                                  <span class="s2">&quot;deviation of the data is probably &quot;</span>
                                  <span class="s2">&quot;very close to 0. &quot;</span><span class="p">)</span>
                    <span class="n">Xr</span> <span class="o">-=</span> <span class="n">mean_2</span>
    <span class="k">return</span> <span class="n">X</span>


<span class="k">class</span> <span class="nc">MinMaxScaler</span><span class="p">(</span><span class="n">TransformerMixin</span><span class="p">,</span> <span class="n">BaseEstimator</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Transform features by scaling each feature to a given range.</span>

<span class="sd">    This estimator scales and translates each feature individually such</span>
<span class="sd">    that it is in the given range on the training set, e.g. between</span>
<span class="sd">    zero and one.</span>

<span class="sd">    The transformation is given by::</span>

<span class="sd">        X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))</span>
<span class="sd">        X_scaled = X_std * (max - min) + min</span>

<span class="sd">    where min, max = feature_range.</span>

<span class="sd">    This transformation is often used as an alternative to zero mean,</span>
<span class="sd">    unit variance scaling.</span>

<span class="sd">    Read more in the :ref:`User Guide &lt;preprocessing_scaler&gt;`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    feature_range : tuple (min, max), default=(0, 1)</span>
<span class="sd">        Desired range of transformed data.</span>

<span class="sd">    copy : bool, default=True</span>
<span class="sd">        Set to False to perform inplace row normalization and avoid a</span>
<span class="sd">        copy (if the input is already a numpy array).</span>

<span class="sd">    clip : bool, default=False</span>
<span class="sd">        Set to True to clip transformed values of held-out data to</span>
<span class="sd">        provided `feature range`.</span>

<span class="sd">        .. versionadded:: 0.24</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    min_ : ndarray of shape (n_features,)</span>
<span class="sd">        Per feature adjustment for minimum. Equivalent to</span>
<span class="sd">        ``min - X.min(axis=0) * self.scale_``</span>

<span class="sd">    scale_ : ndarray of shape (n_features,)</span>
<span class="sd">        Per feature relative scaling of the data. Equivalent to</span>
<span class="sd">        ``(max - min) / (X.max(axis=0) - X.min(axis=0))``</span>

<span class="sd">        .. versionadded:: 0.17</span>
<span class="sd">           *scale_* attribute.</span>

<span class="sd">    data_min_ : ndarray of shape (n_features,)</span>
<span class="sd">        Per feature minimum seen in the data</span>

<span class="sd">        .. versionadded:: 0.17</span>
<span class="sd">           *data_min_*</span>

<span class="sd">    data_max_ : ndarray of shape (n_features,)</span>
<span class="sd">        Per feature maximum seen in the data</span>

<span class="sd">        .. versionadded:: 0.17</span>
<span class="sd">           *data_max_*</span>

<span class="sd">    data_range_ : ndarray of shape (n_features,)</span>
<span class="sd">        Per feature range ``(data_max_ - data_min_)`` seen in the data</span>

<span class="sd">        .. versionadded:: 0.17</span>
<span class="sd">           *data_range_*</span>

<span class="sd">    n_samples_seen_ : int</span>
<span class="sd">        The number of samples processed by the estimator.</span>
<span class="sd">        It will be reset on new calls to fit, but increments across</span>
<span class="sd">        ``partial_fit`` calls.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.preprocessing import MinMaxScaler</span>
<span class="sd">    &gt;&gt;&gt; data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]</span>
<span class="sd">    &gt;&gt;&gt; scaler = MinMaxScaler()</span>
<span class="sd">    &gt;&gt;&gt; print(scaler.fit(data))</span>
<span class="sd">    MinMaxScaler()</span>
<span class="sd">    &gt;&gt;&gt; print(scaler.data_max_)</span>
<span class="sd">    [ 1. 18.]</span>
<span class="sd">    &gt;&gt;&gt; print(scaler.transform(data))</span>
<span class="sd">    [[0.   0.  ]</span>
<span class="sd">     [0.25 0.25]</span>
<span class="sd">     [0.5  0.5 ]</span>
<span class="sd">     [1.   1.  ]]</span>
<span class="sd">    &gt;&gt;&gt; print(scaler.transform([[2, 2]]))</span>
<span class="sd">    [[1.5 0. ]]</span>

<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    minmax_scale : Equivalent function without the estimator API.</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    NaNs are treated as missing values: disregarded in fit, and maintained in</span>
<span class="sd">    transform.</span>

<span class="sd">    For a comparison of the different scalers, transformers, and normalizers,</span>
<span class="sd">    see :ref:`examples/preprocessing/plot_all_scaling.py</span>
<span class="sd">    &lt;sphx_glr_auto_examples_preprocessing_plot_all_scaling.py&gt;`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@_deprecate_positional_args</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">feature_range</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="o">*</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">clip</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">feature_range</span> <span class="o">=</span> <span class="n">feature_range</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">copy</span> <span class="o">=</span> <span class="n">copy</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">clip</span> <span class="o">=</span> <span class="n">clip</span>

    <span class="k">def</span> <span class="nf">_reset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Reset internal data-dependent state of the scaler, if necessary.</span>

<span class="sd">        __init__ parameters are not touched.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Checking one attribute is enough, becase they are all set together</span>
        <span class="c1"># in partial_fit</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;scale_&#39;</span><span class="p">):</span>
            <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale_</span>
            <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_</span>
            <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_samples_seen_</span>
            <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">data_min_</span>
            <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">data_max_</span>
            <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">data_range_</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Compute the minimum and maximum to be used for later scaling.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like of shape (n_samples, n_features)</span>
<span class="sd">            The data used to compute the per-feature minimum and maximum</span>
<span class="sd">            used for later scaling along the features axis.</span>

<span class="sd">        y : None</span>
<span class="sd">            Ignored.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        self : object</span>
<span class="sd">            Fitted scaler.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Reset internal state before fitting</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_reset</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">partial_fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">partial_fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Online computation of min and max on X for later scaling.</span>

<span class="sd">        All of X is processed as a single batch. This is intended for cases</span>
<span class="sd">        when :meth:`fit` is not feasible due to very large number of</span>
<span class="sd">        `n_samples` or because X is read from a continuous stream.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like of shape (n_samples, n_features)</span>
<span class="sd">            The data used to compute the mean and standard deviation</span>
<span class="sd">            used for later scaling along the features axis.</span>

<span class="sd">        y : None</span>
<span class="sd">            Ignored.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        self : object</span>
<span class="sd">            Fitted scaler.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">feature_range</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">feature_range</span>
        <span class="k">if</span> <span class="n">feature_range</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">feature_range</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Minimum of desired feature range must be smaller&quot;</span>
                             <span class="s2">&quot; than maximum. Got </span><span class="si">%s</span><span class="s2">.&quot;</span> <span class="o">%</span> <span class="nb">str</span><span class="p">(</span><span class="n">feature_range</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">sparse</span><span class="o">.</span><span class="n">issparse</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;MinMaxScaler does not support sparse input. &quot;</span>
                            <span class="s2">&quot;Consider using MaxAbsScaler instead.&quot;</span><span class="p">)</span>

        <span class="n">first_pass</span> <span class="o">=</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;n_samples_seen_&#39;</span><span class="p">)</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_data</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">reset</span><span class="o">=</span><span class="n">first_pass</span><span class="p">,</span>
                                <span class="n">estimator</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">FLOAT_DTYPES</span><span class="p">,</span>
                                <span class="n">force_all_finite</span><span class="o">=</span><span class="s2">&quot;allow-nan&quot;</span><span class="p">)</span>

        <span class="n">data_min</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nanmin</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">data_max</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nanmax</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">first_pass</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">n_samples_seen_</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">data_min</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data_min_</span><span class="p">,</span> <span class="n">data_min</span><span class="p">)</span>
            <span class="n">data_max</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data_max_</span><span class="p">,</span> <span class="n">data_max</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">n_samples_seen_</span> <span class="o">+=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="n">data_range</span> <span class="o">=</span> <span class="n">data_max</span> <span class="o">-</span> <span class="n">data_min</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scale_</span> <span class="o">=</span> <span class="p">((</span><span class="n">feature_range</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">feature_range</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">/</span>
                       <span class="n">_handle_zeros_in_scale</span><span class="p">(</span><span class="n">data_range</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_</span> <span class="o">=</span> <span class="n">feature_range</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">data_min</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale_</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data_min_</span> <span class="o">=</span> <span class="n">data_min</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data_max_</span> <span class="o">=</span> <span class="n">data_max</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data_range_</span> <span class="o">=</span> <span class="n">data_range</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Scale features of X according to feature_range.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like of shape (n_samples, n_features)</span>
<span class="sd">            Input data that will be transformed.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        Xt : ndarray of shape (n_samples, n_features)</span>
<span class="sd">            Transformed data.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_data</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">copy</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">FLOAT_DTYPES</span><span class="p">,</span>
                                <span class="n">force_all_finite</span><span class="o">=</span><span class="s2">&quot;allow-nan&quot;</span><span class="p">,</span> <span class="n">reset</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="n">X</span> <span class="o">*=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale_</span>
        <span class="n">X</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">clip</span><span class="p">:</span>
            <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">feature_range</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">feature_range</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">out</span><span class="o">=</span><span class="n">X</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">X</span>

    <span class="k">def</span> <span class="nf">inverse_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Undo the scaling of X according to feature_range.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like of shape (n_samples, n_features)</span>
<span class="sd">            Input data that will be transformed. It cannot be sparse.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        Xt : ndarray of shape (n_samples, n_features)</span>
<span class="sd">            Transformed data.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

        <span class="n">X</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">copy</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">FLOAT_DTYPES</span><span class="p">,</span>
                        <span class="n">force_all_finite</span><span class="o">=</span><span class="s2">&quot;allow-nan&quot;</span><span class="p">)</span>

        <span class="n">X</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_</span>
        <span class="n">X</span> <span class="o">/=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale_</span>
        <span class="k">return</span> <span class="n">X</span>

    <span class="k">def</span> <span class="nf">_more_tags</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;allow_nan&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">}</span>


<span class="nd">@_deprecate_positional_args</span>
<span class="k">def</span> <span class="nf">minmax_scale</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">feature_range</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="o">*</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Transform features by scaling each feature to a given range.</span>

<span class="sd">    This estimator scales and translates each feature individually such</span>
<span class="sd">    that it is in the given range on the training set, i.e. between</span>
<span class="sd">    zero and one.</span>

<span class="sd">    The transformation is given by (when ``axis=0``)::</span>

<span class="sd">        X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))</span>
<span class="sd">        X_scaled = X_std * (max - min) + min</span>

<span class="sd">    where min, max = feature_range.</span>

<span class="sd">    The transformation is calculated as (when ``axis=0``)::</span>

<span class="sd">       X_scaled = scale * X + min - X.min(axis=0) * scale</span>
<span class="sd">       where scale = (max - min) / (X.max(axis=0) - X.min(axis=0))</span>

<span class="sd">    This transformation is often used as an alternative to zero mean,</span>
<span class="sd">    unit variance scaling.</span>

<span class="sd">    Read more in the :ref:`User Guide &lt;preprocessing_scaler&gt;`.</span>

<span class="sd">    .. versionadded:: 0.17</span>
<span class="sd">       *minmax_scale* function interface</span>
<span class="sd">       to :class:`~sklearn.preprocessing.MinMaxScaler`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : array-like of shape (n_samples, n_features)</span>
<span class="sd">        The data.</span>

<span class="sd">    feature_range : tuple (min, max), default=(0, 1)</span>
<span class="sd">        Desired range of transformed data.</span>

<span class="sd">    axis : int, default=0</span>
<span class="sd">        Axis used to scale along. If 0, independently scale each feature,</span>
<span class="sd">        otherwise (if 1) scale each sample.</span>

<span class="sd">    copy : bool, default=True</span>
<span class="sd">        Set to False to perform inplace scaling and avoid a copy (if the input</span>
<span class="sd">        is already a numpy array).</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    X_tr : ndarray of shape (n_samples, n_features)</span>
<span class="sd">        The transformed data.</span>

<span class="sd">    .. warning:: Risk of data leak</span>

<span class="sd">        Do not use :func:`~sklearn.preprocessing.minmax_scale` unless you know</span>
<span class="sd">        what you are doing. A common mistake is to apply it to the entire data</span>
<span class="sd">        *before* splitting into training and test sets. This will bias the</span>
<span class="sd">        model evaluation because information would have leaked from the test</span>
<span class="sd">        set to the training set.</span>
<span class="sd">        In general, we recommend using</span>
<span class="sd">        :class:`~sklearn.preprocessing.MinMaxScaler` within a</span>
<span class="sd">        :ref:`Pipeline &lt;pipeline&gt;` in order to prevent most risks of data</span>
<span class="sd">        leaking: `pipe = make_pipeline(MinMaxScaler(), LogisticRegression())`.</span>

<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    MinMaxScaler : Performs scaling to a given range using the Transformer</span>
<span class="sd">        API (e.g. as part of a preprocessing</span>
<span class="sd">        :class:`~sklearn.pipeline.Pipeline`).</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    For a comparison of the different scalers, transformers, and normalizers,</span>
<span class="sd">    see :ref:`examples/preprocessing/plot_all_scaling.py</span>
<span class="sd">    &lt;sphx_glr_auto_examples_preprocessing_plot_all_scaling.py&gt;`.</span>
<span class="sd">    &quot;&quot;&quot;</span>  <span class="c1"># noqa</span>
    <span class="c1"># Unlike the scaler object, this function allows 1d input.</span>
    <span class="c1"># If copy is required, it will be done inside the scaler object.</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ensure_2d</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                    <span class="n">dtype</span><span class="o">=</span><span class="n">FLOAT_DTYPES</span><span class="p">,</span> <span class="n">force_all_finite</span><span class="o">=</span><span class="s1">&#39;allow-nan&#39;</span><span class="p">)</span>
    <span class="n">original_ndim</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">ndim</span>

    <span class="k">if</span> <span class="n">original_ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>

    <span class="n">s</span> <span class="o">=</span> <span class="n">MinMaxScaler</span><span class="p">(</span><span class="n">feature_range</span><span class="o">=</span><span class="n">feature_range</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="n">copy</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">s</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">s</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>

    <span class="k">if</span> <span class="n">original_ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">X</span>


<span class="k">class</span> <span class="nc">StandardScaler</span><span class="p">(</span><span class="n">TransformerMixin</span><span class="p">,</span> <span class="n">BaseEstimator</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Standardize features by removing the mean and scaling to unit variance</span>

<span class="sd">    The standard score of a sample `x` is calculated as:</span>

<span class="sd">        z = (x - u) / s</span>

<span class="sd">    where `u` is the mean of the training samples or zero if `with_mean=False`,</span>
<span class="sd">    and `s` is the standard deviation of the training samples or one if</span>
<span class="sd">    `with_std=False`.</span>

<span class="sd">    Centering and scaling happen independently on each feature by computing</span>
<span class="sd">    the relevant statistics on the samples in the training set. Mean and</span>
<span class="sd">    standard deviation are then stored to be used on later data using</span>
<span class="sd">    :meth:`transform`.</span>

<span class="sd">    Standardization of a dataset is a common requirement for many</span>
<span class="sd">    machine learning estimators: they might behave badly if the</span>
<span class="sd">    individual features do not more or less look like standard normally</span>
<span class="sd">    distributed data (e.g. Gaussian with 0 mean and unit variance).</span>

<span class="sd">    For instance many elements used in the objective function of</span>
<span class="sd">    a learning algorithm (such as the RBF kernel of Support Vector</span>
<span class="sd">    Machines or the L1 and L2 regularizers of linear models) assume that</span>
<span class="sd">    all features are centered around 0 and have variance in the same</span>
<span class="sd">    order. If a feature has a variance that is orders of magnitude larger</span>
<span class="sd">    that others, it might dominate the objective function and make the</span>
<span class="sd">    estimator unable to learn from other features correctly as expected.</span>

<span class="sd">    This scaler can also be applied to sparse CSR or CSC matrices by passing</span>
<span class="sd">    `with_mean=False` to avoid breaking the sparsity structure of the data.</span>

<span class="sd">    Read more in the :ref:`User Guide &lt;preprocessing_scaler&gt;`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    copy : bool, default=True</span>
<span class="sd">        If False, try to avoid a copy and do inplace scaling instead.</span>
<span class="sd">        This is not guaranteed to always work inplace; e.g. if the data is</span>
<span class="sd">        not a NumPy array or scipy.sparse CSR matrix, a copy may still be</span>
<span class="sd">        returned.</span>

<span class="sd">    with_mean : bool, default=True</span>
<span class="sd">        If True, center the data before scaling.</span>
<span class="sd">        This does not work (and will raise an exception) when attempted on</span>
<span class="sd">        sparse matrices, because centering them entails building a dense</span>
<span class="sd">        matrix which in common use cases is likely to be too large to fit in</span>
<span class="sd">        memory.</span>

<span class="sd">    with_std : bool, default=True</span>
<span class="sd">        If True, scale the data to unit variance (or equivalently,</span>
<span class="sd">        unit standard deviation).</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    scale_ : ndarray of shape (n_features,) or None</span>
<span class="sd">        Per feature relative scaling of the data to achieve zero mean and unit</span>
<span class="sd">        variance. Generally this is calculated using `np.sqrt(var_)`. If a</span>
<span class="sd">        variance is zero, we can&#39;t achieve unit variance, and the data is left</span>
<span class="sd">        as-is, giving a scaling factor of 1. `scale_` is equal to `None`</span>
<span class="sd">        when `with_std=False`.</span>

<span class="sd">        .. versionadded:: 0.17</span>
<span class="sd">           *scale_*</span>

<span class="sd">    mean_ : ndarray of shape (n_features,) or None</span>
<span class="sd">        The mean value for each feature in the training set.</span>
<span class="sd">        Equal to ``None`` when ``with_mean=False``.</span>

<span class="sd">    var_ : ndarray of shape (n_features,) or None</span>
<span class="sd">        The variance for each feature in the training set. Used to compute</span>
<span class="sd">        `scale_`. Equal to ``None`` when ``with_std=False``.</span>

<span class="sd">    n_samples_seen_ : int or ndarray of shape (n_features,)</span>
<span class="sd">        The number of samples processed by the estimator for each feature.</span>
<span class="sd">        If there are no missing samples, the ``n_samples_seen`` will be an</span>
<span class="sd">        integer, otherwise it will be an array of dtype int. If</span>
<span class="sd">        `sample_weights` are used it will be a float (if no missing data)</span>
<span class="sd">        or an array of dtype float that sums the weights seen so far.</span>
<span class="sd">        Will be reset on new calls to fit, but increments across</span>
<span class="sd">        ``partial_fit`` calls.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.preprocessing import StandardScaler</span>
<span class="sd">    &gt;&gt;&gt; data = [[0, 0], [0, 0], [1, 1], [1, 1]]</span>
<span class="sd">    &gt;&gt;&gt; scaler = StandardScaler()</span>
<span class="sd">    &gt;&gt;&gt; print(scaler.fit(data))</span>
<span class="sd">    StandardScaler()</span>
<span class="sd">    &gt;&gt;&gt; print(scaler.mean_)</span>
<span class="sd">    [0.5 0.5]</span>
<span class="sd">    &gt;&gt;&gt; print(scaler.transform(data))</span>
<span class="sd">    [[-1. -1.]</span>
<span class="sd">     [-1. -1.]</span>
<span class="sd">     [ 1.  1.]</span>
<span class="sd">     [ 1.  1.]]</span>
<span class="sd">    &gt;&gt;&gt; print(scaler.transform([[2, 2]]))</span>
<span class="sd">    [[3. 3.]]</span>

<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    scale : Equivalent function without the estimator API.</span>

<span class="sd">    :class:`~sklearn.decomposition.PCA` : Further removes the linear</span>
<span class="sd">        correlation across features with &#39;whiten=True&#39;.</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    NaNs are treated as missing values: disregarded in fit, and maintained in</span>
<span class="sd">    transform.</span>

<span class="sd">    We use a biased estimator for the standard deviation, equivalent to</span>
<span class="sd">    `numpy.std(x, ddof=0)`. Note that the choice of `ddof` is unlikely to</span>
<span class="sd">    affect model performance.</span>

<span class="sd">    For a comparison of the different scalers, transformers, and normalizers,</span>
<span class="sd">    see :ref:`examples/preprocessing/plot_all_scaling.py</span>
<span class="sd">    &lt;sphx_glr_auto_examples_preprocessing_plot_all_scaling.py&gt;`.</span>
<span class="sd">    &quot;&quot;&quot;</span>  <span class="c1"># noqa</span>

    <span class="nd">@_deprecate_positional_args</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">with_mean</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">with_std</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">with_mean</span> <span class="o">=</span> <span class="n">with_mean</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">with_std</span> <span class="o">=</span> <span class="n">with_std</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">copy</span> <span class="o">=</span> <span class="n">copy</span>

    <span class="k">def</span> <span class="nf">_reset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Reset internal data-dependent state of the scaler, if necessary.</span>

<span class="sd">        __init__ parameters are not touched.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Checking one attribute is enough, becase they are all set together</span>
        <span class="c1"># in partial_fit</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;scale_&#39;</span><span class="p">):</span>
            <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale_</span>
            <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_samples_seen_</span>
            <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean_</span>
            <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">var_</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Compute the mean and std to be used for later scaling.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            The data used to compute the mean and standard deviation</span>
<span class="sd">            used for later scaling along the features axis.</span>

<span class="sd">        y : None</span>
<span class="sd">            Ignored.</span>

<span class="sd">        sample_weight : array-like of shape (n_samples,), default=None</span>
<span class="sd">            Individual weights for each sample.</span>

<span class="sd">            .. versionadded:: 0.24</span>
<span class="sd">               parameter *sample_weight* support to StandardScaler.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        self : object</span>
<span class="sd">            Fitted scaler.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Reset internal state before fitting</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_reset</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">partial_fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">partial_fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Online computation of mean and std on X for later scaling.</span>

<span class="sd">        All of X is processed as a single batch. This is intended for cases</span>
<span class="sd">        when :meth:`fit` is not feasible due to very large number of</span>
<span class="sd">        `n_samples` or because X is read from a continuous stream.</span>

<span class="sd">        The algorithm for incremental mean and std is given in Equation 1.5a,b</span>
<span class="sd">        in Chan, Tony F., Gene H. Golub, and Randall J. LeVeque. &quot;Algorithms</span>
<span class="sd">        for computing the sample variance: Analysis and recommendations.&quot;</span>
<span class="sd">        The American Statistician 37.3 (1983): 242-247:</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            The data used to compute the mean and standard deviation</span>
<span class="sd">            used for later scaling along the features axis.</span>

<span class="sd">        y : None</span>
<span class="sd">            Ignored.</span>

<span class="sd">        sample_weight : array-like of shape (n_samples,), default=None</span>
<span class="sd">            Individual weights for each sample.</span>

<span class="sd">            .. versionadded:: 0.24</span>
<span class="sd">               parameter *sample_weight* support to StandardScaler.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        self : object</span>
<span class="sd">            Fitted scaler.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">first_call</span> <span class="o">=</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;n_samples_seen_&quot;</span><span class="p">)</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_data</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">accept_sparse</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;csr&#39;</span><span class="p">,</span> <span class="s1">&#39;csc&#39;</span><span class="p">),</span>
                                <span class="n">estimator</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">FLOAT_DTYPES</span><span class="p">,</span>
                                <span class="n">force_all_finite</span><span class="o">=</span><span class="s1">&#39;allow-nan&#39;</span><span class="p">,</span> <span class="n">reset</span><span class="o">=</span><span class="n">first_call</span><span class="p">)</span>
        <span class="n">n_features</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">_check_sample_weight</span><span class="p">(</span><span class="n">sample_weight</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span>
                                                 <span class="n">dtype</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

        <span class="c1"># Even in the case of `with_mean=False`, we update the mean anyway</span>
        <span class="c1"># This is needed for the incremental computation of the var</span>
        <span class="c1"># See incr_mean_variance_axis and _incremental_mean_variance_axis</span>

        <span class="c1"># if n_samples_seen_ is an integer (i.e. no missing values), we need to</span>
        <span class="c1"># transform it to a NumPy array of shape (n_features,) required by</span>
        <span class="c1"># incr_mean_variance_axis and _incremental_variance_axis</span>
        <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">int64</span> <span class="k">if</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">X</span><span class="o">.</span><span class="n">dtype</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;n_samples_seen_&#39;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">n_samples_seen_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_features</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">np</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_samples_seen_</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">n_samples_seen_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">n_samples_seen_</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">n_samples_seen_</span> <span class="o">=</span> \
                <span class="bp">self</span><span class="o">.</span><span class="n">n_samples_seen_</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">dtype</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">sparse</span><span class="o">.</span><span class="n">issparse</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">with_mean</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;Cannot center sparse matrices: pass `with_mean=False` &quot;</span>
                    <span class="s2">&quot;instead. See docstring for motivation and alternatives.&quot;</span><span class="p">)</span>
            <span class="n">sparse_constructor</span> <span class="o">=</span> <span class="p">(</span><span class="n">sparse</span><span class="o">.</span><span class="n">csr_matrix</span>
                                  <span class="k">if</span> <span class="n">X</span><span class="o">.</span><span class="n">format</span> <span class="o">==</span> <span class="s1">&#39;csr&#39;</span> <span class="k">else</span> <span class="n">sparse</span><span class="o">.</span><span class="n">csc_matrix</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">with_std</span><span class="p">:</span>
                <span class="c1"># First pass</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;scale_&#39;</span><span class="p">):</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">mean_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">var_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_samples_seen_</span> <span class="o">=</span> \
                        <span class="n">mean_variance_axis</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">,</span>
                                           <span class="n">return_sum_weights</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                <span class="c1"># Next passes</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">mean_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">var_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_samples_seen_</span> <span class="o">=</span> \
                        <span class="n">incr_mean_variance_axis</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                                                <span class="n">last_mean</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">mean_</span><span class="p">,</span>
                                                <span class="n">last_var</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">var_</span><span class="p">,</span>
                                                <span class="n">last_n</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_samples_seen_</span><span class="p">,</span>
                                                <span class="n">weights</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">)</span>
                <span class="c1"># We force the mean and variance to float64 for large arrays</span>
                <span class="c1"># See https://github.com/scikit-learn/scikit-learn/pull/12338</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">mean_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean_</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">var_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">var_</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">mean_</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># as with_mean must be False for sparse</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">var_</span> <span class="o">=</span> <span class="kc">None</span>
                <span class="n">weights</span> <span class="o">=</span> <span class="n">_check_sample_weight</span><span class="p">(</span><span class="n">sample_weight</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
                <span class="n">sum_weights_nan</span> <span class="o">=</span> <span class="n">weights</span> <span class="o">@</span> <span class="n">sparse_constructor</span><span class="p">(</span>
                    <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">data</span><span class="p">),</span> <span class="n">X</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">indptr</span><span class="p">),</span>
                    <span class="n">shape</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">n_samples_seen_</span> <span class="o">+=</span> <span class="p">(</span>
                    <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span> <span class="o">-</span> <span class="n">sum_weights_nan</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
                <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># First pass</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;scale_&#39;</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">mean_</span> <span class="o">=</span> <span class="mf">.0</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">with_std</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">var_</span> <span class="o">=</span> <span class="mf">.0</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">var_</span> <span class="o">=</span> <span class="kc">None</span>

            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">with_mean</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">with_std</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">mean_</span> <span class="o">=</span> <span class="kc">None</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">var_</span> <span class="o">=</span> <span class="kc">None</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">n_samples_seen_</span> <span class="o">+=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

            <span class="k">elif</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">mean_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">var_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_samples_seen_</span> <span class="o">=</span> \
                    <span class="n">_incremental_weighted_mean_and_var</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">,</span>
                                                       <span class="bp">self</span><span class="o">.</span><span class="n">mean_</span><span class="p">,</span>
                                                       <span class="bp">self</span><span class="o">.</span><span class="n">var_</span><span class="p">,</span>
                                                       <span class="bp">self</span><span class="o">.</span><span class="n">n_samples_seen_</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">mean_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">var_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_samples_seen_</span> <span class="o">=</span> \
                    <span class="n">_incremental_mean_and_var</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">var_</span><span class="p">,</span>
                                              <span class="bp">self</span><span class="o">.</span><span class="n">n_samples_seen_</span><span class="p">)</span>

        <span class="c1"># for backward-compatibility, reduce n_samples_seen_ to an integer</span>
        <span class="c1"># if the number of samples is the same for each feature (i.e. no</span>
        <span class="c1"># missing values)</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">ptp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_samples_seen_</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">n_samples_seen_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_samples_seen_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">with_std</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">scale_</span> <span class="o">=</span> <span class="n">_handle_zeros_in_scale</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">var_</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">scale_</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Perform standardization by centering and scaling</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix of shape (n_samples, n_features)</span>
<span class="sd">            The data used to scale along the features axis.</span>
<span class="sd">        copy : bool, default=None</span>
<span class="sd">            Copy the input X or not.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        X_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            Transformed array.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

        <span class="n">copy</span> <span class="o">=</span> <span class="n">copy</span> <span class="k">if</span> <span class="n">copy</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">copy</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_data</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">reset</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                <span class="n">accept_sparse</span><span class="o">=</span><span class="s1">&#39;csr&#39;</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="n">copy</span><span class="p">,</span>
                                <span class="n">estimator</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">FLOAT_DTYPES</span><span class="p">,</span>
                                <span class="n">force_all_finite</span><span class="o">=</span><span class="s1">&#39;allow-nan&#39;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">sparse</span><span class="o">.</span><span class="n">issparse</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">with_mean</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;Cannot center sparse matrices: pass `with_mean=False` &quot;</span>
                    <span class="s2">&quot;instead. See docstring for motivation and alternatives.&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale_</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">inplace_column_scale</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="mi">1</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale_</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">with_mean</span><span class="p">:</span>
                <span class="n">X</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean_</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">with_std</span><span class="p">:</span>
                <span class="n">X</span> <span class="o">/=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale_</span>
        <span class="k">return</span> <span class="n">X</span>

    <span class="k">def</span> <span class="nf">inverse_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Scale back the data to the original representation</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            The data used to scale along the features axis.</span>
<span class="sd">        copy : bool, default=None</span>
<span class="sd">            Copy the input X or not.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        X_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            Transformed array.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

        <span class="n">copy</span> <span class="o">=</span> <span class="n">copy</span> <span class="k">if</span> <span class="n">copy</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">copy</span>
        <span class="k">if</span> <span class="n">sparse</span><span class="o">.</span><span class="n">issparse</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">with_mean</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;Cannot uncenter sparse matrices: pass `with_mean=False` &quot;</span>
                    <span class="s2">&quot;instead See docstring for motivation and alternatives.&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">sparse</span><span class="o">.</span><span class="n">isspmatrix_csr</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
                <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">tocsr</span><span class="p">()</span>
                <span class="n">copy</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">if</span> <span class="n">copy</span><span class="p">:</span>
                <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale_</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">inplace_column_scale</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale_</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">copy</span><span class="p">:</span>
                <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">with_std</span><span class="p">:</span>
                <span class="n">X</span> <span class="o">*=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale_</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">with_mean</span><span class="p">:</span>
                <span class="n">X</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean_</span>
        <span class="k">return</span> <span class="n">X</span>

    <span class="k">def</span> <span class="nf">_more_tags</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;allow_nan&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
                <span class="s1">&#39;preserves_dtype&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">]}</span>


<span class="k">class</span> <span class="nc">MaxAbsScaler</span><span class="p">(</span><span class="n">TransformerMixin</span><span class="p">,</span> <span class="n">BaseEstimator</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Scale each feature by its maximum absolute value.</span>

<span class="sd">    This estimator scales and translates each feature individually such</span>
<span class="sd">    that the maximal absolute value of each feature in the</span>
<span class="sd">    training set will be 1.0. It does not shift/center the data, and</span>
<span class="sd">    thus does not destroy any sparsity.</span>

<span class="sd">    This scaler can also be applied to sparse CSR or CSC matrices.</span>

<span class="sd">    .. versionadded:: 0.17</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    copy : bool, default=True</span>
<span class="sd">        Set to False to perform inplace scaling and avoid a copy (if the input</span>
<span class="sd">        is already a numpy array).</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    scale_ : ndarray of shape (n_features,)</span>
<span class="sd">        Per feature relative scaling of the data.</span>

<span class="sd">        .. versionadded:: 0.17</span>
<span class="sd">           *scale_* attribute.</span>

<span class="sd">    max_abs_ : ndarray of shape (n_features,)</span>
<span class="sd">        Per feature maximum absolute value.</span>

<span class="sd">    n_samples_seen_ : int</span>
<span class="sd">        The number of samples processed by the estimator. Will be reset on</span>
<span class="sd">        new calls to fit, but increments across ``partial_fit`` calls.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.preprocessing import MaxAbsScaler</span>
<span class="sd">    &gt;&gt;&gt; X = [[ 1., -1.,  2.],</span>
<span class="sd">    ...      [ 2.,  0.,  0.],</span>
<span class="sd">    ...      [ 0.,  1., -1.]]</span>
<span class="sd">    &gt;&gt;&gt; transformer = MaxAbsScaler().fit(X)</span>
<span class="sd">    &gt;&gt;&gt; transformer</span>
<span class="sd">    MaxAbsScaler()</span>
<span class="sd">    &gt;&gt;&gt; transformer.transform(X)</span>
<span class="sd">    array([[ 0.5, -1. ,  1. ],</span>
<span class="sd">           [ 1. ,  0. ,  0. ],</span>
<span class="sd">           [ 0. ,  1. , -0.5]])</span>

<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    maxabs_scale : Equivalent function without the estimator API.</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    NaNs are treated as missing values: disregarded in fit, and maintained in</span>
<span class="sd">    transform.</span>

<span class="sd">    For a comparison of the different scalers, transformers, and normalizers,</span>
<span class="sd">    see :ref:`examples/preprocessing/plot_all_scaling.py</span>
<span class="sd">    &lt;sphx_glr_auto_examples_preprocessing_plot_all_scaling.py&gt;`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@_deprecate_positional_args</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">copy</span> <span class="o">=</span> <span class="n">copy</span>

    <span class="k">def</span> <span class="nf">_reset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Reset internal data-dependent state of the scaler, if necessary.</span>

<span class="sd">        __init__ parameters are not touched.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Checking one attribute is enough, becase they are all set together</span>
        <span class="c1"># in partial_fit</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;scale_&#39;</span><span class="p">):</span>
            <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale_</span>
            <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_samples_seen_</span>
            <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_abs_</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Compute the maximum absolute value to be used for later scaling.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            The data used to compute the per-feature minimum and maximum</span>
<span class="sd">            used for later scaling along the features axis.</span>

<span class="sd">        y : None</span>
<span class="sd">            Ignored.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        self : object</span>
<span class="sd">            Fitted scaler.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Reset internal state before fitting</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_reset</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">partial_fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">partial_fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Online computation of max absolute value of X for later scaling.</span>

<span class="sd">        All of X is processed as a single batch. This is intended for cases</span>
<span class="sd">        when :meth:`fit` is not feasible due to very large number of</span>
<span class="sd">        `n_samples` or because X is read from a continuous stream.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            The data used to compute the mean and standard deviation</span>
<span class="sd">            used for later scaling along the features axis.</span>

<span class="sd">        y : None</span>
<span class="sd">            Ignored.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        self : object</span>
<span class="sd">            Fitted scaler.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">first_pass</span> <span class="o">=</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;n_samples_seen_&#39;</span><span class="p">)</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_data</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">reset</span><span class="o">=</span><span class="n">first_pass</span><span class="p">,</span>
                                <span class="n">accept_sparse</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;csr&#39;</span><span class="p">,</span> <span class="s1">&#39;csc&#39;</span><span class="p">),</span> <span class="n">estimator</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
                                <span class="n">dtype</span><span class="o">=</span><span class="n">FLOAT_DTYPES</span><span class="p">,</span>
                                <span class="n">force_all_finite</span><span class="o">=</span><span class="s1">&#39;allow-nan&#39;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">sparse</span><span class="o">.</span><span class="n">issparse</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
            <span class="n">mins</span><span class="p">,</span> <span class="n">maxs</span> <span class="o">=</span> <span class="n">min_max_axis</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ignore_nan</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">max_abs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">mins</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">maxs</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">max_abs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nanmax</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">first_pass</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">n_samples_seen_</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">max_abs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">max_abs_</span><span class="p">,</span> <span class="n">max_abs</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">n_samples_seen_</span> <span class="o">+=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">max_abs_</span> <span class="o">=</span> <span class="n">max_abs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scale_</span> <span class="o">=</span> <span class="n">_handle_zeros_in_scale</span><span class="p">(</span><span class="n">max_abs</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Scale the data</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            The data that should be scaled.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        X_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            Transformed array.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_data</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">accept_sparse</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;csr&#39;</span><span class="p">,</span> <span class="s1">&#39;csc&#39;</span><span class="p">),</span>
                                <span class="n">copy</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">copy</span><span class="p">,</span> <span class="n">reset</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                <span class="n">estimator</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">FLOAT_DTYPES</span><span class="p">,</span>
                                <span class="n">force_all_finite</span><span class="o">=</span><span class="s1">&#39;allow-nan&#39;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">sparse</span><span class="o">.</span><span class="n">issparse</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
            <span class="n">inplace_column_scale</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale_</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">X</span> <span class="o">/=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale_</span>
        <span class="k">return</span> <span class="n">X</span>

    <span class="k">def</span> <span class="nf">inverse_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Scale back the data to the original representation</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            The data that should be transformed back.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        X_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            Transformed array.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">accept_sparse</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;csr&#39;</span><span class="p">,</span> <span class="s1">&#39;csc&#39;</span><span class="p">),</span> <span class="n">copy</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">copy</span><span class="p">,</span>
                        <span class="n">estimator</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">FLOAT_DTYPES</span><span class="p">,</span>
                        <span class="n">force_all_finite</span><span class="o">=</span><span class="s1">&#39;allow-nan&#39;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">sparse</span><span class="o">.</span><span class="n">issparse</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
            <span class="n">inplace_column_scale</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale_</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">X</span> <span class="o">*=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale_</span>
        <span class="k">return</span> <span class="n">X</span>

    <span class="k">def</span> <span class="nf">_more_tags</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;allow_nan&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">}</span>


<span class="nd">@_deprecate_positional_args</span>
<span class="k">def</span> <span class="nf">maxabs_scale</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Scale each feature to the [-1, 1] range without breaking the sparsity.</span>

<span class="sd">    This estimator scales each feature individually such</span>
<span class="sd">    that the maximal absolute value of each feature in the</span>
<span class="sd">    training set will be 1.0.</span>

<span class="sd">    This scaler can also be applied to sparse CSR or CSC matrices.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">        The data.</span>

<span class="sd">    axis : int, default=0</span>
<span class="sd">        axis used to scale along. If 0, independently scale each feature,</span>
<span class="sd">        otherwise (if 1) scale each sample.</span>

<span class="sd">    copy : bool, default=True</span>
<span class="sd">        Set to False to perform inplace scaling and avoid a copy (if the input</span>
<span class="sd">        is already a numpy array).</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    X_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">        The transformed data.</span>

<span class="sd">    .. warning:: Risk of data leak</span>

<span class="sd">        Do not use :func:`~sklearn.preprocessing.maxabs_scale` unless you know what</span>
<span class="sd">        you are doing. A common mistake is to apply it to the entire data</span>
<span class="sd">        *before* splitting into training and test sets. This will bias the</span>
<span class="sd">        model evaluation because information would have leaked from the test</span>
<span class="sd">        set to the training set.</span>
<span class="sd">        In general, we recommend using</span>
<span class="sd">        :class:`~sklearn.preprocessing.MaxAbsScaler` within a</span>
<span class="sd">        :ref:`Pipeline &lt;pipeline&gt;` in order to prevent most risks of data</span>
<span class="sd">        leaking: `pipe = make_pipeline(MaxAbsScaler(), LogisticRegression())`.</span>

<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    MaxAbsScaler : Performs scaling to the [-1, 1] range using</span>
<span class="sd">        the Transformer API (e.g. as part of a preprocessing</span>
<span class="sd">        :class:`~sklearn.pipeline.Pipeline`).</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    NaNs are treated as missing values: disregarded to compute the statistics,</span>
<span class="sd">    and maintained during the data transformation.</span>

<span class="sd">    For a comparison of the different scalers, transformers, and normalizers,</span>
<span class="sd">    see :ref:`examples/preprocessing/plot_all_scaling.py</span>
<span class="sd">    &lt;sphx_glr_auto_examples_preprocessing_plot_all_scaling.py&gt;`.</span>
<span class="sd">    &quot;&quot;&quot;</span>  <span class="c1"># noqa</span>
    <span class="c1"># Unlike the scaler object, this function allows 1d input.</span>

    <span class="c1"># If copy is required, it will be done inside the scaler object.</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">accept_sparse</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;csr&#39;</span><span class="p">,</span> <span class="s1">&#39;csc&#39;</span><span class="p">),</span> <span class="n">copy</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                    <span class="n">ensure_2d</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">FLOAT_DTYPES</span><span class="p">,</span>
                    <span class="n">force_all_finite</span><span class="o">=</span><span class="s1">&#39;allow-nan&#39;</span><span class="p">)</span>
    <span class="n">original_ndim</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">ndim</span>

    <span class="k">if</span> <span class="n">original_ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>

    <span class="n">s</span> <span class="o">=</span> <span class="n">MaxAbsScaler</span><span class="p">(</span><span class="n">copy</span><span class="o">=</span><span class="n">copy</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">s</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">s</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>

    <span class="k">if</span> <span class="n">original_ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">X</span>


<span class="k">class</span> <span class="nc">RobustScaler</span><span class="p">(</span><span class="n">TransformerMixin</span><span class="p">,</span> <span class="n">BaseEstimator</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Scale features using statistics that are robust to outliers.</span>

<span class="sd">    This Scaler removes the median and scales the data according to</span>
<span class="sd">    the quantile range (defaults to IQR: Interquartile Range).</span>
<span class="sd">    The IQR is the range between the 1st quartile (25th quantile)</span>
<span class="sd">    and the 3rd quartile (75th quantile).</span>

<span class="sd">    Centering and scaling happen independently on each feature by</span>
<span class="sd">    computing the relevant statistics on the samples in the training</span>
<span class="sd">    set. Median and interquartile range are then stored to be used on</span>
<span class="sd">    later data using the ``transform`` method.</span>

<span class="sd">    Standardization of a dataset is a common requirement for many</span>
<span class="sd">    machine learning estimators. Typically this is done by removing the mean</span>
<span class="sd">    and scaling to unit variance. However, outliers can often influence the</span>
<span class="sd">    sample mean / variance in a negative way. In such cases, the median and</span>
<span class="sd">    the interquartile range often give better results.</span>

<span class="sd">    .. versionadded:: 0.17</span>

<span class="sd">    Read more in the :ref:`User Guide &lt;preprocessing_scaler&gt;`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    with_centering : bool, default=True</span>
<span class="sd">        If True, center the data before scaling.</span>
<span class="sd">        This will cause ``transform`` to raise an exception when attempted on</span>
<span class="sd">        sparse matrices, because centering them entails building a dense</span>
<span class="sd">        matrix which in common use cases is likely to be too large to fit in</span>
<span class="sd">        memory.</span>

<span class="sd">    with_scaling : bool, default=True</span>
<span class="sd">        If True, scale the data to interquartile range.</span>

<span class="sd">    quantile_range : tuple (q_min, q_max), 0.0 &lt; q_min &lt; q_max &lt; 100.0, \</span>
<span class="sd">        default=(25.0, 75.0), == (1st quantile, 3rd quantile), == IQR</span>
<span class="sd">        Quantile range used to calculate ``scale_``.</span>

<span class="sd">        .. versionadded:: 0.18</span>

<span class="sd">    copy : bool, default=True</span>
<span class="sd">        If False, try to avoid a copy and do inplace scaling instead.</span>
<span class="sd">        This is not guaranteed to always work inplace; e.g. if the data is</span>
<span class="sd">        not a NumPy array or scipy.sparse CSR matrix, a copy may still be</span>
<span class="sd">        returned.</span>

<span class="sd">    unit_variance : bool, default=False</span>
<span class="sd">        If True, scale data so that normally distributed features have a</span>
<span class="sd">        variance of 1. In general, if the difference between the x-values of</span>
<span class="sd">        ``q_max`` and ``q_min`` for a standard normal distribution is greater</span>
<span class="sd">        than 1, the dataset will be scaled down. If less than 1, the dataset</span>
<span class="sd">        will be scaled up.</span>

<span class="sd">        .. versionadded:: 0.24</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    center_ : array of floats</span>
<span class="sd">        The median value for each feature in the training set.</span>

<span class="sd">    scale_ : array of floats</span>
<span class="sd">        The (scaled) interquartile range for each feature in the training set.</span>

<span class="sd">        .. versionadded:: 0.17</span>
<span class="sd">           *scale_* attribute.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.preprocessing import RobustScaler</span>
<span class="sd">    &gt;&gt;&gt; X = [[ 1., -2.,  2.],</span>
<span class="sd">    ...      [ -2.,  1.,  3.],</span>
<span class="sd">    ...      [ 4.,  1., -2.]]</span>
<span class="sd">    &gt;&gt;&gt; transformer = RobustScaler().fit(X)</span>
<span class="sd">    &gt;&gt;&gt; transformer</span>
<span class="sd">    RobustScaler()</span>
<span class="sd">    &gt;&gt;&gt; transformer.transform(X)</span>
<span class="sd">    array([[ 0. , -2. ,  0. ],</span>
<span class="sd">           [-1. ,  0. ,  0.4],</span>
<span class="sd">           [ 1. ,  0. , -1.6]])</span>

<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    robust_scale : Equivalent function without the estimator API.</span>

<span class="sd">    :class:`~sklearn.decomposition.PCA`</span>
<span class="sd">        Further removes the linear correlation across features with</span>
<span class="sd">        &#39;whiten=True&#39;.</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    For a comparison of the different scalers, transformers, and normalizers,</span>
<span class="sd">    see :ref:`examples/preprocessing/plot_all_scaling.py</span>
<span class="sd">    &lt;sphx_glr_auto_examples_preprocessing_plot_all_scaling.py&gt;`.</span>

<span class="sd">    https://en.wikipedia.org/wiki/Median</span>
<span class="sd">    https://en.wikipedia.org/wiki/Interquartile_range</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@_deprecate_positional_args</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">with_centering</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">with_scaling</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">quantile_range</span><span class="o">=</span><span class="p">(</span><span class="mf">25.0</span><span class="p">,</span> <span class="mf">75.0</span><span class="p">),</span> <span class="n">copy</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">unit_variance</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">with_centering</span> <span class="o">=</span> <span class="n">with_centering</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">with_scaling</span> <span class="o">=</span> <span class="n">with_scaling</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">quantile_range</span> <span class="o">=</span> <span class="n">quantile_range</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">unit_variance</span> <span class="o">=</span> <span class="n">unit_variance</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">copy</span> <span class="o">=</span> <span class="n">copy</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Compute the median and quantiles to be used for scaling.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            The data used to compute the median and quantiles</span>
<span class="sd">            used for later scaling along the features axis.</span>

<span class="sd">        y : None</span>
<span class="sd">            Ignored.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        self : object</span>
<span class="sd">            Fitted scaler.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># at fit, convert sparse matrices to csc for optimized computation of</span>
        <span class="c1"># the quantiles</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_data</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">accept_sparse</span><span class="o">=</span><span class="s1">&#39;csc&#39;</span><span class="p">,</span> <span class="n">estimator</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
                                <span class="n">dtype</span><span class="o">=</span><span class="n">FLOAT_DTYPES</span><span class="p">,</span>
                                <span class="n">force_all_finite</span><span class="o">=</span><span class="s1">&#39;allow-nan&#39;</span><span class="p">)</span>

        <span class="n">q_min</span><span class="p">,</span> <span class="n">q_max</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">quantile_range</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="mi">0</span> <span class="o">&lt;=</span> <span class="n">q_min</span> <span class="o">&lt;=</span> <span class="n">q_max</span> <span class="o">&lt;=</span> <span class="mi">100</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid quantile range: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span>
                             <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">quantile_range</span><span class="p">))</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">with_centering</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">sparse</span><span class="o">.</span><span class="n">issparse</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;Cannot center sparse matrices: use `with_centering=False`&quot;</span>
                    <span class="s2">&quot; instead. See docstring for motivation and alternatives.&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">center_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nanmedian</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">center_</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">with_scaling</span><span class="p">:</span>
            <span class="n">quantiles</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">feature_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
                <span class="k">if</span> <span class="n">sparse</span><span class="o">.</span><span class="n">issparse</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
                    <span class="n">column_nnz_data</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">X</span><span class="o">.</span><span class="n">indptr</span><span class="p">[</span><span class="n">feature_idx</span><span class="p">]:</span>
                                             <span class="n">X</span><span class="o">.</span><span class="n">indptr</span><span class="p">[</span><span class="n">feature_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]]</span>
                    <span class="n">column_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
                    <span class="n">column_data</span><span class="p">[:</span><span class="nb">len</span><span class="p">(</span><span class="n">column_nnz_data</span><span class="p">)]</span> <span class="o">=</span> <span class="n">column_nnz_data</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">column_data</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="n">feature_idx</span><span class="p">]</span>

                <span class="n">quantiles</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">nanpercentile</span><span class="p">(</span><span class="n">column_data</span><span class="p">,</span>
                                                  <span class="bp">self</span><span class="o">.</span><span class="n">quantile_range</span><span class="p">))</span>

            <span class="n">quantiles</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">quantiles</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">scale_</span> <span class="o">=</span> <span class="n">quantiles</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">quantiles</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">scale_</span> <span class="o">=</span> <span class="n">_handle_zeros_in_scale</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">scale_</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">unit_variance</span><span class="p">:</span>
                <span class="n">adjust</span> <span class="o">=</span> <span class="p">(</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="n">q_max</span> <span class="o">/</span> <span class="mf">100.0</span><span class="p">)</span> <span class="o">-</span>
                          <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="n">q_min</span> <span class="o">/</span> <span class="mf">100.0</span><span class="p">))</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">scale_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale_</span> <span class="o">/</span> <span class="n">adjust</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">scale_</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Center and scale the data.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            The data used to scale along the specified axis.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        X_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            Transformed array.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_data</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">accept_sparse</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;csr&#39;</span><span class="p">,</span> <span class="s1">&#39;csc&#39;</span><span class="p">),</span>
                                <span class="n">copy</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">copy</span><span class="p">,</span> <span class="n">estimator</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
                                <span class="n">dtype</span><span class="o">=</span><span class="n">FLOAT_DTYPES</span><span class="p">,</span> <span class="n">reset</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                <span class="n">force_all_finite</span><span class="o">=</span><span class="s1">&#39;allow-nan&#39;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">sparse</span><span class="o">.</span><span class="n">issparse</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">with_scaling</span><span class="p">:</span>
                <span class="n">inplace_column_scale</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale_</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">with_centering</span><span class="p">:</span>
                <span class="n">X</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">center_</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">with_scaling</span><span class="p">:</span>
                <span class="n">X</span> <span class="o">/=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale_</span>
        <span class="k">return</span> <span class="n">X</span>

    <span class="k">def</span> <span class="nf">inverse_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Scale back the data to the original representation</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            The rescaled data to be transformed back.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        X_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            Transformed array.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">accept_sparse</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;csr&#39;</span><span class="p">,</span> <span class="s1">&#39;csc&#39;</span><span class="p">),</span> <span class="n">copy</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">copy</span><span class="p">,</span>
                        <span class="n">estimator</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">FLOAT_DTYPES</span><span class="p">,</span>
                        <span class="n">force_all_finite</span><span class="o">=</span><span class="s1">&#39;allow-nan&#39;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">sparse</span><span class="o">.</span><span class="n">issparse</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">with_scaling</span><span class="p">:</span>
                <span class="n">inplace_column_scale</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale_</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">with_scaling</span><span class="p">:</span>
                <span class="n">X</span> <span class="o">*=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale_</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">with_centering</span><span class="p">:</span>
                <span class="n">X</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">center_</span>
        <span class="k">return</span> <span class="n">X</span>

    <span class="k">def</span> <span class="nf">_more_tags</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;allow_nan&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">}</span>


<span class="nd">@_deprecate_positional_args</span>
<span class="k">def</span> <span class="nf">robust_scale</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">with_centering</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">with_scaling</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">quantile_range</span><span class="o">=</span><span class="p">(</span><span class="mf">25.0</span><span class="p">,</span> <span class="mf">75.0</span><span class="p">),</span> <span class="n">copy</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">unit_variance</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Standardize a dataset along any axis</span>

<span class="sd">    Center to the median and component wise scale</span>
<span class="sd">    according to the interquartile range.</span>

<span class="sd">    Read more in the :ref:`User Guide &lt;preprocessing_scaler&gt;`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : {array-like, sparse matrix} of shape (n_sample, n_features)</span>
<span class="sd">        The data to center and scale.</span>

<span class="sd">    axis : int, default=0</span>
<span class="sd">        axis used to compute the medians and IQR along. If 0,</span>
<span class="sd">        independently scale each feature, otherwise (if 1) scale</span>
<span class="sd">        each sample.</span>

<span class="sd">    with_centering : bool, default=True</span>
<span class="sd">        If True, center the data before scaling.</span>

<span class="sd">    with_scaling : bool, default=True</span>
<span class="sd">        If True, scale the data to unit variance (or equivalently,</span>
<span class="sd">        unit standard deviation).</span>

<span class="sd">    quantile_range : tuple (q_min, q_max), 0.0 &lt; q_min &lt; q_max &lt; 100.0</span>
<span class="sd">        default=(25.0, 75.0), == (1st quantile, 3rd quantile), == IQR</span>
<span class="sd">        Quantile range used to calculate ``scale_``.</span>

<span class="sd">        .. versionadded:: 0.18</span>

<span class="sd">    copy : bool, default=True</span>
<span class="sd">        set to False to perform inplace row normalization and avoid a</span>
<span class="sd">        copy (if the input is already a numpy array or a scipy.sparse</span>
<span class="sd">        CSR matrix and if axis is 1).</span>

<span class="sd">    unit_variance : bool, default=False</span>
<span class="sd">        If True, scale data so that normally distributed features have a</span>
<span class="sd">        variance of 1. In general, if the difference between the x-values of</span>
<span class="sd">        ``q_max`` and ``q_min`` for a standard normal distribution is greater</span>
<span class="sd">        than 1, the dataset will be scaled down. If less than 1, the dataset</span>
<span class="sd">        will be scaled up.</span>

<span class="sd">        .. versionadded:: 0.24</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    X_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">        The transformed data.</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    This implementation will refuse to center scipy.sparse matrices</span>
<span class="sd">    since it would make them non-sparse and would potentially crash the</span>
<span class="sd">    program with memory exhaustion problems.</span>

<span class="sd">    Instead the caller is expected to either set explicitly</span>
<span class="sd">    `with_centering=False` (in that case, only variance scaling will be</span>
<span class="sd">    performed on the features of the CSR matrix) or to call `X.toarray()`</span>
<span class="sd">    if he/she expects the materialized dense array to fit in memory.</span>

<span class="sd">    To avoid memory copy the caller should pass a CSR matrix.</span>

<span class="sd">    For a comparison of the different scalers, transformers, and normalizers,</span>
<span class="sd">    see :ref:`examples/preprocessing/plot_all_scaling.py</span>
<span class="sd">    &lt;sphx_glr_auto_examples_preprocessing_plot_all_scaling.py&gt;`.</span>

<span class="sd">    .. warning:: Risk of data leak</span>

<span class="sd">        Do not use :func:`~sklearn.preprocessing.robust_scale` unless you know</span>
<span class="sd">        what you are doing. A common mistake is to apply it to the entire data</span>
<span class="sd">        *before* splitting into training and test sets. This will bias the</span>
<span class="sd">        model evaluation because information would have leaked from the test</span>
<span class="sd">        set to the training set.</span>
<span class="sd">        In general, we recommend using</span>
<span class="sd">        :class:`~sklearn.preprocessing.RobustScaler` within a</span>
<span class="sd">        :ref:`Pipeline &lt;pipeline&gt;` in order to prevent most risks of data</span>
<span class="sd">        leaking: `pipe = make_pipeline(RobustScaler(), LogisticRegression())`.</span>

<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    RobustScaler : Performs centering and scaling using the Transformer API</span>
<span class="sd">        (e.g. as part of a preprocessing :class:`~sklearn.pipeline.Pipeline`).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">accept_sparse</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;csr&#39;</span><span class="p">,</span> <span class="s1">&#39;csc&#39;</span><span class="p">),</span> <span class="n">copy</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                    <span class="n">ensure_2d</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">FLOAT_DTYPES</span><span class="p">,</span>
                    <span class="n">force_all_finite</span><span class="o">=</span><span class="s1">&#39;allow-nan&#39;</span><span class="p">)</span>
    <span class="n">original_ndim</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">ndim</span>

    <span class="k">if</span> <span class="n">original_ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>

    <span class="n">s</span> <span class="o">=</span> <span class="n">RobustScaler</span><span class="p">(</span><span class="n">with_centering</span><span class="o">=</span><span class="n">with_centering</span><span class="p">,</span> <span class="n">with_scaling</span><span class="o">=</span><span class="n">with_scaling</span><span class="p">,</span>
                     <span class="n">quantile_range</span><span class="o">=</span><span class="n">quantile_range</span><span class="p">,</span>
                     <span class="n">unit_variance</span><span class="o">=</span><span class="n">unit_variance</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="n">copy</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">s</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">s</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>

    <span class="k">if</span> <span class="n">original_ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">X</span>


<span class="k">class</span> <span class="nc">PolynomialFeatures</span><span class="p">(</span><span class="n">TransformerMixin</span><span class="p">,</span> <span class="n">BaseEstimator</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Generate polynomial and interaction features.</span>

<span class="sd">    Generate a new feature matrix consisting of all polynomial combinations</span>
<span class="sd">    of the features with degree less than or equal to the specified degree.</span>
<span class="sd">    For example, if an input sample is two dimensional and of the form</span>
<span class="sd">    [a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2].</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    degree : int, default=2</span>
<span class="sd">        The degree of the polynomial features.</span>

<span class="sd">    interaction_only : bool, default=False</span>
<span class="sd">        If true, only interaction features are produced: features that are</span>
<span class="sd">        products of at most ``degree`` *distinct* input features (so not</span>
<span class="sd">        ``x[1] ** 2``, ``x[0] * x[2] ** 3``, etc.).</span>

<span class="sd">    include_bias : bool, default=True</span>
<span class="sd">        If True (default), then include a bias column, the feature in which</span>
<span class="sd">        all polynomial powers are zero (i.e. a column of ones - acts as an</span>
<span class="sd">        intercept term in a linear model).</span>

<span class="sd">    order : {&#39;C&#39;, &#39;F&#39;}, default=&#39;C&#39;</span>
<span class="sd">        Order of output array in the dense case. &#39;F&#39; order is faster to</span>
<span class="sd">        compute, but may slow down subsequent estimators.</span>

<span class="sd">        .. versionadded:: 0.21</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import numpy as np</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.preprocessing import PolynomialFeatures</span>
<span class="sd">    &gt;&gt;&gt; X = np.arange(6).reshape(3, 2)</span>
<span class="sd">    &gt;&gt;&gt; X</span>
<span class="sd">    array([[0, 1],</span>
<span class="sd">           [2, 3],</span>
<span class="sd">           [4, 5]])</span>
<span class="sd">    &gt;&gt;&gt; poly = PolynomialFeatures(2)</span>
<span class="sd">    &gt;&gt;&gt; poly.fit_transform(X)</span>
<span class="sd">    array([[ 1.,  0.,  1.,  0.,  0.,  1.],</span>
<span class="sd">           [ 1.,  2.,  3.,  4.,  6.,  9.],</span>
<span class="sd">           [ 1.,  4.,  5., 16., 20., 25.]])</span>
<span class="sd">    &gt;&gt;&gt; poly = PolynomialFeatures(interaction_only=True)</span>
<span class="sd">    &gt;&gt;&gt; poly.fit_transform(X)</span>
<span class="sd">    array([[ 1.,  0.,  1.,  0.],</span>
<span class="sd">           [ 1.,  2.,  3.,  6.],</span>
<span class="sd">           [ 1.,  4.,  5., 20.]])</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    powers_ : ndarray of shape (n_output_features, n_input_features)</span>
<span class="sd">        powers_[i, j] is the exponent of the jth input in the ith output.</span>

<span class="sd">    n_input_features_ : int</span>
<span class="sd">        The total number of input features.</span>

<span class="sd">    n_output_features_ : int</span>
<span class="sd">        The total number of polynomial output features. The number of output</span>
<span class="sd">        features is computed by iterating over all suitably sized combinations</span>
<span class="sd">        of input features.</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    Be aware that the number of features in the output array scales</span>
<span class="sd">    polynomially in the number of features of the input array, and</span>
<span class="sd">    exponentially in the degree. High degrees can cause overfitting.</span>

<span class="sd">    See :ref:`examples/linear_model/plot_polynomial_interpolation.py</span>
<span class="sd">    &lt;sphx_glr_auto_examples_linear_model_plot_polynomial_interpolation.py&gt;`</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@_deprecate_positional_args</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">degree</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">interaction_only</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">include_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">order</span><span class="o">=</span><span class="s1">&#39;C&#39;</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">degree</span> <span class="o">=</span> <span class="n">degree</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">interaction_only</span> <span class="o">=</span> <span class="n">interaction_only</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">include_bias</span> <span class="o">=</span> <span class="n">include_bias</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">order</span> <span class="o">=</span> <span class="n">order</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_combinations</span><span class="p">(</span><span class="n">n_features</span><span class="p">,</span> <span class="n">degree</span><span class="p">,</span> <span class="n">interaction_only</span><span class="p">,</span> <span class="n">include_bias</span><span class="p">):</span>
        <span class="n">comb</span> <span class="o">=</span> <span class="p">(</span><span class="n">combinations</span> <span class="k">if</span> <span class="n">interaction_only</span> <span class="k">else</span> <span class="n">combinations_w_r</span><span class="p">)</span>
        <span class="n">start</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="ow">not</span> <span class="n">include_bias</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">chain</span><span class="o">.</span><span class="n">from_iterable</span><span class="p">(</span><span class="n">comb</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n_features</span><span class="p">),</span> <span class="n">i</span><span class="p">)</span>
                                   <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">degree</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">powers_</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

        <span class="n">combinations</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_combinations</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_input_features_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">degree</span><span class="p">,</span>
                                          <span class="bp">self</span><span class="o">.</span><span class="n">interaction_only</span><span class="p">,</span>
                                          <span class="bp">self</span><span class="o">.</span><span class="n">include_bias</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">bincount</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">minlength</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_input_features_</span><span class="p">)</span>
                          <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">combinations</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">get_feature_names</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_features</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return feature names for output features</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input_features : list of str of shape (n_features,), default=None</span>
<span class="sd">            String names for input features if available. By default,</span>
<span class="sd">            &quot;x0&quot;, &quot;x1&quot;, ... &quot;xn_features&quot; is used.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        output_feature_names : list of str of shape (n_output_features,)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">powers</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">powers_</span>
        <span class="k">if</span> <span class="n">input_features</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">input_features</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;x</span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">powers</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])]</span>
        <span class="n">feature_names</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">powers</span><span class="p">:</span>
            <span class="n">inds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">row</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">inds</span><span class="p">):</span>
                <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot; &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%s</span><span class="s2">^</span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">input_features</span><span class="p">[</span><span class="n">ind</span><span class="p">],</span> <span class="n">exp</span><span class="p">)</span>
                                <span class="k">if</span> <span class="n">exp</span> <span class="o">!=</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">input_features</span><span class="p">[</span><span class="n">ind</span><span class="p">]</span>
                                <span class="k">for</span> <span class="n">ind</span><span class="p">,</span> <span class="n">exp</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">inds</span><span class="p">,</span> <span class="n">row</span><span class="p">[</span><span class="n">inds</span><span class="p">]))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;1&quot;</span>
            <span class="n">feature_names</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">feature_names</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compute number of output features.</span>


<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            The data.</span>

<span class="sd">        y : None</span>
<span class="sd">            Ignored.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        self : object</span>
<span class="sd">            Fitted transformer.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_data</span><span class="p">(</span>
            <span class="n">X</span><span class="p">,</span> <span class="n">accept_sparse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">combinations</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_combinations</span><span class="p">(</span><span class="n">n_features</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">degree</span><span class="p">,</span>
                                          <span class="bp">self</span><span class="o">.</span><span class="n">interaction_only</span><span class="p">,</span>
                                          <span class="bp">self</span><span class="o">.</span><span class="n">include_bias</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_input_features_</span> <span class="o">=</span> <span class="n">n_features</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_output_features_</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="mi">1</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">combinations</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Transform data to polynomial features</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            The data to transform, row by row.</span>

<span class="sd">            Prefer CSR over CSC for sparse input (for speed), but CSC is</span>
<span class="sd">            required if the degree is 4 or higher. If the degree is less than</span>
<span class="sd">            4 and the input format is CSC, it will be converted to CSR, have</span>
<span class="sd">            its polynomial features generated, then converted back to CSC.</span>

<span class="sd">            If the degree is 2 or 3, the method described in &quot;Leveraging</span>
<span class="sd">            Sparsity to Speed Up Polynomial Feature Expansions of CSR Matrices</span>
<span class="sd">            Using K-Simplex Numbers&quot; by Andrew Nystrom and John Hughes is</span>
<span class="sd">            used, which is much faster than the method used on CSC input. For</span>
<span class="sd">            this reason, a CSC input will be converted to CSR, and the output</span>
<span class="sd">            will be converted back to CSC prior to being returned, hence the</span>
<span class="sd">            preference of CSR.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        XP : {ndarray, sparse matrix} of shape (n_samples, NP)</span>
<span class="sd">            The matrix of features, where NP is the number of polynomial</span>
<span class="sd">            features generated from the combination of inputs. If a sparse</span>
<span class="sd">            matrix is provided, it will be converted into a sparse</span>
<span class="sd">            ``csr_matrix``.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_data</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="s1">&#39;F&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">FLOAT_DTYPES</span><span class="p">,</span> <span class="n">reset</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                <span class="n">accept_sparse</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;csr&#39;</span><span class="p">,</span> <span class="s1">&#39;csc&#39;</span><span class="p">))</span>

        <span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>

        <span class="k">if</span> <span class="n">n_features</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_input_features_</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;X shape does not match training shape&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">sparse</span><span class="o">.</span><span class="n">isspmatrix_csr</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">degree</span> <span class="o">&gt;</span> <span class="mi">3</span><span class="p">:</span>
                <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">tocsc</span><span class="p">())</span><span class="o">.</span><span class="n">tocsr</span><span class="p">()</span>
            <span class="n">to_stack</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">include_bias</span><span class="p">:</span>
                <span class="n">to_stack</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>
            <span class="n">to_stack</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">deg</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">degree</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
                <span class="n">Xp_next</span> <span class="o">=</span> <span class="n">_csr_polynomial_expansion</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span>
                                                    <span class="n">X</span><span class="o">.</span><span class="n">indptr</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                                                    <span class="bp">self</span><span class="o">.</span><span class="n">interaction_only</span><span class="p">,</span>
                                                    <span class="n">deg</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">Xp_next</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="k">break</span>
                <span class="n">to_stack</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Xp_next</span><span class="p">)</span>
            <span class="n">XP</span> <span class="o">=</span> <span class="n">sparse</span><span class="o">.</span><span class="n">hstack</span><span class="p">(</span><span class="n">to_stack</span><span class="p">,</span> <span class="nb">format</span><span class="o">=</span><span class="s1">&#39;csr&#39;</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">sparse</span><span class="o">.</span><span class="n">isspmatrix_csc</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">degree</span> <span class="o">&lt;</span> <span class="mi">4</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">tocsr</span><span class="p">())</span><span class="o">.</span><span class="n">tocsc</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">sparse</span><span class="o">.</span><span class="n">isspmatrix</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
                <span class="n">combinations</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_combinations</span><span class="p">(</span><span class="n">n_features</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">degree</span><span class="p">,</span>
                                                  <span class="bp">self</span><span class="o">.</span><span class="n">interaction_only</span><span class="p">,</span>
                                                  <span class="bp">self</span><span class="o">.</span><span class="n">include_bias</span><span class="p">)</span>
                <span class="n">columns</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="k">for</span> <span class="n">comb</span> <span class="ow">in</span> <span class="n">combinations</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">comb</span><span class="p">:</span>
                        <span class="n">out_col</span> <span class="o">=</span> <span class="mi">1</span>
                        <span class="k">for</span> <span class="n">col_idx</span> <span class="ow">in</span> <span class="n">comb</span><span class="p">:</span>
                            <span class="n">out_col</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="n">col_idx</span><span class="p">]</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">out_col</span><span class="p">)</span>
                        <span class="n">columns</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out_col</span><span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">bias</span> <span class="o">=</span> <span class="n">sparse</span><span class="o">.</span><span class="n">csc_matrix</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">)))</span>
                        <span class="n">columns</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">bias</span><span class="p">)</span>
                <span class="n">XP</span> <span class="o">=</span> <span class="n">sparse</span><span class="o">.</span><span class="n">hstack</span><span class="p">(</span><span class="n">columns</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">tocsc</span><span class="p">()</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">XP</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_output_features_</span><span class="p">),</span>
                              <span class="n">dtype</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">order</span><span class="p">)</span>

                <span class="c1"># What follows is a faster implementation of:</span>
                <span class="c1"># for i, comb in enumerate(combinations):</span>
                <span class="c1">#     XP[:, i] = X[:, comb].prod(1)</span>
                <span class="c1"># This implementation uses two optimisations.</span>
                <span class="c1"># First one is broadcasting,</span>
                <span class="c1"># multiply ([X1, ..., Xn], X1) -&gt; [X1 X1, ..., Xn X1]</span>
                <span class="c1"># multiply ([X2, ..., Xn], X2) -&gt; [X2 X2, ..., Xn X2]</span>
                <span class="c1"># ...</span>
                <span class="c1"># multiply ([X[:, start:end], X[:, start]) -&gt; ...</span>
                <span class="c1"># Second optimisation happens for degrees &gt;= 3.</span>
                <span class="c1"># Xi^3 is computed reusing previous computation:</span>
                <span class="c1"># Xi^3 = Xi^2 * Xi.</span>

                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">include_bias</span><span class="p">:</span>
                    <span class="n">XP</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
                    <span class="n">current_col</span> <span class="o">=</span> <span class="mi">1</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">current_col</span> <span class="o">=</span> <span class="mi">0</span>

                <span class="c1"># d = 0</span>
                <span class="n">XP</span><span class="p">[:,</span> <span class="n">current_col</span><span class="p">:</span><span class="n">current_col</span> <span class="o">+</span> <span class="n">n_features</span><span class="p">]</span> <span class="o">=</span> <span class="n">X</span>
                <span class="n">index</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">current_col</span><span class="p">,</span>
                                   <span class="n">current_col</span> <span class="o">+</span> <span class="n">n_features</span><span class="p">))</span>
                <span class="n">current_col</span> <span class="o">+=</span> <span class="n">n_features</span>
                <span class="n">index</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">current_col</span><span class="p">)</span>

                <span class="c1"># d &gt;= 1</span>
                <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">degree</span><span class="p">):</span>
                    <span class="n">new_index</span> <span class="o">=</span> <span class="p">[]</span>
                    <span class="n">end</span> <span class="o">=</span> <span class="n">index</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
                    <span class="k">for</span> <span class="n">feature_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_features</span><span class="p">):</span>
                        <span class="n">start</span> <span class="o">=</span> <span class="n">index</span><span class="p">[</span><span class="n">feature_idx</span><span class="p">]</span>
                        <span class="n">new_index</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">current_col</span><span class="p">)</span>
                        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">interaction_only</span><span class="p">:</span>
                            <span class="n">start</span> <span class="o">+=</span> <span class="p">(</span><span class="n">index</span><span class="p">[</span><span class="n">feature_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">-</span>
                                      <span class="n">index</span><span class="p">[</span><span class="n">feature_idx</span><span class="p">])</span>
                        <span class="n">next_col</span> <span class="o">=</span> <span class="n">current_col</span> <span class="o">+</span> <span class="n">end</span> <span class="o">-</span> <span class="n">start</span>
                        <span class="k">if</span> <span class="n">next_col</span> <span class="o">&lt;=</span> <span class="n">current_col</span><span class="p">:</span>
                            <span class="k">break</span>
                        <span class="c1"># XP[:, start:end] are terms of degree d - 1</span>
                        <span class="c1"># that exclude feature #feature_idx.</span>
                        <span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">XP</span><span class="p">[:,</span> <span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">],</span>
                                    <span class="n">X</span><span class="p">[:,</span> <span class="n">feature_idx</span><span class="p">:</span><span class="n">feature_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">],</span>
                                    <span class="n">out</span><span class="o">=</span><span class="n">XP</span><span class="p">[:,</span> <span class="n">current_col</span><span class="p">:</span><span class="n">next_col</span><span class="p">],</span>
                                    <span class="n">casting</span><span class="o">=</span><span class="s1">&#39;no&#39;</span><span class="p">)</span>
                        <span class="n">current_col</span> <span class="o">=</span> <span class="n">next_col</span>

                    <span class="n">new_index</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">current_col</span><span class="p">)</span>
                    <span class="n">index</span> <span class="o">=</span> <span class="n">new_index</span>

        <span class="k">return</span> <span class="n">XP</span>


<span class="nd">@_deprecate_positional_args</span>
<span class="k">def</span> <span class="nf">normalize</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_norm</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Scale input vectors individually to unit norm (vector length).</span>

<span class="sd">    Read more in the :ref:`User Guide &lt;preprocessing_normalization&gt;`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">        The data to normalize, element by element.</span>
<span class="sd">        scipy.sparse matrices should be in CSR format to avoid an</span>
<span class="sd">        un-necessary copy.</span>

<span class="sd">    norm : {&#39;l1&#39;, &#39;l2&#39;, &#39;max&#39;}, default=&#39;l2&#39;</span>
<span class="sd">        The norm to use to normalize each non zero sample (or each non-zero</span>
<span class="sd">        feature if axis is 0).</span>

<span class="sd">    axis : {0, 1}, default=1</span>
<span class="sd">        axis used to normalize the data along. If 1, independently normalize</span>
<span class="sd">        each sample, otherwise (if 0) normalize each feature.</span>

<span class="sd">    copy : bool, default=True</span>
<span class="sd">        set to False to perform inplace row normalization and avoid a</span>
<span class="sd">        copy (if the input is already a numpy array or a scipy.sparse</span>
<span class="sd">        CSR matrix and if axis is 1).</span>

<span class="sd">    return_norm : bool, default=False</span>
<span class="sd">        whether to return the computed norms</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    X : {ndarray, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">        Normalized input X.</span>

<span class="sd">    norms : ndarray of shape (n_samples, ) if axis=1 else (n_features, )</span>
<span class="sd">        An array of norms along given axis for X.</span>
<span class="sd">        When X is sparse, a NotImplementedError will be raised</span>
<span class="sd">        for norm &#39;l1&#39; or &#39;l2&#39;.</span>

<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    Normalizer : Performs normalization using the Transformer API</span>
<span class="sd">        (e.g. as part of a preprocessing :class:`~sklearn.pipeline.Pipeline`).</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    For a comparison of the different scalers, transformers, and normalizers,</span>
<span class="sd">    see :ref:`examples/preprocessing/plot_all_scaling.py</span>
<span class="sd">    &lt;sphx_glr_auto_examples_preprocessing_plot_all_scaling.py&gt;`.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">norm</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="s1">&#39;max&#39;</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;&#39;</span><span class="si">%s</span><span class="s2">&#39; is not a supported norm&quot;</span> <span class="o">%</span> <span class="n">norm</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">axis</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">sparse_format</span> <span class="o">=</span> <span class="s1">&#39;csc&#39;</span>
    <span class="k">elif</span> <span class="n">axis</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">sparse_format</span> <span class="o">=</span> <span class="s1">&#39;csr&#39;</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;&#39;</span><span class="si">%d</span><span class="s2">&#39; is not a supported axis&quot;</span> <span class="o">%</span> <span class="n">axis</span><span class="p">)</span>

    <span class="n">X</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">accept_sparse</span><span class="o">=</span><span class="n">sparse_format</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="n">copy</span><span class="p">,</span>
                    <span class="n">estimator</span><span class="o">=</span><span class="s1">&#39;the normalize function&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">FLOAT_DTYPES</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span>

    <span class="k">if</span> <span class="n">sparse</span><span class="o">.</span><span class="n">issparse</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">return_norm</span> <span class="ow">and</span> <span class="n">norm</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="s1">&#39;l2&#39;</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;return_norm=True is not implemented &quot;</span>
                                      <span class="s2">&quot;for sparse matrices with norm &#39;l1&#39; &quot;</span>
                                      <span class="s2">&quot;or norm &#39;l2&#39;&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">norm</span> <span class="o">==</span> <span class="s1">&#39;l1&#39;</span><span class="p">:</span>
            <span class="n">inplace_csr_row_normalize_l1</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">norm</span> <span class="o">==</span> <span class="s1">&#39;l2&#39;</span><span class="p">:</span>
            <span class="n">inplace_csr_row_normalize_l2</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">norm</span> <span class="o">==</span> <span class="s1">&#39;max&#39;</span><span class="p">:</span>
            <span class="n">mins</span><span class="p">,</span> <span class="n">maxes</span> <span class="o">=</span> <span class="n">min_max_axis</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">norms</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">mins</span><span class="p">),</span> <span class="n">maxes</span><span class="p">)</span>
            <span class="n">norms_elementwise</span> <span class="o">=</span> <span class="n">norms</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diff</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">indptr</span><span class="p">))</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="n">norms_elementwise</span> <span class="o">!=</span> <span class="mi">0</span>
            <span class="n">X</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span> <span class="o">/=</span> <span class="n">norms_elementwise</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">norm</span> <span class="o">==</span> <span class="s1">&#39;l1&#39;</span><span class="p">:</span>
            <span class="n">norms</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">norm</span> <span class="o">==</span> <span class="s1">&#39;l2&#39;</span><span class="p">:</span>
            <span class="n">norms</span> <span class="o">=</span> <span class="n">row_norms</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">norm</span> <span class="o">==</span> <span class="s1">&#39;max&#39;</span><span class="p">:</span>
            <span class="n">norms</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">norms</span> <span class="o">=</span> <span class="n">_handle_zeros_in_scale</span><span class="p">(</span><span class="n">norms</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">X</span> <span class="o">/=</span> <span class="n">norms</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">axis</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span>

    <span class="k">if</span> <span class="n">return_norm</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">norms</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">X</span>


<span class="k">class</span> <span class="nc">Normalizer</span><span class="p">(</span><span class="n">TransformerMixin</span><span class="p">,</span> <span class="n">BaseEstimator</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Normalize samples individually to unit norm.</span>

<span class="sd">    Each sample (i.e. each row of the data matrix) with at least one</span>
<span class="sd">    non zero component is rescaled independently of other samples so</span>
<span class="sd">    that its norm (l1, l2 or inf) equals one.</span>

<span class="sd">    This transformer is able to work both with dense numpy arrays and</span>
<span class="sd">    scipy.sparse matrix (use CSR format if you want to avoid the burden of</span>
<span class="sd">    a copy / conversion).</span>

<span class="sd">    Scaling inputs to unit norms is a common operation for text</span>
<span class="sd">    classification or clustering for instance. For instance the dot</span>
<span class="sd">    product of two l2-normalized TF-IDF vectors is the cosine similarity</span>
<span class="sd">    of the vectors and is the base similarity metric for the Vector</span>
<span class="sd">    Space Model commonly used by the Information Retrieval community.</span>

<span class="sd">    Read more in the :ref:`User Guide &lt;preprocessing_normalization&gt;`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    norm : {&#39;l1&#39;, &#39;l2&#39;, &#39;max&#39;}, default=&#39;l2&#39;</span>
<span class="sd">        The norm to use to normalize each non zero sample. If norm=&#39;max&#39;</span>
<span class="sd">        is used, values will be rescaled by the maximum of the absolute</span>
<span class="sd">        values.</span>

<span class="sd">    copy : bool, default=True</span>
<span class="sd">        set to False to perform inplace row normalization and avoid a</span>
<span class="sd">        copy (if the input is already a numpy array or a scipy.sparse</span>
<span class="sd">        CSR matrix).</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.preprocessing import Normalizer</span>
<span class="sd">    &gt;&gt;&gt; X = [[4, 1, 2, 2],</span>
<span class="sd">    ...      [1, 3, 9, 3],</span>
<span class="sd">    ...      [5, 7, 5, 1]]</span>
<span class="sd">    &gt;&gt;&gt; transformer = Normalizer().fit(X)  # fit does nothing.</span>
<span class="sd">    &gt;&gt;&gt; transformer</span>
<span class="sd">    Normalizer()</span>
<span class="sd">    &gt;&gt;&gt; transformer.transform(X)</span>
<span class="sd">    array([[0.8, 0.2, 0.4, 0.4],</span>
<span class="sd">           [0.1, 0.3, 0.9, 0.3],</span>
<span class="sd">           [0.5, 0.7, 0.5, 0.1]])</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    This estimator is stateless (besides constructor parameters), the</span>
<span class="sd">    fit method does nothing but is useful when used in a pipeline.</span>

<span class="sd">    For a comparison of the different scalers, transformers, and normalizers,</span>
<span class="sd">    see :ref:`examples/preprocessing/plot_all_scaling.py</span>
<span class="sd">    &lt;sphx_glr_auto_examples_preprocessing_plot_all_scaling.py&gt;`.</span>

<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    normalize : Equivalent function without the estimator API.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@_deprecate_positional_args</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">norm</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">copy</span> <span class="o">=</span> <span class="n">copy</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Do nothing and return the estimator unchanged</span>

<span class="sd">        This method is just there to implement the usual API and hence</span>
<span class="sd">        work in pipelines.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            The data to estimate the normalization parameters.</span>

<span class="sd">        y : None</span>
<span class="sd">            Ignored.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        self : object</span>
<span class="sd">            Fitted transformer.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_validate_data</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">accept_sparse</span><span class="o">=</span><span class="s1">&#39;csr&#39;</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Scale each non zero row of X to unit norm</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            The data to normalize, row by row. scipy.sparse matrices should be</span>
<span class="sd">            in CSR format to avoid an un-necessary copy.</span>

<span class="sd">        copy : bool, default=None</span>
<span class="sd">            Copy the input X or not.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        X_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            Transformed array.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">copy</span> <span class="o">=</span> <span class="n">copy</span> <span class="k">if</span> <span class="n">copy</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">copy</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_data</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">accept_sparse</span><span class="o">=</span><span class="s1">&#39;csr&#39;</span><span class="p">,</span> <span class="n">reset</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">normalize</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="n">copy</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_more_tags</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;stateless&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">}</span>


<span class="nd">@_deprecate_positional_args</span>
<span class="k">def</span> <span class="nf">binarize</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Boolean thresholding of array-like or scipy.sparse matrix.</span>

<span class="sd">    Read more in the :ref:`User Guide &lt;preprocessing_binarization&gt;`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">        The data to binarize, element by element.</span>
<span class="sd">        scipy.sparse matrices should be in CSR or CSC format to avoid an</span>
<span class="sd">        un-necessary copy.</span>

<span class="sd">    threshold : float, default=0.0</span>
<span class="sd">        Feature values below or equal to this are replaced by 0, above it by 1.</span>
<span class="sd">        Threshold may not be less than 0 for operations on sparse matrices.</span>

<span class="sd">    copy : bool, default=True</span>
<span class="sd">        set to False to perform inplace binarization and avoid a copy</span>
<span class="sd">        (if the input is already a numpy array or a scipy.sparse CSR / CSC</span>
<span class="sd">        matrix and if axis is 1).</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    X_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">        The transformed data.</span>

<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    Binarizer : Performs binarization using the Transformer API</span>
<span class="sd">        (e.g. as part of a preprocessing :class:`~sklearn.pipeline.Pipeline`).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">accept_sparse</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;csr&#39;</span><span class="p">,</span> <span class="s1">&#39;csc&#39;</span><span class="p">],</span> <span class="n">copy</span><span class="o">=</span><span class="n">copy</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">sparse</span><span class="o">.</span><span class="n">issparse</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">threshold</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Cannot binarize a sparse matrix with threshold &#39;</span>
                             <span class="s1">&#39;&lt; 0&#39;</span><span class="p">)</span>
        <span class="n">cond</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">data</span> <span class="o">&gt;</span> <span class="n">threshold</span>
        <span class="n">not_cond</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logical_not</span><span class="p">(</span><span class="n">cond</span><span class="p">)</span>
        <span class="n">X</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">cond</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">X</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">not_cond</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">X</span><span class="o">.</span><span class="n">eliminate_zeros</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">cond</span> <span class="o">=</span> <span class="n">X</span> <span class="o">&gt;</span> <span class="n">threshold</span>
        <span class="n">not_cond</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logical_not</span><span class="p">(</span><span class="n">cond</span><span class="p">)</span>
        <span class="n">X</span><span class="p">[</span><span class="n">cond</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">X</span><span class="p">[</span><span class="n">not_cond</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">return</span> <span class="n">X</span>


<span class="k">class</span> <span class="nc">Binarizer</span><span class="p">(</span><span class="n">TransformerMixin</span><span class="p">,</span> <span class="n">BaseEstimator</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Binarize data (set feature values to 0 or 1) according to a threshold.</span>

<span class="sd">    Values greater than the threshold map to 1, while values less than</span>
<span class="sd">    or equal to the threshold map to 0. With the default threshold of 0,</span>
<span class="sd">    only positive values map to 1.</span>

<span class="sd">    Binarization is a common operation on text count data where the</span>
<span class="sd">    analyst can decide to only consider the presence or absence of a</span>
<span class="sd">    feature rather than a quantified number of occurrences for instance.</span>

<span class="sd">    It can also be used as a pre-processing step for estimators that</span>
<span class="sd">    consider boolean random variables (e.g. modelled using the Bernoulli</span>
<span class="sd">    distribution in a Bayesian setting).</span>

<span class="sd">    Read more in the :ref:`User Guide &lt;preprocessing_binarization&gt;`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    threshold : float, default=0.0</span>
<span class="sd">        Feature values below or equal to this are replaced by 0, above it by 1.</span>
<span class="sd">        Threshold may not be less than 0 for operations on sparse matrices.</span>

<span class="sd">    copy : bool, default=True</span>
<span class="sd">        set to False to perform inplace binarization and avoid a copy (if</span>
<span class="sd">        the input is already a numpy array or a scipy.sparse CSR matrix).</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.preprocessing import Binarizer</span>
<span class="sd">    &gt;&gt;&gt; X = [[ 1., -1.,  2.],</span>
<span class="sd">    ...      [ 2.,  0.,  0.],</span>
<span class="sd">    ...      [ 0.,  1., -1.]]</span>
<span class="sd">    &gt;&gt;&gt; transformer = Binarizer().fit(X)  # fit does nothing.</span>
<span class="sd">    &gt;&gt;&gt; transformer</span>
<span class="sd">    Binarizer()</span>
<span class="sd">    &gt;&gt;&gt; transformer.transform(X)</span>
<span class="sd">    array([[1., 0., 1.],</span>
<span class="sd">           [1., 0., 0.],</span>
<span class="sd">           [0., 1., 0.]])</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    If the input is a sparse matrix, only the non-zero values are subject</span>
<span class="sd">    to update by the Binarizer class.</span>

<span class="sd">    This estimator is stateless (besides constructor parameters), the</span>
<span class="sd">    fit method does nothing but is useful when used in a pipeline.</span>

<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    binarize : Equivalent function without the estimator API.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@_deprecate_positional_args</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">threshold</span> <span class="o">=</span> <span class="n">threshold</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">copy</span> <span class="o">=</span> <span class="n">copy</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Do nothing and return the estimator unchanged.</span>

<span class="sd">        This method is just there to implement the usual API and hence</span>
<span class="sd">        work in pipelines.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            The data.</span>

<span class="sd">        y : None</span>
<span class="sd">            Ignored.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        self : object</span>
<span class="sd">            Fitted transformer.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_validate_data</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">accept_sparse</span><span class="o">=</span><span class="s1">&#39;csr&#39;</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Binarize each element of X.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            The data to binarize, element by element.</span>
<span class="sd">            scipy.sparse matrices should be in CSR format to avoid an</span>
<span class="sd">            un-necessary copy.</span>

<span class="sd">        copy : bool</span>
<span class="sd">            Copy the input X or not.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        X_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            Transformed array.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">copy</span> <span class="o">=</span> <span class="n">copy</span> <span class="k">if</span> <span class="n">copy</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">copy</span>
        <span class="c1"># TODO: This should be refactored because binarize also calls</span>
        <span class="c1"># check_array</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_data</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">accept_sparse</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;csr&#39;</span><span class="p">,</span> <span class="s1">&#39;csc&#39;</span><span class="p">],</span> <span class="n">copy</span><span class="o">=</span><span class="n">copy</span><span class="p">,</span>
                                <span class="n">reset</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">binarize</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">threshold</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_more_tags</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;stateless&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">}</span>


<span class="k">class</span> <span class="nc">KernelCenterer</span><span class="p">(</span><span class="n">TransformerMixin</span><span class="p">,</span> <span class="n">BaseEstimator</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Center a kernel matrix.</span>

<span class="sd">    Let K(x, z) be a kernel defined by phi(x)^T phi(z), where phi is a</span>
<span class="sd">    function mapping x to a Hilbert space. KernelCenterer centers (i.e.,</span>
<span class="sd">    normalize to have zero mean) the data without explicitly computing phi(x).</span>
<span class="sd">    It is equivalent to centering phi(x) with</span>
<span class="sd">    sklearn.preprocessing.StandardScaler(with_std=False).</span>

<span class="sd">    Read more in the :ref:`User Guide &lt;kernel_centering&gt;`.</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    K_fit_rows_ : array of shape (n_samples,)</span>
<span class="sd">        Average of each column of kernel matrix.</span>

<span class="sd">    K_fit_all_ : float</span>
<span class="sd">        Average of kernel matrix.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.preprocessing import KernelCenterer</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.metrics.pairwise import pairwise_kernels</span>
<span class="sd">    &gt;&gt;&gt; X = [[ 1., -2.,  2.],</span>
<span class="sd">    ...      [ -2.,  1.,  3.],</span>
<span class="sd">    ...      [ 4.,  1., -2.]]</span>
<span class="sd">    &gt;&gt;&gt; K = pairwise_kernels(X, metric=&#39;linear&#39;)</span>
<span class="sd">    &gt;&gt;&gt; K</span>
<span class="sd">    array([[  9.,   2.,  -2.],</span>
<span class="sd">           [  2.,  14., -13.],</span>
<span class="sd">           [ -2., -13.,  21.]])</span>
<span class="sd">    &gt;&gt;&gt; transformer = KernelCenterer().fit(K)</span>
<span class="sd">    &gt;&gt;&gt; transformer</span>
<span class="sd">    KernelCenterer()</span>
<span class="sd">    &gt;&gt;&gt; transformer.transform(K)</span>
<span class="sd">    array([[  5.,   0.,  -5.],</span>
<span class="sd">           [  0.,  14., -14.],</span>
<span class="sd">           [ -5., -14.,  19.]])</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># Needed for backported inspect.signature compatibility with PyPy</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Fit KernelCenterer</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        K : ndarray of shape (n_samples, n_samples)</span>
<span class="sd">            Kernel matrix.</span>

<span class="sd">        y : None</span>
<span class="sd">            Ignored.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        self : object</span>
<span class="sd">            Fitted transformer.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">K</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_data</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">FLOAT_DTYPES</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">K</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="n">K</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Kernel matrix must be a square matrix.&quot;</span>
                             <span class="s2">&quot; Input is a </span><span class="si">{}</span><span class="s2">x</span><span class="si">{}</span><span class="s2"> matrix.&quot;</span>
                             <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">K</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>

        <span class="n">n_samples</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">K_fit_rows_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">/</span> <span class="n">n_samples</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">K_fit_all_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">K_fit_rows_</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">n_samples</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Center kernel matrix.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        K : ndarray of shape (n_samples1, n_samples2)</span>
<span class="sd">            Kernel matrix.</span>

<span class="sd">        copy : bool, default=True</span>
<span class="sd">            Set to False to perform inplace computation.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        K_new : ndarray of shape (n_samples1, n_samples2)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

        <span class="n">K</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_data</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="n">copy</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">FLOAT_DTYPES</span><span class="p">,</span> <span class="n">reset</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="n">K_pred_cols</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span>
                       <span class="bp">self</span><span class="o">.</span><span class="n">K_fit_rows_</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>

        <span class="n">K</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">K_fit_rows_</span>
        <span class="n">K</span> <span class="o">-=</span> <span class="n">K_pred_cols</span>
        <span class="n">K</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">K_fit_all_</span>

        <span class="k">return</span> <span class="n">K</span>

    <span class="k">def</span> <span class="nf">_more_tags</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;pairwise&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">}</span>

    <span class="c1"># TODO: Remove in 1.1</span>
    <span class="c1"># mypy error: Decorated property not supported</span>
    <span class="nd">@deprecated</span><span class="p">(</span><span class="s2">&quot;Attribute _pairwise was deprecated in &quot;</span>  <span class="c1"># type: ignore</span>
                <span class="s2">&quot;version 0.24 and will be removed in 1.1.&quot;</span><span class="p">)</span>
    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">_pairwise</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="kc">True</span>


<span class="k">def</span> <span class="nf">add_dummy_feature</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Augment dataset with an additional dummy feature.</span>

<span class="sd">    This is useful for fitting an intercept term with implementations which</span>
<span class="sd">    cannot otherwise fit it directly.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">        Data.</span>

<span class="sd">    value : float</span>
<span class="sd">        Value to use for the dummy feature.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    X : {ndarray, sparse matrix} of shape (n_samples, n_features + 1)</span>
<span class="sd">        Same data with dummy feature added as first column.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.preprocessing import add_dummy_feature</span>
<span class="sd">    &gt;&gt;&gt; add_dummy_feature([[0, 1], [1, 0]])</span>
<span class="sd">    array([[1., 0., 1.],</span>
<span class="sd">           [1., 1., 0.]])</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">accept_sparse</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;csc&#39;</span><span class="p">,</span> <span class="s1">&#39;csr&#39;</span><span class="p">,</span> <span class="s1">&#39;coo&#39;</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">FLOAT_DTYPES</span><span class="p">)</span>
    <span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">sparse</span><span class="o">.</span><span class="n">issparse</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">sparse</span><span class="o">.</span><span class="n">isspmatrix_coo</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
            <span class="c1"># Shift columns to the right.</span>
            <span class="n">col</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">col</span> <span class="o">+</span> <span class="mi">1</span>
            <span class="c1"># Column indices of dummy feature are 0 everywhere.</span>
            <span class="n">col</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_samples</span><span class="p">),</span> <span class="n">col</span><span class="p">))</span>
            <span class="c1"># Row indices of dummy feature are 0, ..., n_samples-1.</span>
            <span class="n">row</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_samples</span><span class="p">),</span> <span class="n">X</span><span class="o">.</span><span class="n">row</span><span class="p">))</span>
            <span class="c1"># Prepend the dummy feature n_samples times.</span>
            <span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">value</span><span class="p">),</span> <span class="n">X</span><span class="o">.</span><span class="n">data</span><span class="p">))</span>
            <span class="k">return</span> <span class="n">sparse</span><span class="o">.</span><span class="n">coo_matrix</span><span class="p">((</span><span class="n">data</span><span class="p">,</span> <span class="p">(</span><span class="n">row</span><span class="p">,</span> <span class="n">col</span><span class="p">)),</span> <span class="n">shape</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">sparse</span><span class="o">.</span><span class="n">isspmatrix_csc</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
            <span class="c1"># Shift index pointers since we need to add n_samples elements.</span>
            <span class="n">indptr</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">indptr</span> <span class="o">+</span> <span class="n">n_samples</span>
            <span class="c1"># indptr[0] must be 0.</span>
            <span class="n">indptr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">]),</span> <span class="n">indptr</span><span class="p">))</span>
            <span class="c1"># Row indices of dummy feature are 0, ..., n_samples-1.</span>
            <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_samples</span><span class="p">),</span> <span class="n">X</span><span class="o">.</span><span class="n">indices</span><span class="p">))</span>
            <span class="c1"># Prepend the dummy feature n_samples times.</span>
            <span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">value</span><span class="p">),</span> <span class="n">X</span><span class="o">.</span><span class="n">data</span><span class="p">))</span>
            <span class="k">return</span> <span class="n">sparse</span><span class="o">.</span><span class="n">csc_matrix</span><span class="p">((</span><span class="n">data</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">indptr</span><span class="p">),</span> <span class="n">shape</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">klass</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="vm">__class__</span>
            <span class="k">return</span> <span class="n">klass</span><span class="p">(</span><span class="n">add_dummy_feature</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">tocoo</span><span class="p">(),</span> <span class="n">value</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">value</span><span class="p">),</span> <span class="n">X</span><span class="p">))</span>


<span class="k">class</span> <span class="nc">QuantileTransformer</span><span class="p">(</span><span class="n">TransformerMixin</span><span class="p">,</span> <span class="n">BaseEstimator</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Transform features using quantiles information.</span>

<span class="sd">    This method transforms the features to follow a uniform or a normal</span>
<span class="sd">    distribution. Therefore, for a given feature, this transformation tends</span>
<span class="sd">    to spread out the most frequent values. It also reduces the impact of</span>
<span class="sd">    (marginal) outliers: this is therefore a robust preprocessing scheme.</span>

<span class="sd">    The transformation is applied on each feature independently. First an</span>
<span class="sd">    estimate of the cumulative distribution function of a feature is</span>
<span class="sd">    used to map the original values to a uniform distribution. The obtained</span>
<span class="sd">    values are then mapped to the desired output distribution using the</span>
<span class="sd">    associated quantile function. Features values of new/unseen data that fall</span>
<span class="sd">    below or above the fitted range will be mapped to the bounds of the output</span>
<span class="sd">    distribution. Note that this transform is non-linear. It may distort linear</span>
<span class="sd">    correlations between variables measured at the same scale but renders</span>
<span class="sd">    variables measured at different scales more directly comparable.</span>

<span class="sd">    Read more in the :ref:`User Guide &lt;preprocessing_transformer&gt;`.</span>

<span class="sd">    .. versionadded:: 0.19</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_quantiles : int, default=1000 or n_samples</span>
<span class="sd">        Number of quantiles to be computed. It corresponds to the number</span>
<span class="sd">        of landmarks used to discretize the cumulative distribution function.</span>
<span class="sd">        If n_quantiles is larger than the number of samples, n_quantiles is set</span>
<span class="sd">        to the number of samples as a larger number of quantiles does not give</span>
<span class="sd">        a better approximation of the cumulative distribution function</span>
<span class="sd">        estimator.</span>

<span class="sd">    output_distribution : {&#39;uniform&#39;, &#39;normal&#39;}, default=&#39;uniform&#39;</span>
<span class="sd">        Marginal distribution for the transformed data. The choices are</span>
<span class="sd">        &#39;uniform&#39; (default) or &#39;normal&#39;.</span>

<span class="sd">    ignore_implicit_zeros : bool, default=False</span>
<span class="sd">        Only applies to sparse matrices. If True, the sparse entries of the</span>
<span class="sd">        matrix are discarded to compute the quantile statistics. If False,</span>
<span class="sd">        these entries are treated as zeros.</span>

<span class="sd">    subsample : int, default=1e5</span>
<span class="sd">        Maximum number of samples used to estimate the quantiles for</span>
<span class="sd">        computational efficiency. Note that the subsampling procedure may</span>
<span class="sd">        differ for value-identical sparse and dense matrices.</span>

<span class="sd">    random_state : int, RandomState instance or None, default=None</span>
<span class="sd">        Determines random number generation for subsampling and smoothing</span>
<span class="sd">        noise.</span>
<span class="sd">        Please see ``subsample`` for more details.</span>
<span class="sd">        Pass an int for reproducible results across multiple function calls.</span>
<span class="sd">        See :term:`Glossary &lt;random_state&gt;`</span>

<span class="sd">    copy : bool, default=True</span>
<span class="sd">        Set to False to perform inplace transformation and avoid a copy (if the</span>
<span class="sd">        input is already a numpy array).</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    n_quantiles_ : int</span>
<span class="sd">        The actual number of quantiles used to discretize the cumulative</span>
<span class="sd">        distribution function.</span>

<span class="sd">    quantiles_ : ndarray of shape (n_quantiles, n_features)</span>
<span class="sd">        The values corresponding the quantiles of reference.</span>

<span class="sd">    references_ : ndarray of shape (n_quantiles, )</span>
<span class="sd">        Quantiles of references.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import numpy as np</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.preprocessing import QuantileTransformer</span>
<span class="sd">    &gt;&gt;&gt; rng = np.random.RandomState(0)</span>
<span class="sd">    &gt;&gt;&gt; X = np.sort(rng.normal(loc=0.5, scale=0.25, size=(25, 1)), axis=0)</span>
<span class="sd">    &gt;&gt;&gt; qt = QuantileTransformer(n_quantiles=10, random_state=0)</span>
<span class="sd">    &gt;&gt;&gt; qt.fit_transform(X)</span>
<span class="sd">    array([...])</span>

<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    quantile_transform : Equivalent function without the estimator API.</span>
<span class="sd">    PowerTransformer : Perform mapping to a normal distribution using a power</span>
<span class="sd">        transform.</span>
<span class="sd">    StandardScaler : Perform standardization that is faster, but less robust</span>
<span class="sd">        to outliers.</span>
<span class="sd">    RobustScaler : Perform robust standardization that removes the influence</span>
<span class="sd">        of outliers but does not put outliers and inliers on the same scale.</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    NaNs are treated as missing values: disregarded in fit, and maintained in</span>
<span class="sd">    transform.</span>

<span class="sd">    For a comparison of the different scalers, transformers, and normalizers,</span>
<span class="sd">    see :ref:`examples/preprocessing/plot_all_scaling.py</span>
<span class="sd">    &lt;sphx_glr_auto_examples_preprocessing_plot_all_scaling.py&gt;`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@_deprecate_positional_args</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">n_quantiles</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">output_distribution</span><span class="o">=</span><span class="s1">&#39;uniform&#39;</span><span class="p">,</span>
                 <span class="n">ignore_implicit_zeros</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">subsample</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="mf">1e5</span><span class="p">),</span>
                 <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_quantiles</span> <span class="o">=</span> <span class="n">n_quantiles</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_distribution</span> <span class="o">=</span> <span class="n">output_distribution</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ignore_implicit_zeros</span> <span class="o">=</span> <span class="n">ignore_implicit_zeros</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">subsample</span> <span class="o">=</span> <span class="n">subsample</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span> <span class="o">=</span> <span class="n">random_state</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">copy</span> <span class="o">=</span> <span class="n">copy</span>

    <span class="k">def</span> <span class="nf">_dense_fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">random_state</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Compute percentiles for dense matrices.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : ndarray of shape (n_samples, n_features)</span>
<span class="sd">            The data used to scale along the features axis.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">ignore_implicit_zeros</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;&#39;ignore_implicit_zeros&#39; takes effect only with&quot;</span>
                          <span class="s2">&quot; sparse matrix. This parameter has no effect.&quot;</span><span class="p">)</span>

        <span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">references</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">references_</span> <span class="o">*</span> <span class="mi">100</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">quantiles_</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">subsample</span> <span class="o">&lt;</span> <span class="n">n_samples</span><span class="p">:</span>
                <span class="n">subsample_idx</span> <span class="o">=</span> <span class="n">random_state</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span>
                                                    <span class="n">size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">subsample</span><span class="p">,</span>
                                                    <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
                <span class="n">col</span> <span class="o">=</span> <span class="n">col</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">subsample_idx</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;clip&#39;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">quantiles_</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">nanpercentile</span><span class="p">(</span><span class="n">col</span><span class="p">,</span> <span class="n">references</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">quantiles_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">quantiles_</span><span class="p">)</span>
        <span class="c1"># Due to floating-point precision error in `np.nanpercentile`,</span>
        <span class="c1"># make sure that quantiles are monotonically increasing.</span>
        <span class="c1"># Upstream issue in numpy:</span>
        <span class="c1"># https://github.com/numpy/numpy/issues/14685</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">quantiles_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="o">.</span><span class="n">accumulate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">quantiles_</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_sparse_fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">random_state</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Compute percentiles for sparse matrices.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : sparse matrix of shape (n_samples, n_features)</span>
<span class="sd">            The data used to scale along the features axis. The sparse matrix</span>
<span class="sd">            needs to be nonnegative. If a sparse matrix is provided,</span>
<span class="sd">            it will be converted into a sparse ``csc_matrix``.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">references</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">references_</span> <span class="o">*</span> <span class="mi">100</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">quantiles_</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">feature_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_features</span><span class="p">):</span>
            <span class="n">column_nnz_data</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">X</span><span class="o">.</span><span class="n">indptr</span><span class="p">[</span><span class="n">feature_idx</span><span class="p">]:</span>
                                     <span class="n">X</span><span class="o">.</span><span class="n">indptr</span><span class="p">[</span><span class="n">feature_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]]</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">column_nnz_data</span><span class="p">)</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">subsample</span><span class="p">:</span>
                <span class="n">column_subsample</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">subsample</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">column_nnz_data</span><span class="p">)</span> <span class="o">//</span>
                                    <span class="n">n_samples</span><span class="p">)</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">ignore_implicit_zeros</span><span class="p">:</span>
                    <span class="n">column_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">column_subsample</span><span class="p">,</span>
                                           <span class="n">dtype</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">column_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">subsample</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
                <span class="n">column_data</span><span class="p">[:</span><span class="n">column_subsample</span><span class="p">]</span> <span class="o">=</span> <span class="n">random_state</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span>
                    <span class="n">column_nnz_data</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">column_subsample</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">ignore_implicit_zeros</span><span class="p">:</span>
                    <span class="n">column_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">column_nnz_data</span><span class="p">),</span>
                                           <span class="n">dtype</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">column_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
                <span class="n">column_data</span><span class="p">[:</span><span class="nb">len</span><span class="p">(</span><span class="n">column_nnz_data</span><span class="p">)]</span> <span class="o">=</span> <span class="n">column_nnz_data</span>

            <span class="k">if</span> <span class="ow">not</span> <span class="n">column_data</span><span class="o">.</span><span class="n">size</span><span class="p">:</span>
                <span class="c1"># if no nnz, an error will be raised for computing the</span>
                <span class="c1"># quantiles. Force the quantiles to be zeros.</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">quantiles_</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">references</span><span class="p">))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">quantiles_</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                        <span class="n">np</span><span class="o">.</span><span class="n">nanpercentile</span><span class="p">(</span><span class="n">column_data</span><span class="p">,</span> <span class="n">references</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">quantiles_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">quantiles_</span><span class="p">)</span>
        <span class="c1"># due to floating-point precision error in `np.nanpercentile`,</span>
        <span class="c1"># make sure the quantiles are monotonically increasing</span>
        <span class="c1"># Upstream issue in numpy:</span>
        <span class="c1"># https://github.com/numpy/numpy/issues/14685</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">quantiles_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="o">.</span><span class="n">accumulate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">quantiles_</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Compute the quantiles used for transforming.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            The data used to scale along the features axis. If a sparse</span>
<span class="sd">            matrix is provided, it will be converted into a sparse</span>
<span class="sd">            ``csc_matrix``. Additionally, the sparse matrix needs to be</span>
<span class="sd">            nonnegative if `ignore_implicit_zeros` is False.</span>

<span class="sd">        y : None</span>
<span class="sd">            Ignored.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        self : object</span>
<span class="sd">           Fitted transformer.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_quantiles</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid value for &#39;n_quantiles&#39;: </span><span class="si">%d</span><span class="s2">. &quot;</span>
                             <span class="s2">&quot;The number of quantiles must be at least one.&quot;</span>
                             <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_quantiles</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">subsample</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid value for &#39;subsample&#39;: </span><span class="si">%d</span><span class="s2">. &quot;</span>
                             <span class="s2">&quot;The number of subsamples must be at least one.&quot;</span>
                             <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">subsample</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_quantiles</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">subsample</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The number of quantiles cannot be greater than&quot;</span>
                             <span class="s2">&quot; the number of samples used. Got </span><span class="si">{}</span><span class="s2"> quantiles&quot;</span>
                             <span class="s2">&quot; and </span><span class="si">{}</span><span class="s2"> samples.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_quantiles</span><span class="p">,</span>
                                                       <span class="bp">self</span><span class="o">.</span><span class="n">subsample</span><span class="p">))</span>

        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_check_inputs</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">in_fit</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">n_samples</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_quantiles</span> <span class="o">&gt;</span> <span class="n">n_samples</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;n_quantiles (</span><span class="si">%s</span><span class="s2">) is greater than the total number &quot;</span>
                          <span class="s2">&quot;of samples (</span><span class="si">%s</span><span class="s2">). n_quantiles is set to &quot;</span>
                          <span class="s2">&quot;n_samples.&quot;</span>
                          <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_quantiles</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_quantiles_</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">min</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_quantiles</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">))</span>

        <span class="n">rng</span> <span class="o">=</span> <span class="n">check_random_state</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">)</span>

        <span class="c1"># Create the quantiles of reference</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">references_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_quantiles_</span><span class="p">,</span>
                                       <span class="n">endpoint</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">sparse</span><span class="o">.</span><span class="n">issparse</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_sparse_fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">rng</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_dense_fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">rng</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">_transform_col</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X_col</span><span class="p">,</span> <span class="n">quantiles</span><span class="p">,</span> <span class="n">inverse</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Private function to transform a single feature.&quot;&quot;&quot;</span>

        <span class="n">output_distribution</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_distribution</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">inverse</span><span class="p">:</span>
            <span class="n">lower_bound_x</span> <span class="o">=</span> <span class="n">quantiles</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">upper_bound_x</span> <span class="o">=</span> <span class="n">quantiles</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">lower_bound_y</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">upper_bound_y</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">lower_bound_x</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">upper_bound_x</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="n">lower_bound_y</span> <span class="o">=</span> <span class="n">quantiles</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">upper_bound_y</span> <span class="o">=</span> <span class="n">quantiles</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="c1"># for inverse transform, match a uniform distribution</span>
            <span class="k">with</span> <span class="n">np</span><span class="o">.</span><span class="n">errstate</span><span class="p">(</span><span class="n">invalid</span><span class="o">=</span><span class="s1">&#39;ignore&#39;</span><span class="p">):</span>  <span class="c1"># hide NaN comparison warnings</span>
                <span class="k">if</span> <span class="n">output_distribution</span> <span class="o">==</span> <span class="s1">&#39;normal&#39;</span><span class="p">:</span>
                    <span class="n">X_col</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">X_col</span><span class="p">)</span>
                <span class="c1"># else output distribution is already a uniform distribution</span>

        <span class="c1"># find index for lower and higher bounds</span>
        <span class="k">with</span> <span class="n">np</span><span class="o">.</span><span class="n">errstate</span><span class="p">(</span><span class="n">invalid</span><span class="o">=</span><span class="s1">&#39;ignore&#39;</span><span class="p">):</span>  <span class="c1"># hide NaN comparison warnings</span>
            <span class="k">if</span> <span class="n">output_distribution</span> <span class="o">==</span> <span class="s1">&#39;normal&#39;</span><span class="p">:</span>
                <span class="n">lower_bounds_idx</span> <span class="o">=</span> <span class="p">(</span><span class="n">X_col</span> <span class="o">-</span> <span class="n">BOUNDS_THRESHOLD</span> <span class="o">&lt;</span>
                                    <span class="n">lower_bound_x</span><span class="p">)</span>
                <span class="n">upper_bounds_idx</span> <span class="o">=</span> <span class="p">(</span><span class="n">X_col</span> <span class="o">+</span> <span class="n">BOUNDS_THRESHOLD</span> <span class="o">&gt;</span>
                                    <span class="n">upper_bound_x</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">output_distribution</span> <span class="o">==</span> <span class="s1">&#39;uniform&#39;</span><span class="p">:</span>
                <span class="n">lower_bounds_idx</span> <span class="o">=</span> <span class="p">(</span><span class="n">X_col</span> <span class="o">==</span> <span class="n">lower_bound_x</span><span class="p">)</span>
                <span class="n">upper_bounds_idx</span> <span class="o">=</span> <span class="p">(</span><span class="n">X_col</span> <span class="o">==</span> <span class="n">upper_bound_x</span><span class="p">)</span>

        <span class="n">isfinite_mask</span> <span class="o">=</span> <span class="o">~</span><span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">X_col</span><span class="p">)</span>
        <span class="n">X_col_finite</span> <span class="o">=</span> <span class="n">X_col</span><span class="p">[</span><span class="n">isfinite_mask</span><span class="p">]</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">inverse</span><span class="p">:</span>
            <span class="c1"># Interpolate in one direction and in the other and take the</span>
            <span class="c1"># mean. This is in case of repeated values in the features</span>
            <span class="c1"># and hence repeated quantiles</span>
            <span class="c1">#</span>
            <span class="c1"># If we don&#39;t do this, only one extreme of the duplicated is</span>
            <span class="c1"># used (the upper when we do ascending, and the</span>
            <span class="c1"># lower for descending). We take the mean of these two</span>
            <span class="n">X_col</span><span class="p">[</span><span class="n">isfinite_mask</span><span class="p">]</span> <span class="o">=</span> <span class="mf">.5</span> <span class="o">*</span> <span class="p">(</span>
                <span class="n">np</span><span class="o">.</span><span class="n">interp</span><span class="p">(</span><span class="n">X_col_finite</span><span class="p">,</span> <span class="n">quantiles</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">references_</span><span class="p">)</span>
                <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">interp</span><span class="p">(</span><span class="o">-</span><span class="n">X_col_finite</span><span class="p">,</span> <span class="o">-</span><span class="n">quantiles</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
                            <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">references_</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">X_col</span><span class="p">[</span><span class="n">isfinite_mask</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">interp</span><span class="p">(</span><span class="n">X_col_finite</span><span class="p">,</span>
                                             <span class="bp">self</span><span class="o">.</span><span class="n">references_</span><span class="p">,</span> <span class="n">quantiles</span><span class="p">)</span>

        <span class="n">X_col</span><span class="p">[</span><span class="n">upper_bounds_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">upper_bound_y</span>
        <span class="n">X_col</span><span class="p">[</span><span class="n">lower_bounds_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">lower_bound_y</span>
        <span class="c1"># for forward transform, match the output distribution</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">inverse</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">np</span><span class="o">.</span><span class="n">errstate</span><span class="p">(</span><span class="n">invalid</span><span class="o">=</span><span class="s1">&#39;ignore&#39;</span><span class="p">):</span>  <span class="c1"># hide NaN comparison warnings</span>
                <span class="k">if</span> <span class="n">output_distribution</span> <span class="o">==</span> <span class="s1">&#39;normal&#39;</span><span class="p">:</span>
                    <span class="n">X_col</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="n">X_col</span><span class="p">)</span>
                    <span class="c1"># find the value to clip the data to avoid mapping to</span>
                    <span class="c1"># infinity. Clip such that the inverse transform will be</span>
                    <span class="c1"># consistent</span>
                    <span class="n">clip_min</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="n">BOUNDS_THRESHOLD</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">spacing</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
                    <span class="n">clip_max</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="n">BOUNDS_THRESHOLD</span> <span class="o">-</span>
                                                   <span class="n">np</span><span class="o">.</span><span class="n">spacing</span><span class="p">(</span><span class="mi">1</span><span class="p">)))</span>
                    <span class="n">X_col</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">X_col</span><span class="p">,</span> <span class="n">clip_min</span><span class="p">,</span> <span class="n">clip_max</span><span class="p">)</span>
                <span class="c1"># else output distribution is uniform and the ppf is the</span>
                <span class="c1"># identity function so we let X_col unchanged</span>

        <span class="k">return</span> <span class="n">X_col</span>

    <span class="k">def</span> <span class="nf">_check_inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">in_fit</span><span class="p">,</span> <span class="n">accept_sparse_negative</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                      <span class="n">copy</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Check inputs before fit and transform.&quot;&quot;&quot;</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_data</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">reset</span><span class="o">=</span><span class="n">in_fit</span><span class="p">,</span>
                                <span class="n">accept_sparse</span><span class="o">=</span><span class="s1">&#39;csc&#39;</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="n">copy</span><span class="p">,</span>
                                <span class="n">dtype</span><span class="o">=</span><span class="n">FLOAT_DTYPES</span><span class="p">,</span>
                                <span class="n">force_all_finite</span><span class="o">=</span><span class="s1">&#39;allow-nan&#39;</span><span class="p">)</span>
        <span class="c1"># we only accept positive sparse matrix when ignore_implicit_zeros is</span>
        <span class="c1"># false and that we call fit or transform.</span>
        <span class="k">with</span> <span class="n">np</span><span class="o">.</span><span class="n">errstate</span><span class="p">(</span><span class="n">invalid</span><span class="o">=</span><span class="s1">&#39;ignore&#39;</span><span class="p">):</span>  <span class="c1"># hide NaN comparison warnings</span>
            <span class="k">if</span> <span class="p">(</span><span class="ow">not</span> <span class="n">accept_sparse_negative</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">ignore_implicit_zeros</span>
                    <span class="ow">and</span> <span class="p">(</span><span class="n">sparse</span><span class="o">.</span><span class="n">issparse</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="ow">and</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">data</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">))):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;QuantileTransformer only accepts&#39;</span>
                                 <span class="s1">&#39; non-negative sparse matrices.&#39;</span><span class="p">)</span>

        <span class="c1"># check the output distribution</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_distribution</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;normal&#39;</span><span class="p">,</span> <span class="s1">&#39;uniform&#39;</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;&#39;output_distribution&#39; has to be either &#39;normal&#39;&quot;</span>
                             <span class="s2">&quot; or &#39;uniform&#39;. Got &#39;</span><span class="si">{}</span><span class="s2">&#39; instead.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                                 <span class="bp">self</span><span class="o">.</span><span class="n">output_distribution</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">X</span>

    <span class="k">def</span> <span class="nf">_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">inverse</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Forward and inverse transform.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : ndarray of shape (n_samples, n_features)</span>
<span class="sd">            The data used to scale along the features axis.</span>

<span class="sd">        inverse : bool, default=False</span>
<span class="sd">            If False, apply forward transform. If True, apply</span>
<span class="sd">            inverse transform.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        X : ndarray of shape (n_samples, n_features)</span>
<span class="sd">            Projected data.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="n">sparse</span><span class="o">.</span><span class="n">issparse</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">feature_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
                <span class="n">column_slice</span> <span class="o">=</span> <span class="nb">slice</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">indptr</span><span class="p">[</span><span class="n">feature_idx</span><span class="p">],</span>
                                     <span class="n">X</span><span class="o">.</span><span class="n">indptr</span><span class="p">[</span><span class="n">feature_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">])</span>
                <span class="n">X</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">column_slice</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_transform_col</span><span class="p">(</span>
                    <span class="n">X</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">column_slice</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">quantiles_</span><span class="p">[:,</span> <span class="n">feature_idx</span><span class="p">],</span>
                    <span class="n">inverse</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">feature_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
                <span class="n">X</span><span class="p">[:,</span> <span class="n">feature_idx</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_transform_col</span><span class="p">(</span>
                    <span class="n">X</span><span class="p">[:,</span> <span class="n">feature_idx</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">quantiles_</span><span class="p">[:,</span> <span class="n">feature_idx</span><span class="p">],</span>
                    <span class="n">inverse</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">X</span>

    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Feature-wise transformation of the data.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            The data used to scale along the features axis. If a sparse</span>
<span class="sd">            matrix is provided, it will be converted into a sparse</span>
<span class="sd">            ``csc_matrix``. Additionally, the sparse matrix needs to be</span>
<span class="sd">            nonnegative if `ignore_implicit_zeros` is False.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        Xt : {ndarray, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            The projected data.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_check_inputs</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">in_fit</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">copy</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_transform</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">inverse</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">inverse_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Back-projection to the original space.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            The data used to scale along the features axis. If a sparse</span>
<span class="sd">            matrix is provided, it will be converted into a sparse</span>
<span class="sd">            ``csc_matrix``. Additionally, the sparse matrix needs to be</span>
<span class="sd">            nonnegative if `ignore_implicit_zeros` is False.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        Xt : {ndarray, sparse matrix} of (n_samples, n_features)</span>
<span class="sd">            The projected data.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_check_inputs</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">in_fit</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">accept_sparse_negative</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                               <span class="n">copy</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">copy</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_transform</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">inverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_more_tags</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;allow_nan&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">}</span>


<span class="nd">@_deprecate_positional_args</span>
<span class="k">def</span> <span class="nf">quantile_transform</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_quantiles</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
                       <span class="n">output_distribution</span><span class="o">=</span><span class="s1">&#39;uniform&#39;</span><span class="p">,</span>
                       <span class="n">ignore_implicit_zeros</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                       <span class="n">subsample</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="mf">1e5</span><span class="p">),</span>
                       <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                       <span class="n">copy</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Transform features using quantiles information.</span>

<span class="sd">    This method transforms the features to follow a uniform or a normal</span>
<span class="sd">    distribution. Therefore, for a given feature, this transformation tends</span>
<span class="sd">    to spread out the most frequent values. It also reduces the impact of</span>
<span class="sd">    (marginal) outliers: this is therefore a robust preprocessing scheme.</span>

<span class="sd">    The transformation is applied on each feature independently. First an</span>
<span class="sd">    estimate of the cumulative distribution function of a feature is</span>
<span class="sd">    used to map the original values to a uniform distribution. The obtained</span>
<span class="sd">    values are then mapped to the desired output distribution using the</span>
<span class="sd">    associated quantile function. Features values of new/unseen data that fall</span>
<span class="sd">    below or above the fitted range will be mapped to the bounds of the output</span>
<span class="sd">    distribution. Note that this transform is non-linear. It may distort linear</span>
<span class="sd">    correlations between variables measured at the same scale but renders</span>
<span class="sd">    variables measured at different scales more directly comparable.</span>

<span class="sd">    Read more in the :ref:`User Guide &lt;preprocessing_transformer&gt;`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">        The data to transform.</span>

<span class="sd">    axis : int, default=0</span>
<span class="sd">        Axis used to compute the means and standard deviations along. If 0,</span>
<span class="sd">        transform each feature, otherwise (if 1) transform each sample.</span>

<span class="sd">    n_quantiles : int, default=1000 or n_samples</span>
<span class="sd">        Number of quantiles to be computed. It corresponds to the number</span>
<span class="sd">        of landmarks used to discretize the cumulative distribution function.</span>
<span class="sd">        If n_quantiles is larger than the number of samples, n_quantiles is set</span>
<span class="sd">        to the number of samples as a larger number of quantiles does not give</span>
<span class="sd">        a better approximation of the cumulative distribution function</span>
<span class="sd">        estimator.</span>

<span class="sd">    output_distribution : {&#39;uniform&#39;, &#39;normal&#39;}, default=&#39;uniform&#39;</span>
<span class="sd">        Marginal distribution for the transformed data. The choices are</span>
<span class="sd">        &#39;uniform&#39; (default) or &#39;normal&#39;.</span>

<span class="sd">    ignore_implicit_zeros : bool, default=False</span>
<span class="sd">        Only applies to sparse matrices. If True, the sparse entries of the</span>
<span class="sd">        matrix are discarded to compute the quantile statistics. If False,</span>
<span class="sd">        these entries are treated as zeros.</span>

<span class="sd">    subsample : int, default=1e5</span>
<span class="sd">        Maximum number of samples used to estimate the quantiles for</span>
<span class="sd">        computational efficiency. Note that the subsampling procedure may</span>
<span class="sd">        differ for value-identical sparse and dense matrices.</span>

<span class="sd">    random_state : int, RandomState instance or None, default=None</span>
<span class="sd">        Determines random number generation for subsampling and smoothing</span>
<span class="sd">        noise.</span>
<span class="sd">        Please see ``subsample`` for more details.</span>
<span class="sd">        Pass an int for reproducible results across multiple function calls.</span>
<span class="sd">        See :term:`Glossary &lt;random_state&gt;`</span>

<span class="sd">    copy : bool, default=True</span>
<span class="sd">        Set to False to perform inplace transformation and avoid a copy (if the</span>
<span class="sd">        input is already a numpy array). If True, a copy of `X` is transformed,</span>
<span class="sd">        leaving the original `X` unchanged</span>

<span class="sd">        ..versionchanged:: 0.23</span>
<span class="sd">            The default value of `copy` changed from False to True in 0.23.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    Xt : {ndarray, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">        The transformed data.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import numpy as np</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.preprocessing import quantile_transform</span>
<span class="sd">    &gt;&gt;&gt; rng = np.random.RandomState(0)</span>
<span class="sd">    &gt;&gt;&gt; X = np.sort(rng.normal(loc=0.5, scale=0.25, size=(25, 1)), axis=0)</span>
<span class="sd">    &gt;&gt;&gt; quantile_transform(X, n_quantiles=10, random_state=0, copy=True)</span>
<span class="sd">    array([...])</span>

<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    QuantileTransformer : Performs quantile-based scaling using the</span>
<span class="sd">        Transformer API (e.g. as part of a preprocessing</span>
<span class="sd">        :class:`~sklearn.pipeline.Pipeline`).</span>
<span class="sd">    power_transform : Maps data to a normal distribution using a</span>
<span class="sd">        power transformation.</span>
<span class="sd">    scale : Performs standardization that is faster, but less robust</span>
<span class="sd">        to outliers.</span>
<span class="sd">    robust_scale : Performs robust standardization that removes the influence</span>
<span class="sd">        of outliers but does not put outliers and inliers on the same scale.</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    NaNs are treated as missing values: disregarded in fit, and maintained in</span>
<span class="sd">    transform.</span>

<span class="sd">    .. warning:: Risk of data leak</span>

<span class="sd">        Do not use :func:`~sklearn.preprocessing.quantile_transform` unless</span>
<span class="sd">        you know what you are doing. A common mistake is to apply it</span>
<span class="sd">        to the entire data *before* splitting into training and</span>
<span class="sd">        test sets. This will bias the model evaluation because</span>
<span class="sd">        information would have leaked from the test set to the</span>
<span class="sd">        training set.</span>
<span class="sd">        In general, we recommend using</span>
<span class="sd">        :class:`~sklearn.preprocessing.QuantileTransformer` within a</span>
<span class="sd">        :ref:`Pipeline &lt;pipeline&gt;` in order to prevent most risks of data</span>
<span class="sd">        leaking:`pipe = make_pipeline(QuantileTransformer(),</span>
<span class="sd">        LogisticRegression())`.</span>

<span class="sd">    For a comparison of the different scalers, transformers, and normalizers,</span>
<span class="sd">    see :ref:`examples/preprocessing/plot_all_scaling.py</span>
<span class="sd">    &lt;sphx_glr_auto_examples_preprocessing_plot_all_scaling.py&gt;`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">QuantileTransformer</span><span class="p">(</span><span class="n">n_quantiles</span><span class="o">=</span><span class="n">n_quantiles</span><span class="p">,</span>
                            <span class="n">output_distribution</span><span class="o">=</span><span class="n">output_distribution</span><span class="p">,</span>
                            <span class="n">subsample</span><span class="o">=</span><span class="n">subsample</span><span class="p">,</span>
                            <span class="n">ignore_implicit_zeros</span><span class="o">=</span><span class="n">ignore_implicit_zeros</span><span class="p">,</span>
                            <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
                            <span class="n">copy</span><span class="o">=</span><span class="n">copy</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">n</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">axis</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">n</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;axis should be either equal to 0 or 1. Got&quot;</span>
                         <span class="s2">&quot; axis=</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">axis</span><span class="p">))</span>


<span class="k">class</span> <span class="nc">PowerTransformer</span><span class="p">(</span><span class="n">TransformerMixin</span><span class="p">,</span> <span class="n">BaseEstimator</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Apply a power transform featurewise to make data more Gaussian-like.</span>

<span class="sd">    Power transforms are a family of parametric, monotonic transformations</span>
<span class="sd">    that are applied to make data more Gaussian-like. This is useful for</span>
<span class="sd">    modeling issues related to heteroscedasticity (non-constant variance),</span>
<span class="sd">    or other situations where normality is desired.</span>

<span class="sd">    Currently, PowerTransformer supports the Box-Cox transform and the</span>
<span class="sd">    Yeo-Johnson transform. The optimal parameter for stabilizing variance and</span>
<span class="sd">    minimizing skewness is estimated through maximum likelihood.</span>

<span class="sd">    Box-Cox requires input data to be strictly positive, while Yeo-Johnson</span>
<span class="sd">    supports both positive or negative data.</span>

<span class="sd">    By default, zero-mean, unit-variance normalization is applied to the</span>
<span class="sd">    transformed data.</span>

<span class="sd">    Read more in the :ref:`User Guide &lt;preprocessing_transformer&gt;`.</span>

<span class="sd">    .. versionadded:: 0.20</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    method : {&#39;yeo-johnson&#39;, &#39;box-cox&#39;}, default=&#39;yeo-johnson&#39;</span>
<span class="sd">        The power transform method. Available methods are:</span>

<span class="sd">        - &#39;yeo-johnson&#39; [1]_, works with positive and negative values</span>
<span class="sd">        - &#39;box-cox&#39; [2]_, only works with strictly positive values</span>

<span class="sd">    standardize : bool, default=True</span>
<span class="sd">        Set to True to apply zero-mean, unit-variance normalization to the</span>
<span class="sd">        transformed output.</span>

<span class="sd">    copy : bool, default=True</span>
<span class="sd">        Set to False to perform inplace computation during transformation.</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    lambdas_ : ndarray of float of shape (n_features,)</span>
<span class="sd">        The parameters of the power transformation for the selected features.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import numpy as np</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.preprocessing import PowerTransformer</span>
<span class="sd">    &gt;&gt;&gt; pt = PowerTransformer()</span>
<span class="sd">    &gt;&gt;&gt; data = [[1, 2], [3, 2], [4, 5]]</span>
<span class="sd">    &gt;&gt;&gt; print(pt.fit(data))</span>
<span class="sd">    PowerTransformer()</span>
<span class="sd">    &gt;&gt;&gt; print(pt.lambdas_)</span>
<span class="sd">    [ 1.386... -3.100...]</span>
<span class="sd">    &gt;&gt;&gt; print(pt.transform(data))</span>
<span class="sd">    [[-1.316... -0.707...]</span>
<span class="sd">     [ 0.209... -0.707...]</span>
<span class="sd">     [ 1.106...  1.414...]]</span>

<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    power_transform : Equivalent function without the estimator API.</span>

<span class="sd">    QuantileTransformer : Maps data to a standard normal distribution with</span>
<span class="sd">        the parameter `output_distribution=&#39;normal&#39;`.</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    NaNs are treated as missing values: disregarded in ``fit``, and maintained</span>
<span class="sd">    in ``transform``.</span>

<span class="sd">    For a comparison of the different scalers, transformers, and normalizers,</span>
<span class="sd">    see :ref:`examples/preprocessing/plot_all_scaling.py</span>
<span class="sd">    &lt;sphx_glr_auto_examples_preprocessing_plot_all_scaling.py&gt;`.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>

<span class="sd">    .. [1] I.K. Yeo and R.A. Johnson, &quot;A new family of power transformations to</span>
<span class="sd">           improve normality or symmetry.&quot; Biometrika, 87(4), pp.954-959,</span>
<span class="sd">           (2000).</span>

<span class="sd">    .. [2] G.E.P. Box and D.R. Cox, &quot;An Analysis of Transformations&quot;, Journal</span>
<span class="sd">           of the Royal Statistical Society B, 26, 211-252 (1964).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@_deprecate_positional_args</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;yeo-johnson&#39;</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">standardize</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">method</span> <span class="o">=</span> <span class="n">method</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">standardize</span> <span class="o">=</span> <span class="n">standardize</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">copy</span> <span class="o">=</span> <span class="n">copy</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Estimate the optimal parameter lambda for each feature.</span>

<span class="sd">        The optimal lambda parameter for minimizing skewness is estimated on</span>
<span class="sd">        each feature independently using maximum likelihood.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like of shape (n_samples, n_features)</span>
<span class="sd">            The data used to estimate the optimal transformation parameters.</span>

<span class="sd">        y : None</span>
<span class="sd">            Ignored.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        self : object</span>
<span class="sd">            Fitted transformer.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">force_transform</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">fit_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">force_transform</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">force_transform</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_check_input</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">in_fit</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">check_positive</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                              <span class="n">check_method</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">copy</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">force_transform</span><span class="p">:</span>  <span class="c1"># if call from fit()</span>
            <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>  <span class="c1"># force copy so that fit does not change X inplace</span>

        <span class="n">optim_function</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;box-cox&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_box_cox_optimize</span><span class="p">,</span>
                          <span class="s1">&#39;yeo-johnson&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_yeo_johnson_optimize</span>
                          <span class="p">}[</span><span class="bp">self</span><span class="o">.</span><span class="n">method</span><span class="p">]</span>
        <span class="k">with</span> <span class="n">np</span><span class="o">.</span><span class="n">errstate</span><span class="p">(</span><span class="n">invalid</span><span class="o">=</span><span class="s1">&#39;ignore&#39;</span><span class="p">):</span>  <span class="c1"># hide NaN warnings</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">lambdas_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">optim_function</span><span class="p">(</span><span class="n">col</span><span class="p">)</span> <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">])</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">standardize</span> <span class="ow">or</span> <span class="n">force_transform</span><span class="p">:</span>
            <span class="n">transform_function</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;box-cox&#39;</span><span class="p">:</span> <span class="n">boxcox</span><span class="p">,</span>
                                  <span class="s1">&#39;yeo-johnson&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_yeo_johnson_transform</span>
                                  <span class="p">}[</span><span class="bp">self</span><span class="o">.</span><span class="n">method</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">lmbda</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lambdas_</span><span class="p">):</span>
                <span class="k">with</span> <span class="n">np</span><span class="o">.</span><span class="n">errstate</span><span class="p">(</span><span class="n">invalid</span><span class="o">=</span><span class="s1">&#39;ignore&#39;</span><span class="p">):</span>  <span class="c1"># hide NaN warnings</span>
                    <span class="n">X</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">transform_function</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="n">i</span><span class="p">],</span> <span class="n">lmbda</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">standardize</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">(</span><span class="n">copy</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">force_transform</span><span class="p">:</span>
                <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_scaler</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">X</span>

    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Apply the power transform to each feature using the fitted lambdas.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like of shape (n_samples, n_features)</span>
<span class="sd">            The data to be transformed using a power transformation.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        X_trans : ndarray of shape (n_samples, n_features)</span>
<span class="sd">            The transformed data.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_check_input</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">in_fit</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">check_positive</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                              <span class="n">check_shape</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="n">transform_function</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;box-cox&#39;</span><span class="p">:</span> <span class="n">boxcox</span><span class="p">,</span>
                              <span class="s1">&#39;yeo-johnson&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_yeo_johnson_transform</span>
                              <span class="p">}[</span><span class="bp">self</span><span class="o">.</span><span class="n">method</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">lmbda</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lambdas_</span><span class="p">):</span>
            <span class="k">with</span> <span class="n">np</span><span class="o">.</span><span class="n">errstate</span><span class="p">(</span><span class="n">invalid</span><span class="o">=</span><span class="s1">&#39;ignore&#39;</span><span class="p">):</span>  <span class="c1"># hide NaN warnings</span>
                <span class="n">X</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">transform_function</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="n">i</span><span class="p">],</span> <span class="n">lmbda</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">standardize</span><span class="p">:</span>
            <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">X</span>

    <span class="k">def</span> <span class="nf">inverse_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Apply the inverse power transformation using the fitted lambdas.</span>

<span class="sd">        The inverse of the Box-Cox transformation is given by::</span>

<span class="sd">            if lambda_ == 0:</span>
<span class="sd">                X = exp(X_trans)</span>
<span class="sd">            else:</span>
<span class="sd">                X = (X_trans * lambda_ + 1) ** (1 / lambda_)</span>

<span class="sd">        The inverse of the Yeo-Johnson transformation is given by::</span>

<span class="sd">            if X &gt;= 0 and lambda_ == 0:</span>
<span class="sd">                X = exp(X_trans) - 1</span>
<span class="sd">            elif X &gt;= 0 and lambda_ != 0:</span>
<span class="sd">                X = (X_trans * lambda_ + 1) ** (1 / lambda_) - 1</span>
<span class="sd">            elif X &lt; 0 and lambda_ != 2:</span>
<span class="sd">                X = 1 - (-(2 - lambda_) * X_trans + 1) ** (1 / (2 - lambda_))</span>
<span class="sd">            elif X &lt; 0 and lambda_ == 2:</span>
<span class="sd">                X = 1 - exp(-X_trans)</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like of shape (n_samples, n_features)</span>
<span class="sd">            The transformed data.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        X : ndarray of shape (n_samples, n_features)</span>
<span class="sd">            The original data.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_check_input</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">in_fit</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">check_shape</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">standardize</span><span class="p">:</span>
            <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_scaler</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

        <span class="n">inv_fun</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;box-cox&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_box_cox_inverse_tranform</span><span class="p">,</span>
                   <span class="s1">&#39;yeo-johnson&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_yeo_johnson_inverse_transform</span>
                   <span class="p">}[</span><span class="bp">self</span><span class="o">.</span><span class="n">method</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">lmbda</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lambdas_</span><span class="p">):</span>
            <span class="k">with</span> <span class="n">np</span><span class="o">.</span><span class="n">errstate</span><span class="p">(</span><span class="n">invalid</span><span class="o">=</span><span class="s1">&#39;ignore&#39;</span><span class="p">):</span>  <span class="c1"># hide NaN warnings</span>
                <span class="n">X</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">inv_fun</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="n">i</span><span class="p">],</span> <span class="n">lmbda</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">X</span>

    <span class="k">def</span> <span class="nf">_box_cox_inverse_tranform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">lmbda</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Return inverse-transformed input x following Box-Cox inverse</span>
<span class="sd">        transform with parameter lambda.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">lmbda</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">x_inv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">x_inv</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">*</span> <span class="n">lmbda</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">**</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">lmbda</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">x_inv</span>

    <span class="k">def</span> <span class="nf">_yeo_johnson_inverse_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">lmbda</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Return inverse-transformed input x following Yeo-Johnson inverse</span>
<span class="sd">        transform with parameter lambda.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">x_inv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">pos</span> <span class="o">=</span> <span class="n">x</span> <span class="o">&gt;=</span> <span class="mi">0</span>

        <span class="c1"># when x &gt;= 0</span>
        <span class="k">if</span> <span class="nb">abs</span><span class="p">(</span><span class="n">lmbda</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">np</span><span class="o">.</span><span class="n">spacing</span><span class="p">(</span><span class="mf">1.</span><span class="p">):</span>
            <span class="n">x_inv</span><span class="p">[</span><span class="n">pos</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">pos</span><span class="p">])</span> <span class="o">-</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>  <span class="c1"># lmbda != 0</span>
            <span class="n">x_inv</span><span class="p">[</span><span class="n">pos</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">pos</span><span class="p">]</span> <span class="o">*</span> <span class="n">lmbda</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">lmbda</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>

        <span class="c1"># when x &lt; 0</span>
        <span class="k">if</span> <span class="nb">abs</span><span class="p">(</span><span class="n">lmbda</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">spacing</span><span class="p">(</span><span class="mf">1.</span><span class="p">):</span>
            <span class="n">x_inv</span><span class="p">[</span><span class="o">~</span><span class="n">pos</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="mi">2</span> <span class="o">-</span> <span class="n">lmbda</span><span class="p">)</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="o">~</span><span class="n">pos</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
                                       <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">-</span> <span class="n">lmbda</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>  <span class="c1"># lmbda == 2</span>
            <span class="n">x_inv</span><span class="p">[</span><span class="o">~</span><span class="n">pos</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">[</span><span class="o">~</span><span class="n">pos</span><span class="p">])</span>

        <span class="k">return</span> <span class="n">x_inv</span>

    <span class="k">def</span> <span class="nf">_yeo_johnson_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">lmbda</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Return transformed input x following Yeo-Johnson transform with</span>
<span class="sd">        parameter lambda.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">pos</span> <span class="o">=</span> <span class="n">x</span> <span class="o">&gt;=</span> <span class="mi">0</span>  <span class="c1"># binary mask</span>

        <span class="c1"># when x &gt;= 0</span>
        <span class="k">if</span> <span class="nb">abs</span><span class="p">(</span><span class="n">lmbda</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">np</span><span class="o">.</span><span class="n">spacing</span><span class="p">(</span><span class="mf">1.</span><span class="p">):</span>
            <span class="n">out</span><span class="p">[</span><span class="n">pos</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log1p</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">pos</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>  <span class="c1"># lmbda != 0</span>
            <span class="n">out</span><span class="p">[</span><span class="n">pos</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">pos</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">lmbda</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">lmbda</span>

        <span class="c1"># when x &lt; 0</span>
        <span class="k">if</span> <span class="nb">abs</span><span class="p">(</span><span class="n">lmbda</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">spacing</span><span class="p">(</span><span class="mf">1.</span><span class="p">):</span>
            <span class="n">out</span><span class="p">[</span><span class="o">~</span><span class="n">pos</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">[</span><span class="o">~</span><span class="n">pos</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span> <span class="o">-</span> <span class="n">lmbda</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">-</span> <span class="n">lmbda</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>  <span class="c1"># lmbda == 2</span>
            <span class="n">out</span><span class="p">[</span><span class="o">~</span><span class="n">pos</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log1p</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">[</span><span class="o">~</span><span class="n">pos</span><span class="p">])</span>

        <span class="k">return</span> <span class="n">out</span>

    <span class="k">def</span> <span class="nf">_box_cox_optimize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Find and return optimal lambda parameter of the Box-Cox transform by</span>
<span class="sd">        MLE, for observed data x.</span>

<span class="sd">        We here use scipy builtins which uses the brent optimizer.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># the computation of lambda is influenced by NaNs so we need to</span>
        <span class="c1"># get rid of them</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">lmbda</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">boxcox</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="o">~</span><span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">x</span><span class="p">)],</span> <span class="n">lmbda</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">lmbda</span>

    <span class="k">def</span> <span class="nf">_yeo_johnson_optimize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Find and return optimal lambda parameter of the Yeo-Johnson</span>
<span class="sd">        transform by MLE, for observed data x.</span>

<span class="sd">        Like for Box-Cox, MLE is done via the brent optimizer.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">def</span> <span class="nf">_neg_log_likelihood</span><span class="p">(</span><span class="n">lmbda</span><span class="p">):</span>
            <span class="sd">&quot;&quot;&quot;Return the negative log likelihood of the observed data x as a</span>
<span class="sd">            function of lambda.&quot;&quot;&quot;</span>
            <span class="n">x_trans</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_yeo_johnson_transform</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lmbda</span><span class="p">)</span>
            <span class="n">n_samples</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

            <span class="n">loglike</span> <span class="o">=</span> <span class="o">-</span><span class="n">n_samples</span> <span class="o">/</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">x_trans</span><span class="o">.</span><span class="n">var</span><span class="p">())</span>
            <span class="n">loglike</span> <span class="o">+=</span> <span class="p">(</span><span class="n">lmbda</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log1p</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

            <span class="k">return</span> <span class="o">-</span><span class="n">loglike</span>

        <span class="c1"># the computation of lambda is influenced by NaNs so we need to</span>
        <span class="c1"># get rid of them</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="o">~</span><span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">x</span><span class="p">)]</span>
        <span class="c1"># choosing bracket -2, 2 like for boxcox</span>
        <span class="k">return</span> <span class="n">optimize</span><span class="o">.</span><span class="n">brent</span><span class="p">(</span><span class="n">_neg_log_likelihood</span><span class="p">,</span> <span class="n">brack</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">_check_input</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">in_fit</span><span class="p">,</span> <span class="n">check_positive</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">check_shape</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                     <span class="n">check_method</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Validate the input before fit and transform.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like of shape (n_samples, n_features)</span>

<span class="sd">        in_fit : bool</span>
<span class="sd">            Whether or not `_check_input` is called from `fit` or other</span>
<span class="sd">            methods, e.g. `predict`, `transform`, etc.</span>

<span class="sd">        check_positive : bool, default=False</span>
<span class="sd">            If True, check that all data is positive and non-zero (only if</span>
<span class="sd">            ``self.method==&#39;box-cox&#39;``).</span>

<span class="sd">        check_shape : bool, default=False</span>
<span class="sd">            If True, check that n_features matches the length of self.lambdas_</span>

<span class="sd">        check_method : bool, default=False</span>
<span class="sd">            If True, check that the transformation method is valid.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_data</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">ensure_2d</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">FLOAT_DTYPES</span><span class="p">,</span>
                                <span class="n">copy</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">copy</span><span class="p">,</span> <span class="n">force_all_finite</span><span class="o">=</span><span class="s1">&#39;allow-nan&#39;</span><span class="p">,</span>
                                <span class="n">reset</span><span class="o">=</span><span class="n">in_fit</span><span class="p">)</span>

        <span class="k">with</span> <span class="n">np</span><span class="o">.</span><span class="n">warnings</span><span class="o">.</span><span class="n">catch_warnings</span><span class="p">():</span>
            <span class="n">np</span><span class="o">.</span><span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span>
                <span class="s1">&#39;ignore&#39;</span><span class="p">,</span> <span class="sa">r</span><span class="s1">&#39;All-NaN (slice|axis) encountered&#39;</span><span class="p">)</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">check_positive</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">method</span> <span class="o">==</span> <span class="s1">&#39;box-cox&#39;</span> <span class="ow">and</span>
                    <span class="n">np</span><span class="o">.</span><span class="n">nanmin</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The Box-Cox transformation can only be &quot;</span>
                                 <span class="s2">&quot;applied to strictly positive data&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">check_shape</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lambdas_</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Input data has a different number of features &quot;</span>
                             <span class="s2">&quot;than fitting data. Should have </span><span class="si">{n}</span><span class="s2">, data has </span><span class="si">{m}</span><span class="s2">&quot;</span>
                             <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lambdas_</span><span class="p">),</span> <span class="n">m</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>

        <span class="n">valid_methods</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;box-cox&#39;</span><span class="p">,</span> <span class="s1">&#39;yeo-johnson&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">check_method</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">method</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">valid_methods</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;&#39;method&#39; must be one of </span><span class="si">{}</span><span class="s2">, &quot;</span>
                             <span class="s2">&quot;got </span><span class="si">{}</span><span class="s2"> instead.&quot;</span>
                             <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">valid_methods</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">method</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">X</span>

    <span class="k">def</span> <span class="nf">_more_tags</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;allow_nan&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">}</span>


<span class="nd">@_deprecate_positional_args</span>
<span class="k">def</span> <span class="nf">power_transform</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;yeo-johnson&#39;</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">standardize</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Power transforms are a family of parametric, monotonic transformations</span>
<span class="sd">    that are applied to make data more Gaussian-like. This is useful for</span>
<span class="sd">    modeling issues related to heteroscedasticity (non-constant variance),</span>
<span class="sd">    or other situations where normality is desired.</span>

<span class="sd">    Currently, power_transform supports the Box-Cox transform and the</span>
<span class="sd">    Yeo-Johnson transform. The optimal parameter for stabilizing variance and</span>
<span class="sd">    minimizing skewness is estimated through maximum likelihood.</span>

<span class="sd">    Box-Cox requires input data to be strictly positive, while Yeo-Johnson</span>
<span class="sd">    supports both positive or negative data.</span>

<span class="sd">    By default, zero-mean, unit-variance normalization is applied to the</span>
<span class="sd">    transformed data.</span>

<span class="sd">    Read more in the :ref:`User Guide &lt;preprocessing_transformer&gt;`.</span>


<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : array-like of shape (n_samples, n_features)</span>
<span class="sd">        The data to be transformed using a power transformation.</span>

<span class="sd">    method : {&#39;yeo-johnson&#39;, &#39;box-cox&#39;}, default=&#39;yeo-johnson&#39;</span>
<span class="sd">        The power transform method. Available methods are:</span>

<span class="sd">        - &#39;yeo-johnson&#39; [1]_, works with positive and negative values</span>
<span class="sd">        - &#39;box-cox&#39; [2]_, only works with strictly positive values</span>

<span class="sd">        .. versionchanged:: 0.23</span>
<span class="sd">            The default value of the `method` parameter changed from</span>
<span class="sd">            &#39;box-cox&#39; to &#39;yeo-johnson&#39; in 0.23.</span>

<span class="sd">    standardize : bool, default=True</span>
<span class="sd">        Set to True to apply zero-mean, unit-variance normalization to the</span>
<span class="sd">        transformed output.</span>

<span class="sd">    copy : bool, default=True</span>
<span class="sd">        Set to False to perform inplace computation during transformation.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    X_trans : ndarray of shape (n_samples, n_features)</span>
<span class="sd">        The transformed data.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import numpy as np</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.preprocessing import power_transform</span>
<span class="sd">    &gt;&gt;&gt; data = [[1, 2], [3, 2], [4, 5]]</span>
<span class="sd">    &gt;&gt;&gt; print(power_transform(data, method=&#39;box-cox&#39;))</span>
<span class="sd">    [[-1.332... -0.707...]</span>
<span class="sd">     [ 0.256... -0.707...]</span>
<span class="sd">     [ 1.076...  1.414...]]</span>

<span class="sd">    .. warning:: Risk of data leak.</span>
<span class="sd">        Do not use :func:`~sklearn.preprocessing.power_transform` unless you</span>
<span class="sd">        know what you are doing. A common mistake is to apply it to the entire</span>
<span class="sd">        data *before* splitting into training and test sets. This will bias the</span>
<span class="sd">        model evaluation because information would have leaked from the test</span>
<span class="sd">        set to the training set.</span>
<span class="sd">        In general, we recommend using</span>
<span class="sd">        :class:`~sklearn.preprocessing.PowerTransformer` within a</span>
<span class="sd">        :ref:`Pipeline &lt;pipeline&gt;` in order to prevent most risks of data</span>
<span class="sd">        leaking, e.g.: `pipe = make_pipeline(PowerTransformer(),</span>
<span class="sd">        LogisticRegression())`.</span>

<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    PowerTransformer : Equivalent transformation with the</span>
<span class="sd">        Transformer API (e.g. as part of a preprocessing</span>
<span class="sd">        :class:`~sklearn.pipeline.Pipeline`).</span>

<span class="sd">    quantile_transform : Maps data to a standard normal distribution with</span>
<span class="sd">        the parameter `output_distribution=&#39;normal&#39;`.</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    NaNs are treated as missing values: disregarded in ``fit``, and maintained</span>
<span class="sd">    in ``transform``.</span>

<span class="sd">    For a comparison of the different scalers, transformers, and normalizers,</span>
<span class="sd">    see :ref:`examples/preprocessing/plot_all_scaling.py</span>
<span class="sd">    &lt;sphx_glr_auto_examples_preprocessing_plot_all_scaling.py&gt;`.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>

<span class="sd">    .. [1] I.K. Yeo and R.A. Johnson, &quot;A new family of power transformations to</span>
<span class="sd">           improve normality or symmetry.&quot; Biometrika, 87(4), pp.954-959,</span>
<span class="sd">           (2000).</span>

<span class="sd">    .. [2] G.E.P. Box and D.R. Cox, &quot;An Analysis of Transformations&quot;, Journal</span>
<span class="sd">           of the Royal Statistical Society B, 26, 211-252 (1964).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">pt</span> <span class="o">=</span> <span class="n">PowerTransformer</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="n">method</span><span class="p">,</span> <span class="n">standardize</span><span class="o">=</span><span class="n">standardize</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="n">copy</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">pt</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>

              </div>
              
              
              <div class='prev-next-bottom'>
                

              </div>
              
          </main>
          

      </div>
    </div>

    
  <script src="../../../_static/js/index.d3f166471bb80abb5163.js"></script>


    <footer class="footer mt-5 mt-md-0">
  <div class="container">
    <p>
          &copy; Copyright 2021, Boston Consulting Group (BCG).<br/>
        Created using <a href="http://sphinx-doc.org/">Sphinx</a> 3.4.3.<br/>
    </p>
  </div>
</footer>
  </body>
</html>