
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>sklearn.ensemble._gb &#8212; sklearndf  documentation</title>
    
  <link rel="stylesheet" href="../../../_static/css/index.f658d18f9b420779cfdf24aa0a7e2d77.css">

    
  <link rel="stylesheet"
    href="../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../../../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../../../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/basic.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/gamma.css" />
    
  <link rel="preload" as="script" href="../../../_static/js/index.d3f166471bb80abb5163.js">

    <script id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/js/gamma.js"></script>
    <script src="../../../_static/js/versions.js"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main">
<div class="container-xl">

    <a class="navbar-brand" href="../../../index.html">
    
      <img src="../../../_static/gamma_sklearndf_logo.png" class="logo" alt="logo" />
    
    </a>
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-menu" aria-controls="navbar-menu" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar-menu" class="col-lg-9 collapse navbar-collapse">
      <ul id="navbar-main-elements" class="navbar-nav mr-auto">
        
        
        <li class="nav-item ">
            <a class="nav-link" href="../../../getting_started/getting_started.html">Getting started</a>
        </li>
        
        <li class="nav-item ">
            <a class="nav-link" href="../../../apidoc/sklearndf.html">API reference</a>
        </li>
        
        <li class="nav-item ">
            <a class="nav-link" href="../../../tutorials.html">Tutorials</a>
        </li>
        
        <li class="nav-item ">
            <a class="nav-link" href="../../../contribution_guide.html">Development Guidelines</a>
        </li>
        
        <li class="nav-item ">
            <a class="nav-link" href="../../../faqs.html">FAQ</a>
        </li>
        
        <li class="nav-item ">
            <a class="nav-link" href="../../../release_notes.html">Release Notes</a>
        </li>
        
        
      </ul>


      

      <ul class="navbar-nav">
        
        
      </ul>
    </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
          <div class="col-12 col-md-3 bd-sidebar"><form class="bd-search d-flex align-items-center" action="../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">

    <div class="bd-toc-item active">
    
  
    <ul class="nav bd-sidenav">
        
        
        
        
        
        
        
        
        
        
        
        
        
      </ul>
  
  </nav>
          </div>
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
              

<nav id="bd-toc-nav">
    <ul class="nav section-nav flex-column">
    
    </ul>
</nav>


              
          </div>
          

          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <h1>Source code for sklearn.ensemble._gb</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;Gradient Boosted Regression Trees</span>

<span class="sd">This module contains methods for fitting gradient boosted regression trees for</span>
<span class="sd">both classification and regression.</span>

<span class="sd">The module structure is the following:</span>

<span class="sd">- The ``BaseGradientBoosting`` base class implements a common ``fit`` method</span>
<span class="sd">  for all the estimators in the module. Regression and classification</span>
<span class="sd">  only differ in the concrete ``LossFunction`` used.</span>

<span class="sd">- ``GradientBoostingClassifier`` implements gradient boosting for</span>
<span class="sd">  classification problems.</span>

<span class="sd">- ``GradientBoostingRegressor`` implements gradient boosting for</span>
<span class="sd">  regression problems.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="c1"># Authors: Peter Prettenhofer, Scott White, Gilles Louppe, Emanuele Olivetti,</span>
<span class="c1">#          Arnaud Joly, Jacob Schreiber</span>
<span class="c1"># License: BSD 3 clause</span>

<span class="kn">from</span> <span class="nn">abc</span> <span class="kn">import</span> <span class="n">ABCMeta</span>
<span class="kn">from</span> <span class="nn">abc</span> <span class="kn">import</span> <span class="n">abstractmethod</span>
<span class="kn">import</span> <span class="nn">warnings</span>

<span class="kn">from</span> <span class="nn">._base</span> <span class="kn">import</span> <span class="n">BaseEnsemble</span>
<span class="kn">from</span> <span class="nn">..base</span> <span class="kn">import</span> <span class="n">ClassifierMixin</span>
<span class="kn">from</span> <span class="nn">..base</span> <span class="kn">import</span> <span class="n">RegressorMixin</span>
<span class="kn">from</span> <span class="nn">..base</span> <span class="kn">import</span> <span class="n">BaseEstimator</span>
<span class="kn">from</span> <span class="nn">..base</span> <span class="kn">import</span> <span class="n">is_classifier</span>

<span class="kn">from</span> <span class="nn">._gradient_boosting</span> <span class="kn">import</span> <span class="n">predict_stages</span>
<span class="kn">from</span> <span class="nn">._gradient_boosting</span> <span class="kn">import</span> <span class="n">predict_stage</span>
<span class="kn">from</span> <span class="nn">._gradient_boosting</span> <span class="kn">import</span> <span class="n">_random_sample_mask</span>

<span class="kn">import</span> <span class="nn">numbers</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">from</span> <span class="nn">scipy.sparse</span> <span class="kn">import</span> <span class="n">csc_matrix</span>
<span class="kn">from</span> <span class="nn">scipy.sparse</span> <span class="kn">import</span> <span class="n">csr_matrix</span>
<span class="kn">from</span> <span class="nn">scipy.sparse</span> <span class="kn">import</span> <span class="n">issparse</span>

<span class="kn">from</span> <span class="nn">time</span> <span class="kn">import</span> <span class="n">time</span>
<span class="kn">from</span> <span class="nn">..model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">..tree</span> <span class="kn">import</span> <span class="n">DecisionTreeRegressor</span>
<span class="kn">from</span> <span class="nn">..tree._tree</span> <span class="kn">import</span> <span class="n">DTYPE</span><span class="p">,</span> <span class="n">DOUBLE</span>
<span class="kn">from</span> <span class="nn">.</span> <span class="kn">import</span> <span class="n">_gb_losses</span>

<span class="kn">from</span> <span class="nn">..utils</span> <span class="kn">import</span> <span class="n">check_random_state</span>
<span class="kn">from</span> <span class="nn">..utils</span> <span class="kn">import</span> <span class="n">check_array</span>
<span class="kn">from</span> <span class="nn">..utils</span> <span class="kn">import</span> <span class="n">column_or_1d</span>
<span class="kn">from</span> <span class="nn">..utils.validation</span> <span class="kn">import</span> <span class="n">check_is_fitted</span><span class="p">,</span> <span class="n">_check_sample_weight</span>
<span class="kn">from</span> <span class="nn">..utils.multiclass</span> <span class="kn">import</span> <span class="n">check_classification_targets</span>
<span class="kn">from</span> <span class="nn">..exceptions</span> <span class="kn">import</span> <span class="n">NotFittedError</span>
<span class="kn">from</span> <span class="nn">..utils.validation</span> <span class="kn">import</span> <span class="n">_deprecate_positional_args</span>


<span class="k">class</span> <span class="nc">VerboseReporter</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Reports verbose output to stdout.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    verbose : int</span>
<span class="sd">        Verbosity level. If ``verbose==1`` output is printed once in a while</span>
<span class="sd">        (when iteration mod verbose_mod is zero).; if larger than 1 then output</span>
<span class="sd">        is printed for each update.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">verbose</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">=</span> <span class="n">verbose</span>

    <span class="k">def</span> <span class="nf">init</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">est</span><span class="p">,</span> <span class="n">begin_at_stage</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialize reporter</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        est : Estimator</span>
<span class="sd">            The estimator</span>

<span class="sd">        begin_at_stage : int, default=0</span>
<span class="sd">            stage at which to begin reporting</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># header fields and line format str</span>
        <span class="n">header_fields</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Iter&#39;</span><span class="p">,</span> <span class="s1">&#39;Train Loss&#39;</span><span class="p">]</span>
        <span class="n">verbose_fmt</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;</span><span class="si">{iter:&gt;10d}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="s1">&#39;</span><span class="si">{train_score:&gt;16.4f}</span><span class="s1">&#39;</span><span class="p">]</span>
        <span class="c1"># do oob?</span>
        <span class="k">if</span> <span class="n">est</span><span class="o">.</span><span class="n">subsample</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">header_fields</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s1">&#39;OOB Improve&#39;</span><span class="p">)</span>
            <span class="n">verbose_fmt</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">{oob_impr:&gt;16.4f}</span><span class="s1">&#39;</span><span class="p">)</span>
        <span class="n">header_fields</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s1">&#39;Remaining Time&#39;</span><span class="p">)</span>
        <span class="n">verbose_fmt</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">{remaining_time:&gt;16s}</span><span class="s1">&#39;</span><span class="p">)</span>

        <span class="c1"># print the header line</span>
        <span class="nb">print</span><span class="p">((</span><span class="s1">&#39;</span><span class="si">%10s</span><span class="s1"> &#39;</span> <span class="o">+</span> <span class="s1">&#39;</span><span class="si">%16s</span><span class="s1"> &#39;</span> <span class="o">*</span>
               <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">header_fields</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> <span class="o">%</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">header_fields</span><span class="p">))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">verbose_fmt</span> <span class="o">=</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">verbose_fmt</span><span class="p">)</span>
        <span class="c1"># plot verbose info each time i % verbose_mod == 0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">verbose_mod</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">begin_at_stage</span> <span class="o">=</span> <span class="n">begin_at_stage</span>

    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">est</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Update reporter with new iteration.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        j : int</span>
<span class="sd">            The new iteration</span>
<span class="sd">        est : Estimator</span>
<span class="sd">            The estimator</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">do_oob</span> <span class="o">=</span> <span class="n">est</span><span class="o">.</span><span class="n">subsample</span> <span class="o">&lt;</span> <span class="mi">1</span>
        <span class="c1"># we need to take into account if we fit additional estimators.</span>
        <span class="n">i</span> <span class="o">=</span> <span class="n">j</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">begin_at_stage</span>  <span class="c1"># iteration relative to the start iter</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose_mod</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">oob_impr</span> <span class="o">=</span> <span class="n">est</span><span class="o">.</span><span class="n">oob_improvement_</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="k">if</span> <span class="n">do_oob</span> <span class="k">else</span> <span class="mi">0</span>
            <span class="n">remaining_time</span> <span class="o">=</span> <span class="p">((</span><span class="n">est</span><span class="o">.</span><span class="n">n_estimators</span> <span class="o">-</span> <span class="p">(</span><span class="n">j</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span> <span class="o">*</span>
                              <span class="p">(</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">start_time</span><span class="p">)</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
            <span class="k">if</span> <span class="n">remaining_time</span> <span class="o">&gt;</span> <span class="mi">60</span><span class="p">:</span>
                <span class="n">remaining_time</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="si">{0:.2f}</span><span class="s1">m&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">remaining_time</span> <span class="o">/</span> <span class="mf">60.0</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">remaining_time</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="si">{0:.2f}</span><span class="s1">s&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">remaining_time</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose_fmt</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">iter</span><span class="o">=</span><span class="n">j</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
                                          <span class="n">train_score</span><span class="o">=</span><span class="n">est</span><span class="o">.</span><span class="n">train_score_</span><span class="p">[</span><span class="n">j</span><span class="p">],</span>
                                          <span class="n">oob_impr</span><span class="o">=</span><span class="n">oob_impr</span><span class="p">,</span>
                                          <span class="n">remaining_time</span><span class="o">=</span><span class="n">remaining_time</span><span class="p">))</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="p">((</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose_mod</span> <span class="o">*</span> <span class="mi">10</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">):</span>
                <span class="c1"># adjust verbose frequency (powers of 10)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">verbose_mod</span> <span class="o">*=</span> <span class="mi">10</span>


<span class="k">class</span> <span class="nc">BaseGradientBoosting</span><span class="p">(</span><span class="n">BaseEnsemble</span><span class="p">,</span> <span class="n">metaclass</span><span class="o">=</span><span class="n">ABCMeta</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Abstract base class for Gradient Boosting. &quot;&quot;&quot;</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span> <span class="n">n_estimators</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span>
                 <span class="n">min_samples_split</span><span class="p">,</span> <span class="n">min_samples_leaf</span><span class="p">,</span> <span class="n">min_weight_fraction_leaf</span><span class="p">,</span>
                 <span class="n">max_depth</span><span class="p">,</span> <span class="n">min_impurity_decrease</span><span class="p">,</span> <span class="n">min_impurity_split</span><span class="p">,</span>
                 <span class="n">init</span><span class="p">,</span> <span class="n">subsample</span><span class="p">,</span> <span class="n">max_features</span><span class="p">,</span> <span class="n">ccp_alpha</span><span class="p">,</span>
                 <span class="n">random_state</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_leaf_nodes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">warm_start</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">presort</span><span class="o">=</span><span class="s1">&#39;deprecated&#39;</span><span class="p">,</span>
                 <span class="n">validation_fraction</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">n_iter_no_change</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">tol</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">):</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">n_estimators</span> <span class="o">=</span> <span class="n">n_estimators</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">learning_rate</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span> <span class="o">=</span> <span class="n">criterion</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_samples_split</span> <span class="o">=</span> <span class="n">min_samples_split</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_samples_leaf</span> <span class="o">=</span> <span class="n">min_samples_leaf</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_weight_fraction_leaf</span> <span class="o">=</span> <span class="n">min_weight_fraction_leaf</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">subsample</span> <span class="o">=</span> <span class="n">subsample</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_features</span> <span class="o">=</span> <span class="n">max_features</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_depth</span> <span class="o">=</span> <span class="n">max_depth</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_impurity_decrease</span> <span class="o">=</span> <span class="n">min_impurity_decrease</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_impurity_split</span> <span class="o">=</span> <span class="n">min_impurity_split</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ccp_alpha</span> <span class="o">=</span> <span class="n">ccp_alpha</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init</span> <span class="o">=</span> <span class="n">init</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span> <span class="o">=</span> <span class="n">random_state</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">=</span> <span class="n">verbose</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_leaf_nodes</span> <span class="o">=</span> <span class="n">max_leaf_nodes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">warm_start</span> <span class="o">=</span> <span class="n">warm_start</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">presort</span> <span class="o">=</span> <span class="n">presort</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">validation_fraction</span> <span class="o">=</span> <span class="n">validation_fraction</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_iter_no_change</span> <span class="o">=</span> <span class="n">n_iter_no_change</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tol</span> <span class="o">=</span> <span class="n">tol</span>

    <span class="k">def</span> <span class="nf">_fit_stage</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">raw_predictions</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">,</span> <span class="n">sample_mask</span><span class="p">,</span>
                   <span class="n">random_state</span><span class="p">,</span> <span class="n">X_idx_sorted</span><span class="p">,</span> <span class="n">X_csc</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">X_csr</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Fit another stage of ``n_classes_`` trees to the boosting model. &quot;&quot;&quot;</span>

        <span class="k">assert</span> <span class="n">sample_mask</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">bool</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_</span>
        <span class="n">original_y</span> <span class="o">=</span> <span class="n">y</span>

        <span class="c1"># Need to pass a copy of raw_predictions to negative_gradient()</span>
        <span class="c1"># because raw_predictions is partially updated at the end of the loop</span>
        <span class="c1"># in update_terminal_regions(), and gradients need to be evaluated at</span>
        <span class="c1"># iteration i - 1.</span>
        <span class="n">raw_predictions_copy</span> <span class="o">=</span> <span class="n">raw_predictions</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">K</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">loss</span><span class="o">.</span><span class="n">is_multi_class</span><span class="p">:</span>
                <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">original_y</span> <span class="o">==</span> <span class="n">k</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>

            <span class="n">residual</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">negative_gradient</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">raw_predictions_copy</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">,</span>
                                              <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">)</span>

            <span class="c1"># induce regression tree on residuals</span>
            <span class="n">tree</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span>
                <span class="n">criterion</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">criterion</span><span class="p">,</span>
                <span class="n">splitter</span><span class="o">=</span><span class="s1">&#39;best&#39;</span><span class="p">,</span>
                <span class="n">max_depth</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_depth</span><span class="p">,</span>
                <span class="n">min_samples_split</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">min_samples_split</span><span class="p">,</span>
                <span class="n">min_samples_leaf</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">min_samples_leaf</span><span class="p">,</span>
                <span class="n">min_weight_fraction_leaf</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">min_weight_fraction_leaf</span><span class="p">,</span>
                <span class="n">min_impurity_decrease</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">min_impurity_decrease</span><span class="p">,</span>
                <span class="n">min_impurity_split</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">min_impurity_split</span><span class="p">,</span>
                <span class="n">max_features</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_features</span><span class="p">,</span>
                <span class="n">max_leaf_nodes</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_leaf_nodes</span><span class="p">,</span>
                <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
                <span class="n">ccp_alpha</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">ccp_alpha</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">subsample</span> <span class="o">&lt;</span> <span class="mf">1.0</span><span class="p">:</span>
                <span class="c1"># no inplace multiplication!</span>
                <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">sample_weight</span> <span class="o">*</span> <span class="n">sample_mask</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>

            <span class="n">X</span> <span class="o">=</span> <span class="n">X_csr</span> <span class="k">if</span> <span class="n">X_csr</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">X</span>
            <span class="n">tree</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">residual</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">,</span>
                     <span class="n">check_input</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">X_idx_sorted</span><span class="o">=</span><span class="n">X_idx_sorted</span><span class="p">)</span>

            <span class="c1"># update tree leaves</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">update_terminal_regions</span><span class="p">(</span>
                <span class="n">tree</span><span class="o">.</span><span class="n">tree_</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">residual</span><span class="p">,</span> <span class="n">raw_predictions</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">,</span>
                <span class="n">sample_mask</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>

            <span class="c1"># add tree to ensemble</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">tree</span>

        <span class="k">return</span> <span class="n">raw_predictions</span>

    <span class="k">def</span> <span class="nf">_check_params</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Check validity of parameters and raise ValueError if not valid. &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_estimators</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;n_estimators must be greater than 0 but &quot;</span>
                             <span class="s2">&quot;was </span><span class="si">%r</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_estimators</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">&lt;=</span> <span class="mf">0.0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;learning_rate must be greater than 0 but &quot;</span>
                             <span class="s2">&quot;was </span><span class="si">%r</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">)</span>

        <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_SUPPORTED_LOSS</span>
                <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">_gb_losses</span><span class="o">.</span><span class="n">LOSS_FUNCTIONS</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Loss &#39;</span><span class="si">{0:s}</span><span class="s2">&#39; not supported. &quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">))</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">==</span> <span class="s1">&#39;deviance&#39;</span><span class="p">:</span>
            <span class="n">loss_class</span> <span class="o">=</span> <span class="p">(</span><span class="n">_gb_losses</span><span class="o">.</span><span class="n">MultinomialDeviance</span>
                          <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">2</span>
                          <span class="k">else</span> <span class="n">_gb_losses</span><span class="o">.</span><span class="n">BinomialDeviance</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">loss_class</span> <span class="o">=</span> <span class="n">_gb_losses</span><span class="o">.</span><span class="n">LOSS_FUNCTIONS</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">]</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;huber&#39;</span><span class="p">,</span> <span class="s1">&#39;quantile&#39;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">loss_</span> <span class="o">=</span> <span class="n">loss_class</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_classes_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">loss_</span> <span class="o">=</span> <span class="n">loss_class</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_classes_</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="mf">0.0</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">subsample</span> <span class="o">&lt;=</span> <span class="mf">1.0</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;subsample must be in (0,1] but &quot;</span>
                             <span class="s2">&quot;was </span><span class="si">%r</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">subsample</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">init</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># init must be an estimator or &#39;zero&#39;</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">init</span><span class="p">,</span> <span class="n">BaseEstimator</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">loss_</span><span class="o">.</span><span class="n">check_init_estimator</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">init</span><span class="p">)</span>
            <span class="k">elif</span> <span class="ow">not</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">init</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">init</span> <span class="o">==</span> <span class="s1">&#39;zero&#39;</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;The init parameter must be an estimator or &#39;zero&#39;. &quot;</span>
                    <span class="s2">&quot;Got init=</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">init</span><span class="p">)</span>
                <span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="mf">0.0</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">&lt;</span> <span class="mf">1.0</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;alpha must be in (0.0, 1.0) but &quot;</span>
                             <span class="s2">&quot;was </span><span class="si">%r</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">max_features</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_features</span> <span class="o">==</span> <span class="s2">&quot;auto&quot;</span><span class="p">:</span>
                <span class="c1"># if is_classification</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_classes_</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="n">max_features</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_features_</span><span class="p">)))</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># is regression</span>
                    <span class="n">max_features</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_features_</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_features</span> <span class="o">==</span> <span class="s2">&quot;sqrt&quot;</span><span class="p">:</span>
                <span class="n">max_features</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_features_</span><span class="p">)))</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_features</span> <span class="o">==</span> <span class="s2">&quot;log2&quot;</span><span class="p">:</span>
                <span class="n">max_features</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_features_</span><span class="p">)))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid value for max_features: </span><span class="si">%r</span><span class="s2">. &quot;</span>
                                 <span class="s2">&quot;Allowed string values are &#39;auto&#39;, &#39;sqrt&#39; &quot;</span>
                                 <span class="s2">&quot;or &#39;log2&#39;.&quot;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_features</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_features</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">max_features</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_features_</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">max_features</span><span class="p">,</span> <span class="n">numbers</span><span class="o">.</span><span class="n">Integral</span><span class="p">):</span>
            <span class="n">max_features</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_features</span>
        <span class="k">else</span><span class="p">:</span>  <span class="c1"># float</span>
            <span class="k">if</span> <span class="mf">0.</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_features</span> <span class="o">&lt;=</span> <span class="mf">1.</span><span class="p">:</span>
                <span class="n">max_features</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">max_features</span> <span class="o">*</span>
                                       <span class="bp">self</span><span class="o">.</span><span class="n">n_features_</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;max_features must be in (0, n_features]&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">max_features_</span> <span class="o">=</span> <span class="n">max_features</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_iter_no_change</span><span class="p">,</span>
                          <span class="p">(</span><span class="n">numbers</span><span class="o">.</span><span class="n">Integral</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="kc">None</span><span class="p">))):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;n_iter_no_change should either be None or an &quot;</span>
                             <span class="s2">&quot;integer. </span><span class="si">%r</span><span class="s2"> was passed&quot;</span>
                             <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_iter_no_change</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">presort</span> <span class="o">!=</span> <span class="s1">&#39;deprecated&#39;</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;The parameter &#39;presort&#39; is deprecated and has no &quot;</span>
                          <span class="s2">&quot;effect. It will be removed in v0.24. You can &quot;</span>
                          <span class="s2">&quot;suppress this warning by not passing any value &quot;</span>
                          <span class="s2">&quot;to the &#39;presort&#39; parameter. We also recommend &quot;</span>
                          <span class="s2">&quot;using HistGradientBoosting models instead.&quot;</span><span class="p">,</span>
                          <span class="ne">FutureWarning</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_init_state</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialize model state and allocate model state data structures. &quot;&quot;&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">init_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">init</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">init_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_</span><span class="o">.</span><span class="n">init_estimator</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">n_estimators</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_</span><span class="o">.</span><span class="n">K</span><span class="p">),</span>
                                    <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">object</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_score_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">n_estimators</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
        <span class="c1"># do oob?</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">subsample</span> <span class="o">&lt;</span> <span class="mf">1.0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">oob_improvement_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">n_estimators</span><span class="p">),</span>
                                             <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_clear_state</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Clear the state of the gradient boosting model. &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;estimators_&#39;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">object</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;train_score_&#39;</span><span class="p">):</span>
            <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_score_</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;oob_improvement_&#39;</span><span class="p">):</span>
            <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">oob_improvement_</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;init_&#39;</span><span class="p">):</span>
            <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_rng&#39;</span><span class="p">):</span>
            <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">_rng</span>

    <span class="k">def</span> <span class="nf">_resize_state</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Add additional ``n_estimators`` entries to all attributes. &quot;&quot;&quot;</span>
        <span class="c1"># self.n_estimators is the number of additional est to fit</span>
        <span class="n">total_n_estimators</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_estimators</span>
        <span class="k">if</span> <span class="n">total_n_estimators</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;resize with smaller n_estimators </span><span class="si">%d</span><span class="s1"> &lt; </span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span>
                             <span class="p">(</span><span class="n">total_n_estimators</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">resize</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">,</span>
                                     <span class="p">(</span><span class="n">total_n_estimators</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_</span><span class="o">.</span><span class="n">K</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_score_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">resize</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">train_score_</span><span class="p">,</span> <span class="n">total_n_estimators</span><span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">subsample</span> <span class="o">&lt;</span> <span class="mi">1</span> <span class="ow">or</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;oob_improvement_&#39;</span><span class="p">)):</span>
            <span class="c1"># if do oob resize arrays or create new if not available</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;oob_improvement_&#39;</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">oob_improvement_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">resize</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">oob_improvement_</span><span class="p">,</span>
                                                  <span class="n">total_n_estimators</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">oob_improvement_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">total_n_estimators</span><span class="p">,),</span>
                                                 <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_is_initialized</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;estimators_&#39;</span><span class="p">,</span> <span class="p">[]))</span> <span class="o">&gt;</span> <span class="mi">0</span>

    <span class="k">def</span> <span class="nf">_check_initialized</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Check that the estimator is initialized, raising an error if not.&quot;&quot;&quot;</span>
        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">monitor</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Fit the gradient boosting model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            The input samples. Internally, it will be converted to</span>
<span class="sd">            ``dtype=np.float32`` and if a sparse matrix is provided</span>
<span class="sd">            to a sparse ``csr_matrix``.</span>

<span class="sd">        y : array-like of shape (n_samples,)</span>
<span class="sd">            Target values (strings or integers in classification, real numbers</span>
<span class="sd">            in regression)</span>
<span class="sd">            For classification, labels must correspond to classes.</span>

<span class="sd">        sample_weight : array-like of shape (n_samples,), default=None</span>
<span class="sd">            Sample weights. If None, then samples are equally weighted. Splits</span>
<span class="sd">            that would create child nodes with net zero or negative weight are</span>
<span class="sd">            ignored while searching for a split in each node. In the case of</span>
<span class="sd">            classification, splits are also ignored if they would result in any</span>
<span class="sd">            single class carrying a negative weight in either child node.</span>

<span class="sd">        monitor : callable, default=None</span>
<span class="sd">            The monitor is called after each iteration with the current</span>
<span class="sd">            iteration, a reference to the estimator and the local variables of</span>
<span class="sd">            ``_fit_stages`` as keyword arguments ``callable(i, self,</span>
<span class="sd">            locals())``. If the callable returns ``True`` the fitting procedure</span>
<span class="sd">            is stopped. The monitor can be used for various things such as</span>
<span class="sd">            computing held-out estimates, early stopping, model introspect, and</span>
<span class="sd">            snapshoting.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        self : object</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># if not warmstart - clear the estimator state</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">warm_start</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_clear_state</span><span class="p">()</span>

        <span class="c1"># Check input</span>
        <span class="c1"># Since check_array converts both X and y to the same dtype, but the</span>
        <span class="c1"># trees use different types for X and y, checking them separately.</span>

        <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_data</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">accept_sparse</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;csr&#39;</span><span class="p">,</span> <span class="s1">&#39;csc&#39;</span><span class="p">,</span> <span class="s1">&#39;coo&#39;</span><span class="p">],</span>
                                   <span class="n">dtype</span><span class="o">=</span><span class="n">DTYPE</span><span class="p">,</span> <span class="n">multi_output</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">n_samples</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_features_</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>

        <span class="n">sample_weight_is_none</span> <span class="o">=</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="kc">None</span>

        <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">_check_sample_weight</span><span class="p">(</span><span class="n">sample_weight</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>

        <span class="n">y</span> <span class="o">=</span> <span class="n">column_or_1d</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">warn</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_y</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_iter_no_change</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">stratify</span> <span class="o">=</span> <span class="n">y</span> <span class="k">if</span> <span class="n">is_classifier</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span>
            <span class="n">X</span><span class="p">,</span> <span class="n">X_val</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">y_val</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">,</span> <span class="n">sample_weight_val</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">,</span>
                                 <span class="n">random_state</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">,</span>
                                 <span class="n">test_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">validation_fraction</span><span class="p">,</span>
                                 <span class="n">stratify</span><span class="o">=</span><span class="n">stratify</span><span class="p">))</span>
            <span class="k">if</span> <span class="n">is_classifier</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_classes_</span> <span class="o">!=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
                    <span class="c1"># We choose to error here. The problem is that the init</span>
                    <span class="c1"># estimator would be trained on y, which has some missing</span>
                    <span class="c1"># classes now, so its predictions would not have the</span>
                    <span class="c1"># correct shape.</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="s1">&#39;The training data after the early stopping split &#39;</span>
                        <span class="s1">&#39;is missing some classes. Try using another random &#39;</span>
                        <span class="s1">&#39;seed.&#39;</span>
                    <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">X_val</span> <span class="o">=</span> <span class="n">y_val</span> <span class="o">=</span> <span class="n">sample_weight_val</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_check_params</span><span class="p">()</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_initialized</span><span class="p">():</span>
            <span class="c1"># init state</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_init_state</span><span class="p">()</span>

            <span class="c1"># fit initial model and initialize raw predictions</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_</span> <span class="o">==</span> <span class="s1">&#39;zero&#39;</span><span class="p">:</span>
                <span class="n">raw_predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_</span><span class="o">.</span><span class="n">K</span><span class="p">),</span>
                                           <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># XXX clean this once we have a support_sample_weight tag</span>
                <span class="k">if</span> <span class="n">sample_weight_is_none</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">init_</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;The initial estimator </span><span class="si">{}</span><span class="s2"> does not support sample &quot;</span>
                           <span class="s2">&quot;weights.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">init_</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">))</span>
                    <span class="k">try</span><span class="p">:</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">init_</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">)</span>
                    <span class="k">except</span> <span class="ne">TypeError</span><span class="p">:</span>  <span class="c1"># regular estimator without SW support</span>
                        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
                    <span class="k">except</span> <span class="ne">ValueError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                        <span class="k">if</span> <span class="s2">&quot;pass parameters to specific steps of &quot;</span>\
                           <span class="s2">&quot;your pipeline using the &quot;</span>\
                           <span class="s2">&quot;stepname__parameter&quot;</span> <span class="ow">in</span> <span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">):</span>  <span class="c1"># pipeline</span>
                            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span> <span class="kn">from</span> <span class="nn">e</span>
                        <span class="k">else</span><span class="p">:</span>  <span class="c1"># regular estimator whose input checking failed</span>
                            <span class="k">raise</span>

                <span class="n">raw_predictions</span> <span class="o">=</span> \
                    <span class="bp">self</span><span class="o">.</span><span class="n">loss_</span><span class="o">.</span><span class="n">get_init_raw_predictions</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_</span><span class="p">)</span>

            <span class="n">begin_at_stage</span> <span class="o">=</span> <span class="mi">0</span>

            <span class="c1"># The rng state must be preserved if warm_start is True</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_rng</span> <span class="o">=</span> <span class="n">check_random_state</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># add more estimators to fitted model</span>
            <span class="c1"># invariant: warm_start = True</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_estimators</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;n_estimators=</span><span class="si">%d</span><span class="s1"> must be larger or equal to &#39;</span>
                                 <span class="s1">&#39;estimators_.shape[0]=</span><span class="si">%d</span><span class="s1"> when &#39;</span>
                                 <span class="s1">&#39;warm_start==True&#39;</span>
                                 <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_estimators</span><span class="p">,</span>
                                    <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
            <span class="n">begin_at_stage</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="c1"># The requirements of _decision_function (called in two lines</span>
            <span class="c1"># below) are more constrained than fit. It accepts only CSR</span>
            <span class="c1"># matrices.</span>
            <span class="n">X</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">DTYPE</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="s2">&quot;C&quot;</span><span class="p">,</span> <span class="n">accept_sparse</span><span class="o">=</span><span class="s1">&#39;csr&#39;</span><span class="p">)</span>
            <span class="n">raw_predictions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_raw_predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_resize_state</span><span class="p">()</span>

        <span class="n">X_idx_sorted</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># fit the boosting stages</span>
        <span class="n">n_stages</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fit_stages</span><span class="p">(</span>
            <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">raw_predictions</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_rng</span><span class="p">,</span> <span class="n">X_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">,</span>
            <span class="n">sample_weight_val</span><span class="p">,</span> <span class="n">begin_at_stage</span><span class="p">,</span> <span class="n">monitor</span><span class="p">,</span> <span class="n">X_idx_sorted</span><span class="p">)</span>

        <span class="c1"># change shape of arrays after fit (early-stopping or additional ests)</span>
        <span class="k">if</span> <span class="n">n_stages</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">[:</span><span class="n">n_stages</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">train_score_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_score_</span><span class="p">[:</span><span class="n">n_stages</span><span class="p">]</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;oob_improvement_&#39;</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">oob_improvement_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">oob_improvement_</span><span class="p">[:</span><span class="n">n_stages</span><span class="p">]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">n_estimators_</span> <span class="o">=</span> <span class="n">n_stages</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">_fit_stages</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">raw_predictions</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">,</span> <span class="n">random_state</span><span class="p">,</span>
                    <span class="n">X_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">,</span> <span class="n">sample_weight_val</span><span class="p">,</span>
                    <span class="n">begin_at_stage</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">monitor</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">X_idx_sorted</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Iteratively fits the stages.</span>

<span class="sd">        For each stage it computes the progress (OOB, train score)</span>
<span class="sd">        and delegates to ``_fit_stage``.</span>
<span class="sd">        Returns the number of stages fit; might differ from ``n_estimators``</span>
<span class="sd">        due to early stopping.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">n_samples</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">do_oob</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">subsample</span> <span class="o">&lt;</span> <span class="mf">1.0</span>
        <span class="n">sample_mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,</span> <span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">bool</span><span class="p">)</span>
        <span class="n">n_inbag</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">subsample</span> <span class="o">*</span> <span class="n">n_samples</span><span class="p">))</span>
        <span class="n">loss_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
            <span class="n">verbose_reporter</span> <span class="o">=</span> <span class="n">VerboseReporter</span><span class="p">(</span><span class="n">verbose</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">)</span>
            <span class="n">verbose_reporter</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">begin_at_stage</span><span class="p">)</span>

        <span class="n">X_csc</span> <span class="o">=</span> <span class="n">csc_matrix</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="k">if</span> <span class="n">issparse</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">X_csr</span> <span class="o">=</span> <span class="n">csr_matrix</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="k">if</span> <span class="n">issparse</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_iter_no_change</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">loss_history</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_iter_no_change</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">)</span>
            <span class="c1"># We create a generator to get the predictions for X_val after</span>
            <span class="c1"># the addition of each successive stage</span>
            <span class="n">y_val_pred_iter</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_staged_raw_predict</span><span class="p">(</span><span class="n">X_val</span><span class="p">)</span>

        <span class="c1"># perform boosting iterations</span>
        <span class="n">i</span> <span class="o">=</span> <span class="n">begin_at_stage</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">begin_at_stage</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_estimators</span><span class="p">):</span>

            <span class="c1"># subsampling</span>
            <span class="k">if</span> <span class="n">do_oob</span><span class="p">:</span>
                <span class="n">sample_mask</span> <span class="o">=</span> <span class="n">_random_sample_mask</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_inbag</span><span class="p">,</span>
                                                  <span class="n">random_state</span><span class="p">)</span>
                <span class="c1"># OOB score before adding this stage</span>
                <span class="n">old_oob_score</span> <span class="o">=</span> <span class="n">loss_</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="o">~</span><span class="n">sample_mask</span><span class="p">],</span>
                                      <span class="n">raw_predictions</span><span class="p">[</span><span class="o">~</span><span class="n">sample_mask</span><span class="p">],</span>
                                      <span class="n">sample_weight</span><span class="p">[</span><span class="o">~</span><span class="n">sample_mask</span><span class="p">])</span>

            <span class="c1"># fit next stage of trees</span>
            <span class="n">raw_predictions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fit_stage</span><span class="p">(</span>
                <span class="n">i</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">raw_predictions</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">,</span> <span class="n">sample_mask</span><span class="p">,</span>
                <span class="n">random_state</span><span class="p">,</span> <span class="n">X_idx_sorted</span><span class="p">,</span> <span class="n">X_csc</span><span class="p">,</span> <span class="n">X_csr</span><span class="p">)</span>

            <span class="c1"># track deviance (= loss)</span>
            <span class="k">if</span> <span class="n">do_oob</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">train_score_</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">loss_</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">sample_mask</span><span class="p">],</span>
                                             <span class="n">raw_predictions</span><span class="p">[</span><span class="n">sample_mask</span><span class="p">],</span>
                                             <span class="n">sample_weight</span><span class="p">[</span><span class="n">sample_mask</span><span class="p">])</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">oob_improvement_</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="n">old_oob_score</span> <span class="o">-</span> <span class="n">loss_</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="o">~</span><span class="n">sample_mask</span><span class="p">],</span>
                                          <span class="n">raw_predictions</span><span class="p">[</span><span class="o">~</span><span class="n">sample_mask</span><span class="p">],</span>
                                          <span class="n">sample_weight</span><span class="p">[</span><span class="o">~</span><span class="n">sample_mask</span><span class="p">]))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># no need to fancy index w/ no subsampling</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">train_score_</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">loss_</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">raw_predictions</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">verbose_reporter</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">monitor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">early_stopping</span> <span class="o">=</span> <span class="n">monitor</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="nb">locals</span><span class="p">())</span>
                <span class="k">if</span> <span class="n">early_stopping</span><span class="p">:</span>
                    <span class="k">break</span>

            <span class="c1"># We also provide an early stopping based on the score from</span>
            <span class="c1"># validation set (X_val, y_val), if n_iter_no_change is set</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_iter_no_change</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># By calling next(y_val_pred_iter), we get the predictions</span>
                <span class="c1"># for X_val after the addition of the current stage</span>
                <span class="n">validation_loss</span> <span class="o">=</span> <span class="n">loss_</span><span class="p">(</span><span class="n">y_val</span><span class="p">,</span> <span class="nb">next</span><span class="p">(</span><span class="n">y_val_pred_iter</span><span class="p">),</span>
                                        <span class="n">sample_weight_val</span><span class="p">)</span>

                <span class="c1"># Require validation_score to be better (less) than at least</span>
                <span class="c1"># one of the last n_iter_no_change evaluations</span>
                <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">validation_loss</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">tol</span> <span class="o">&lt;</span> <span class="n">loss_history</span><span class="p">):</span>
                    <span class="n">loss_history</span><span class="p">[</span><span class="n">i</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">loss_history</span><span class="p">)]</span> <span class="o">=</span> <span class="n">validation_loss</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">break</span>

        <span class="k">return</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span>

    <span class="k">def</span> <span class="nf">_make_estimator</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">append</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="c1"># we don&#39;t need _make_estimator</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_raw_predict_init</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Check input and compute raw predictions of the init estimator.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_initialized</span><span class="p">()</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">_validate_X_predict</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">check_input</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_features_</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;X.shape[1] should be </span><span class="si">{0:d}</span><span class="s2">, not </span><span class="si">{1:d}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">n_features_</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_</span> <span class="o">==</span> <span class="s1">&#39;zero&#39;</span><span class="p">:</span>
            <span class="n">raw_predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_</span><span class="o">.</span><span class="n">K</span><span class="p">),</span>
                                       <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">raw_predictions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_</span><span class="o">.</span><span class="n">get_init_raw_predictions</span><span class="p">(</span>
                <span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">raw_predictions</span>

    <span class="k">def</span> <span class="nf">_raw_predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Return the sum of the trees raw predictions (+ init estimator).&quot;&quot;&quot;</span>
        <span class="n">raw_predictions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_raw_predict_init</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">predict_stages</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">,</span>
                       <span class="n">raw_predictions</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">raw_predictions</span>

    <span class="k">def</span> <span class="nf">_staged_raw_predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Compute raw predictions of ``X`` for each iteration.</span>

<span class="sd">        This method allows monitoring (i.e. determine error on testing set)</span>
<span class="sd">        after each stage.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            The input samples. Internally, it will be converted to</span>
<span class="sd">            ``dtype=np.float32`` and if a sparse matrix is provided</span>
<span class="sd">            to a sparse ``csr_matrix``.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        raw_predictions : generator of ndarray of shape (n_samples, k)</span>
<span class="sd">            The raw predictions of the input samples. The order of the</span>
<span class="sd">            classes corresponds to that in the attribute :term:`classes_`.</span>
<span class="sd">            Regression and binary classification are special cases with</span>
<span class="sd">            ``k == 1``, otherwise ``k==n_classes``.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">DTYPE</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="s2">&quot;C&quot;</span><span class="p">,</span> <span class="n">accept_sparse</span><span class="o">=</span><span class="s1">&#39;csr&#39;</span><span class="p">)</span>
        <span class="n">raw_predictions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_raw_predict_init</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
            <span class="n">predict_stage</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">,</span>
                          <span class="n">raw_predictions</span><span class="p">)</span>
            <span class="k">yield</span> <span class="n">raw_predictions</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">feature_importances_</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;The impurity-based feature importances.</span>

<span class="sd">        The higher, the more important the feature.</span>
<span class="sd">        The importance of a feature is computed as the (normalized)</span>
<span class="sd">        total reduction of the criterion brought by that feature.  It is also</span>
<span class="sd">        known as the Gini importance.</span>

<span class="sd">        Warning: impurity-based feature importances can be misleading for</span>
<span class="sd">        high cardinality features (many unique values). See</span>
<span class="sd">        :func:`sklearn.inspection.permutation_importance` as an alternative.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        feature_importances_ : array, shape (n_features,)</span>
<span class="sd">            The values of this array sum to 1, unless all trees are single node</span>
<span class="sd">            trees consisting of only the root node, in which case it will be an</span>
<span class="sd">            array of zeros.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_initialized</span><span class="p">()</span>

        <span class="n">relevant_trees</span> <span class="o">=</span> <span class="p">[</span><span class="n">tree</span>
                          <span class="k">for</span> <span class="n">stage</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span> <span class="k">for</span> <span class="n">tree</span> <span class="ow">in</span> <span class="n">stage</span>
                          <span class="k">if</span> <span class="n">tree</span><span class="o">.</span><span class="n">tree_</span><span class="o">.</span><span class="n">node_count</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">]</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">relevant_trees</span><span class="p">:</span>
            <span class="c1"># degenerate case where all trees have only one node</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_features_</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>

        <span class="n">relevant_feature_importances</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">tree</span><span class="o">.</span><span class="n">tree_</span><span class="o">.</span><span class="n">compute_feature_importances</span><span class="p">(</span><span class="n">normalize</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">tree</span> <span class="ow">in</span> <span class="n">relevant_trees</span>
        <span class="p">]</span>
        <span class="n">avg_feature_importances</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">relevant_feature_importances</span><span class="p">,</span>
                                          <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">avg_feature_importances</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">avg_feature_importances</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_compute_partial_dependence_recursion</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grid</span><span class="p">,</span> <span class="n">target_features</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Fast partial dependence computation.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        grid : ndarray of shape (n_samples, n_target_features)</span>
<span class="sd">            The grid points on which the partial dependence should be</span>
<span class="sd">            evaluated.</span>
<span class="sd">        target_features : ndarray of shape (n_target_features,)</span>
<span class="sd">            The set of target features for which the partial dependence</span>
<span class="sd">            should be evaluated.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        averaged_predictions : ndarray of shape \</span>
<span class="sd">                (n_trees_per_iteration, n_samples)</span>
<span class="sd">            The value of the partial dependence function on each grid point.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">init</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s1">&#39;Using recursion method with a non-constant init predictor &#39;</span>
                <span class="s1">&#39;will lead to incorrect partial dependence values. &#39;</span>
                <span class="s1">&#39;Got init=</span><span class="si">%s</span><span class="s1">.&#39;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">init</span><span class="p">,</span>
                <span class="ne">UserWarning</span>
            <span class="p">)</span>
        <span class="n">grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">DTYPE</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="s1">&#39;C&#39;</span><span class="p">)</span>
        <span class="n">n_estimators</span><span class="p">,</span> <span class="n">n_trees_per_stage</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">averaged_predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_trees_per_stage</span><span class="p">,</span> <span class="n">grid</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span>
                                        <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="s1">&#39;C&#39;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">stage</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_estimators</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_trees_per_stage</span><span class="p">):</span>
                <span class="n">tree</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">[</span><span class="n">stage</span><span class="p">,</span> <span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">tree_</span>
                <span class="n">tree</span><span class="o">.</span><span class="n">compute_partial_dependence</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">target_features</span><span class="p">,</span>
                                                <span class="n">averaged_predictions</span><span class="p">[</span><span class="n">k</span><span class="p">])</span>
        <span class="n">averaged_predictions</span> <span class="o">*=</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span>

        <span class="k">return</span> <span class="n">averaged_predictions</span>

    <span class="k">def</span> <span class="nf">_validate_y</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">):</span>
        <span class="c1"># &#39;sample_weight&#39; is not utilised but is used for</span>
        <span class="c1"># consistency with similar method _validate_y of GBC</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_classes_</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="n">y</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">kind</span> <span class="o">==</span> <span class="s1">&#39;O&#39;</span><span class="p">:</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">DOUBLE</span><span class="p">)</span>
        <span class="c1"># Default implementation</span>
        <span class="k">return</span> <span class="n">y</span>

    <span class="k">def</span> <span class="nf">apply</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Apply trees in the ensemble to X, return leaf indices.</span>

<span class="sd">        .. versionadded:: 0.17</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            The input samples. Internally, its dtype will be converted to</span>
<span class="sd">            ``dtype=np.float32``. If a sparse matrix is provided, it will</span>
<span class="sd">            be converted to a sparse ``csr_matrix``.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        X_leaves : array-like of shape (n_samples, n_estimators, n_classes)</span>
<span class="sd">            For each datapoint x in X and for each tree in the ensemble,</span>
<span class="sd">            return the index of the leaf x ends up in each estimator.</span>
<span class="sd">            In the case of binary classification n_classes is 1.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_check_initialized</span><span class="p">()</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">_validate_X_predict</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">check_input</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1"># n_classes will be equal to 1 in the binary classification or the</span>
        <span class="c1"># regression case.</span>
        <span class="n">n_estimators</span><span class="p">,</span> <span class="n">n_classes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">leaves</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">n_estimators</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">))</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_estimators</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_classes</span><span class="p">):</span>
                <span class="n">estimator</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span>
                <span class="n">leaves</span><span class="p">[:,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">estimator</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">check_input</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">leaves</span>


<span class="k">class</span> <span class="nc">GradientBoostingClassifier</span><span class="p">(</span><span class="n">ClassifierMixin</span><span class="p">,</span> <span class="n">BaseGradientBoosting</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Gradient Boosting for classification.</span>

<span class="sd">    GB builds an additive model in a</span>
<span class="sd">    forward stage-wise fashion; it allows for the optimization of</span>
<span class="sd">    arbitrary differentiable loss functions. In each stage ``n_classes_``</span>
<span class="sd">    regression trees are fit on the negative gradient of the</span>
<span class="sd">    binomial or multinomial deviance loss function. Binary classification</span>
<span class="sd">    is a special case where only a single regression tree is induced.</span>

<span class="sd">    Read more in the :ref:`User Guide &lt;gradient_boosting&gt;`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    loss : {&#39;deviance&#39;, &#39;exponential&#39;}, default=&#39;deviance&#39;</span>
<span class="sd">        loss function to be optimized. &#39;deviance&#39; refers to</span>
<span class="sd">        deviance (= logistic regression) for classification</span>
<span class="sd">        with probabilistic outputs. For loss &#39;exponential&#39; gradient</span>
<span class="sd">        boosting recovers the AdaBoost algorithm.</span>

<span class="sd">    learning_rate : float, default=0.1</span>
<span class="sd">        learning rate shrinks the contribution of each tree by `learning_rate`.</span>
<span class="sd">        There is a trade-off between learning_rate and n_estimators.</span>

<span class="sd">    n_estimators : int, default=100</span>
<span class="sd">        The number of boosting stages to perform. Gradient boosting</span>
<span class="sd">        is fairly robust to over-fitting so a large number usually</span>
<span class="sd">        results in better performance.</span>

<span class="sd">    subsample : float, default=1.0</span>
<span class="sd">        The fraction of samples to be used for fitting the individual base</span>
<span class="sd">        learners. If smaller than 1.0 this results in Stochastic Gradient</span>
<span class="sd">        Boosting. `subsample` interacts with the parameter `n_estimators`.</span>
<span class="sd">        Choosing `subsample &lt; 1.0` leads to a reduction of variance</span>
<span class="sd">        and an increase in bias.</span>

<span class="sd">    criterion : {&#39;friedman_mse&#39;, &#39;mse&#39;, &#39;mae&#39;}, default=&#39;friedman_mse&#39;</span>
<span class="sd">        The function to measure the quality of a split. Supported criteria</span>
<span class="sd">        are &#39;friedman_mse&#39; for the mean squared error with improvement</span>
<span class="sd">        score by Friedman, &#39;mse&#39; for mean squared error, and &#39;mae&#39; for</span>
<span class="sd">        the mean absolute error. The default value of &#39;friedman_mse&#39; is</span>
<span class="sd">        generally the best as it can provide a better approximation in</span>
<span class="sd">        some cases.</span>

<span class="sd">        .. versionadded:: 0.18</span>

<span class="sd">    min_samples_split : int or float, default=2</span>
<span class="sd">        The minimum number of samples required to split an internal node:</span>

<span class="sd">        - If int, then consider `min_samples_split` as the minimum number.</span>
<span class="sd">        - If float, then `min_samples_split` is a fraction and</span>
<span class="sd">          `ceil(min_samples_split * n_samples)` are the minimum</span>
<span class="sd">          number of samples for each split.</span>

<span class="sd">        .. versionchanged:: 0.18</span>
<span class="sd">           Added float values for fractions.</span>

<span class="sd">    min_samples_leaf : int or float, default=1</span>
<span class="sd">        The minimum number of samples required to be at a leaf node.</span>
<span class="sd">        A split point at any depth will only be considered if it leaves at</span>
<span class="sd">        least ``min_samples_leaf`` training samples in each of the left and</span>
<span class="sd">        right branches.  This may have the effect of smoothing the model,</span>
<span class="sd">        especially in regression.</span>

<span class="sd">        - If int, then consider `min_samples_leaf` as the minimum number.</span>
<span class="sd">        - If float, then `min_samples_leaf` is a fraction and</span>
<span class="sd">          `ceil(min_samples_leaf * n_samples)` are the minimum</span>
<span class="sd">          number of samples for each node.</span>

<span class="sd">        .. versionchanged:: 0.18</span>
<span class="sd">           Added float values for fractions.</span>

<span class="sd">    min_weight_fraction_leaf : float, default=0.0</span>
<span class="sd">        The minimum weighted fraction of the sum total of weights (of all</span>
<span class="sd">        the input samples) required to be at a leaf node. Samples have</span>
<span class="sd">        equal weight when sample_weight is not provided.</span>

<span class="sd">    max_depth : int, default=3</span>
<span class="sd">        maximum depth of the individual regression estimators. The maximum</span>
<span class="sd">        depth limits the number of nodes in the tree. Tune this parameter</span>
<span class="sd">        for best performance; the best value depends on the interaction</span>
<span class="sd">        of the input variables.</span>

<span class="sd">    min_impurity_decrease : float, default=0.0</span>
<span class="sd">        A node will be split if this split induces a decrease of the impurity</span>
<span class="sd">        greater than or equal to this value.</span>

<span class="sd">        The weighted impurity decrease equation is the following::</span>

<span class="sd">            N_t / N * (impurity - N_t_R / N_t * right_impurity</span>
<span class="sd">                                - N_t_L / N_t * left_impurity)</span>

<span class="sd">        where ``N`` is the total number of samples, ``N_t`` is the number of</span>
<span class="sd">        samples at the current node, ``N_t_L`` is the number of samples in the</span>
<span class="sd">        left child, and ``N_t_R`` is the number of samples in the right child.</span>

<span class="sd">        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,</span>
<span class="sd">        if ``sample_weight`` is passed.</span>

<span class="sd">        .. versionadded:: 0.19</span>

<span class="sd">    min_impurity_split : float, default=None</span>
<span class="sd">        Threshold for early stopping in tree growth. A node will split</span>
<span class="sd">        if its impurity is above the threshold, otherwise it is a leaf.</span>

<span class="sd">        .. deprecated:: 0.19</span>
<span class="sd">           ``min_impurity_split`` has been deprecated in favor of</span>
<span class="sd">           ``min_impurity_decrease`` in 0.19. The default value of</span>
<span class="sd">           ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it</span>
<span class="sd">           will be removed in 0.25. Use ``min_impurity_decrease`` instead.</span>

<span class="sd">    init : estimator or &#39;zero&#39;, default=None</span>
<span class="sd">        An estimator object that is used to compute the initial predictions.</span>
<span class="sd">        ``init`` has to provide :meth:`fit` and :meth:`predict_proba`. If</span>
<span class="sd">        &#39;zero&#39;, the initial raw predictions are set to zero. By default, a</span>
<span class="sd">        ``DummyEstimator`` predicting the classes priors is used.</span>

<span class="sd">    random_state : int or RandomState, default=None</span>
<span class="sd">        Controls the random seed given to each Tree estimator at each</span>
<span class="sd">        boosting iteration.</span>
<span class="sd">        In addition, it controls the random permutation of the features at</span>
<span class="sd">        each split (see Notes for more details).</span>
<span class="sd">        It also controls the random spliting of the training data to obtain a</span>
<span class="sd">        validation set if `n_iter_no_change` is not None.</span>
<span class="sd">        Pass an int for reproducible output across multiple function calls.</span>
<span class="sd">        See :term:`Glossary &lt;random_state&gt;`.</span>

<span class="sd">    max_features : {&#39;auto&#39;, &#39;sqrt&#39;, &#39;log2&#39;}, int or float, default=None</span>
<span class="sd">        The number of features to consider when looking for the best split:</span>

<span class="sd">        - If int, then consider `max_features` features at each split.</span>
<span class="sd">        - If float, then `max_features` is a fraction and</span>
<span class="sd">          `int(max_features * n_features)` features are considered at each</span>
<span class="sd">          split.</span>
<span class="sd">        - If &#39;auto&#39;, then `max_features=sqrt(n_features)`.</span>
<span class="sd">        - If &#39;sqrt&#39;, then `max_features=sqrt(n_features)`.</span>
<span class="sd">        - If &#39;log2&#39;, then `max_features=log2(n_features)`.</span>
<span class="sd">        - If None, then `max_features=n_features`.</span>

<span class="sd">        Choosing `max_features &lt; n_features` leads to a reduction of variance</span>
<span class="sd">        and an increase in bias.</span>

<span class="sd">        Note: the search for a split does not stop until at least one</span>
<span class="sd">        valid partition of the node samples is found, even if it requires to</span>
<span class="sd">        effectively inspect more than ``max_features`` features.</span>

<span class="sd">    verbose : int, default=0</span>
<span class="sd">        Enable verbose output. If 1 then it prints progress and performance</span>
<span class="sd">        once in a while (the more trees the lower the frequency). If greater</span>
<span class="sd">        than 1 then it prints progress and performance for every tree.</span>

<span class="sd">    max_leaf_nodes : int, default=None</span>
<span class="sd">        Grow trees with ``max_leaf_nodes`` in best-first fashion.</span>
<span class="sd">        Best nodes are defined as relative reduction in impurity.</span>
<span class="sd">        If None then unlimited number of leaf nodes.</span>

<span class="sd">    warm_start : bool, default=False</span>
<span class="sd">        When set to ``True``, reuse the solution of the previous call to fit</span>
<span class="sd">        and add more estimators to the ensemble, otherwise, just erase the</span>
<span class="sd">        previous solution. See :term:`the Glossary &lt;warm_start&gt;`.</span>

<span class="sd">    presort : deprecated, default=&#39;deprecated&#39;</span>
<span class="sd">        This parameter is deprecated and will be removed in v0.24.</span>

<span class="sd">        .. deprecated :: 0.22</span>

<span class="sd">    validation_fraction : float, default=0.1</span>
<span class="sd">        The proportion of training data to set aside as validation set for</span>
<span class="sd">        early stopping. Must be between 0 and 1.</span>
<span class="sd">        Only used if ``n_iter_no_change`` is set to an integer.</span>

<span class="sd">        .. versionadded:: 0.20</span>

<span class="sd">    n_iter_no_change : int, default=None</span>
<span class="sd">        ``n_iter_no_change`` is used to decide if early stopping will be used</span>
<span class="sd">        to terminate training when validation score is not improving. By</span>
<span class="sd">        default it is set to None to disable early stopping. If set to a</span>
<span class="sd">        number, it will set aside ``validation_fraction`` size of the training</span>
<span class="sd">        data as validation and terminate training when validation score is not</span>
<span class="sd">        improving in all of the previous ``n_iter_no_change`` numbers of</span>
<span class="sd">        iterations. The split is stratified.</span>

<span class="sd">        .. versionadded:: 0.20</span>

<span class="sd">    tol : float, default=1e-4</span>
<span class="sd">        Tolerance for the early stopping. When the loss is not improving</span>
<span class="sd">        by at least tol for ``n_iter_no_change`` iterations (if set to a</span>
<span class="sd">        number), the training stops.</span>

<span class="sd">        .. versionadded:: 0.20</span>

<span class="sd">    ccp_alpha : non-negative float, default=0.0</span>
<span class="sd">        Complexity parameter used for Minimal Cost-Complexity Pruning. The</span>
<span class="sd">        subtree with the largest cost complexity that is smaller than</span>
<span class="sd">        ``ccp_alpha`` will be chosen. By default, no pruning is performed. See</span>
<span class="sd">        :ref:`minimal_cost_complexity_pruning` for details.</span>

<span class="sd">        .. versionadded:: 0.22</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    n_estimators_ : int</span>
<span class="sd">        The number of estimators as selected by early stopping (if</span>
<span class="sd">        ``n_iter_no_change`` is specified). Otherwise it is set to</span>
<span class="sd">        ``n_estimators``.</span>

<span class="sd">        .. versionadded:: 0.20</span>

<span class="sd">    feature_importances_ : ndarray of shape (n_features,)</span>
<span class="sd">        The impurity-based feature importances.</span>
<span class="sd">        The higher, the more important the feature.</span>
<span class="sd">        The importance of a feature is computed as the (normalized)</span>
<span class="sd">        total reduction of the criterion brought by that feature.  It is also</span>
<span class="sd">        known as the Gini importance.</span>

<span class="sd">        Warning: impurity-based feature importances can be misleading for</span>
<span class="sd">        high cardinality features (many unique values). See</span>
<span class="sd">        :func:`sklearn.inspection.permutation_importance` as an alternative.</span>

<span class="sd">    oob_improvement_ : ndarray of shape (n_estimators,)</span>
<span class="sd">        The improvement in loss (= deviance) on the out-of-bag samples</span>
<span class="sd">        relative to the previous iteration.</span>
<span class="sd">        ``oob_improvement_[0]`` is the improvement in</span>
<span class="sd">        loss of the first stage over the ``init`` estimator.</span>
<span class="sd">        Only available if ``subsample &lt; 1.0``</span>

<span class="sd">    train_score_ : ndarray of shape (n_estimators,)</span>
<span class="sd">        The i-th score ``train_score_[i]`` is the deviance (= loss) of the</span>
<span class="sd">        model at iteration ``i`` on the in-bag sample.</span>
<span class="sd">        If ``subsample == 1`` this is the deviance on the training data.</span>

<span class="sd">    loss_ : LossFunction</span>
<span class="sd">        The concrete ``LossFunction`` object.</span>

<span class="sd">    init_ : estimator</span>
<span class="sd">        The estimator that provides the initial predictions.</span>
<span class="sd">        Set via the ``init`` argument or ``loss.init_estimator``.</span>

<span class="sd">    estimators_ : ndarray of DecisionTreeRegressor of \</span>
<span class="sd">shape (n_estimators, ``loss_.K``)</span>
<span class="sd">        The collection of fitted sub-estimators. ``loss_.K`` is 1 for binary</span>
<span class="sd">        classification, otherwise n_classes.</span>

<span class="sd">    classes_ : ndarray of shape (n_classes,)</span>
<span class="sd">        The classes labels.</span>

<span class="sd">    n_features_ : int</span>
<span class="sd">        The number of data features.</span>

<span class="sd">    n_classes_ : int</span>
<span class="sd">        The number of classes.</span>

<span class="sd">    max_features_ : int</span>
<span class="sd">        The inferred value of max_features.</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    The features are always randomly permuted at each split. Therefore,</span>
<span class="sd">    the best found split may vary, even with the same training data and</span>
<span class="sd">    ``max_features=n_features``, if the improvement of the criterion is</span>
<span class="sd">    identical for several splits enumerated during the search of the best</span>
<span class="sd">    split. To obtain a deterministic behaviour during fitting,</span>
<span class="sd">    ``random_state`` has to be fixed.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.datasets import make_classification</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.ensemble import GradientBoostingClassifier</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.model_selection import train_test_split</span>
<span class="sd">    &gt;&gt;&gt; X, y = make_classification(random_state=0)</span>
<span class="sd">    &gt;&gt;&gt; X_train, X_test, y_train, y_test = train_test_split(</span>
<span class="sd">    ...     X, y, random_state=0)</span>
<span class="sd">    &gt;&gt;&gt; clf = GradientBoostingClassifier(random_state=0)</span>
<span class="sd">    &gt;&gt;&gt; clf.fit(X_train, y_train)</span>
<span class="sd">    GradientBoostingClassifier(random_state=0)</span>
<span class="sd">    &gt;&gt;&gt; clf.predict(X_test[:2])</span>
<span class="sd">    array([1, 0])</span>
<span class="sd">    &gt;&gt;&gt; clf.score(X_test, y_test)</span>
<span class="sd">    0.88</span>

<span class="sd">    See also</span>
<span class="sd">    --------</span>
<span class="sd">    sklearn.ensemble.HistGradientBoostingClassifier,</span>
<span class="sd">    sklearn.tree.DecisionTreeClassifier, RandomForestClassifier</span>
<span class="sd">    AdaBoostClassifier</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    J. Friedman, Greedy Function Approximation: A Gradient Boosting</span>
<span class="sd">    Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.</span>

<span class="sd">    J. Friedman, Stochastic Gradient Boosting, 1999</span>

<span class="sd">    T. Hastie, R. Tibshirani and J. Friedman.</span>
<span class="sd">    Elements of Statistical Learning Ed. 2, Springer, 2009.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_SUPPORTED_LOSS</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;deviance&#39;</span><span class="p">,</span> <span class="s1">&#39;exponential&#39;</span><span class="p">)</span>

    <span class="nd">@_deprecate_positional_args</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;deviance&#39;</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                 <span class="n">subsample</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">criterion</span><span class="o">=</span><span class="s1">&#39;friedman_mse&#39;</span><span class="p">,</span> <span class="n">min_samples_split</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                 <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">min_weight_fraction_leaf</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span>
                 <span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">min_impurity_decrease</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span>
                 <span class="n">min_impurity_split</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">max_features</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">max_leaf_nodes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">warm_start</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">presort</span><span class="o">=</span><span class="s1">&#39;deprecated&#39;</span><span class="p">,</span> <span class="n">validation_fraction</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                 <span class="n">n_iter_no_change</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span> <span class="n">ccp_alpha</span><span class="o">=</span><span class="mf">0.0</span><span class="p">):</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">n_estimators</span><span class="o">=</span><span class="n">n_estimators</span><span class="p">,</span>
            <span class="n">criterion</span><span class="o">=</span><span class="n">criterion</span><span class="p">,</span> <span class="n">min_samples_split</span><span class="o">=</span><span class="n">min_samples_split</span><span class="p">,</span>
            <span class="n">min_samples_leaf</span><span class="o">=</span><span class="n">min_samples_leaf</span><span class="p">,</span>
            <span class="n">min_weight_fraction_leaf</span><span class="o">=</span><span class="n">min_weight_fraction_leaf</span><span class="p">,</span>
            <span class="n">max_depth</span><span class="o">=</span><span class="n">max_depth</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="n">init</span><span class="p">,</span> <span class="n">subsample</span><span class="o">=</span><span class="n">subsample</span><span class="p">,</span>
            <span class="n">max_features</span><span class="o">=</span><span class="n">max_features</span><span class="p">,</span>
            <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
            <span class="n">max_leaf_nodes</span><span class="o">=</span><span class="n">max_leaf_nodes</span><span class="p">,</span>
            <span class="n">min_impurity_decrease</span><span class="o">=</span><span class="n">min_impurity_decrease</span><span class="p">,</span>
            <span class="n">min_impurity_split</span><span class="o">=</span><span class="n">min_impurity_split</span><span class="p">,</span>
            <span class="n">warm_start</span><span class="o">=</span><span class="n">warm_start</span><span class="p">,</span> <span class="n">presort</span><span class="o">=</span><span class="n">presort</span><span class="p">,</span>
            <span class="n">validation_fraction</span><span class="o">=</span><span class="n">validation_fraction</span><span class="p">,</span>
            <span class="n">n_iter_no_change</span><span class="o">=</span><span class="n">n_iter_no_change</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="n">tol</span><span class="p">,</span> <span class="n">ccp_alpha</span><span class="o">=</span><span class="n">ccp_alpha</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_validate_y</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">):</span>
        <span class="n">check_classification_targets</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">return_inverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">n_trim_classes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">count_nonzero</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">bincount</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">n_trim_classes</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;y contains </span><span class="si">%d</span><span class="s2"> class after sample_weight &quot;</span>
                             <span class="s2">&quot;trimmed classes with zero weights, while a &quot;</span>
                             <span class="s2">&quot;minimum of 2 classes are required.&quot;</span>
                             <span class="o">%</span> <span class="n">n_trim_classes</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_classes_</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">y</span>

    <span class="k">def</span> <span class="nf">decision_function</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Compute the decision function of ``X``.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            The input samples. Internally, it will be converted to</span>
<span class="sd">            ``dtype=np.float32`` and if a sparse matrix is provided</span>
<span class="sd">            to a sparse ``csr_matrix``.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        score : ndarray of shape (n_samples, n_classes) or (n_samples,)</span>
<span class="sd">            The decision function of the input samples, which corresponds to</span>
<span class="sd">            the raw values predicted from the trees of the ensemble . The</span>
<span class="sd">            order of the classes corresponds to that in the attribute</span>
<span class="sd">            :term:`classes_`. Regression and binary classification produce an</span>
<span class="sd">            array of shape [n_samples].</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">DTYPE</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="s2">&quot;C&quot;</span><span class="p">,</span> <span class="n">accept_sparse</span><span class="o">=</span><span class="s1">&#39;csr&#39;</span><span class="p">)</span>
        <span class="n">raw_predictions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_raw_predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">raw_predictions</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">raw_predictions</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">raw_predictions</span>

    <span class="k">def</span> <span class="nf">staged_decision_function</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Compute decision function of ``X`` for each iteration.</span>

<span class="sd">        This method allows monitoring (i.e. determine error on testing set)</span>
<span class="sd">        after each stage.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            The input samples. Internally, it will be converted to</span>
<span class="sd">            ``dtype=np.float32`` and if a sparse matrix is provided</span>
<span class="sd">            to a sparse ``csr_matrix``.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        score : generator of ndarray of shape (n_samples, k)</span>
<span class="sd">            The decision function of the input samples, which corresponds to</span>
<span class="sd">            the raw values predicted from the trees of the ensemble . The</span>
<span class="sd">            classes corresponds to that in the attribute :term:`classes_`.</span>
<span class="sd">            Regression and binary classification are special cases with</span>
<span class="sd">            ``k == 1``, otherwise ``k==n_classes``.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">yield from</span> <span class="bp">self</span><span class="o">.</span><span class="n">_staged_raw_predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Predict class for X.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            The input samples. Internally, it will be converted to</span>
<span class="sd">            ``dtype=np.float32`` and if a sparse matrix is provided</span>
<span class="sd">            to a sparse ``csr_matrix``.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        y : ndarray of shape (n_samples,)</span>
<span class="sd">            The predicted values.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">raw_predictions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decision_function</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">encoded_labels</span> <span class="o">=</span> \
            <span class="bp">self</span><span class="o">.</span><span class="n">loss_</span><span class="o">.</span><span class="n">_raw_prediction_to_decision</span><span class="p">(</span><span class="n">raw_predictions</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">encoded_labels</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">staged_predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Predict class at each stage for X.</span>

<span class="sd">        This method allows monitoring (i.e. determine error on testing set)</span>
<span class="sd">        after each stage.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            The input samples. Internally, it will be converted to</span>
<span class="sd">            ``dtype=np.float32`` and if a sparse matrix is provided</span>
<span class="sd">            to a sparse ``csr_matrix``.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        y : generator of ndarray of shape (n_samples,)</span>
<span class="sd">            The predicted value of the input samples.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">raw_predictions</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_staged_raw_predict</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
            <span class="n">encoded_labels</span> <span class="o">=</span> \
                <span class="bp">self</span><span class="o">.</span><span class="n">loss_</span><span class="o">.</span><span class="n">_raw_prediction_to_decision</span><span class="p">(</span><span class="n">raw_predictions</span><span class="p">)</span>
            <span class="k">yield</span> <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">encoded_labels</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">predict_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Predict class probabilities for X.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            The input samples. Internally, it will be converted to</span>
<span class="sd">            ``dtype=np.float32`` and if a sparse matrix is provided</span>
<span class="sd">            to a sparse ``csr_matrix``.</span>

<span class="sd">        Raises</span>
<span class="sd">        ------</span>
<span class="sd">        AttributeError</span>
<span class="sd">            If the ``loss`` does not support probabilities.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        p : ndarray of shape (n_samples, n_classes)</span>
<span class="sd">            The class probabilities of the input samples. The order of the</span>
<span class="sd">            classes corresponds to that in the attribute :term:`classes_`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">raw_predictions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decision_function</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_</span><span class="o">.</span><span class="n">_raw_prediction_to_proba</span><span class="p">(</span><span class="n">raw_predictions</span><span class="p">)</span>
        <span class="k">except</span> <span class="n">NotFittedError</span><span class="p">:</span>
            <span class="k">raise</span>
        <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="s1">&#39;loss=</span><span class="si">%r</span><span class="s1"> does not support predict_proba&#39;</span> <span class="o">%</span>
                                 <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">predict_log_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Predict class log-probabilities for X.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            The input samples. Internally, it will be converted to</span>
<span class="sd">            ``dtype=np.float32`` and if a sparse matrix is provided</span>
<span class="sd">            to a sparse ``csr_matrix``.</span>

<span class="sd">        Raises</span>
<span class="sd">        ------</span>
<span class="sd">        AttributeError</span>
<span class="sd">            If the ``loss`` does not support probabilities.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        p : ndarray of shape (n_samples, n_classes)</span>
<span class="sd">            The class log-probabilities of the input samples. The order of the</span>
<span class="sd">            classes corresponds to that in the attribute :term:`classes_`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">proba</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">proba</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">staged_predict_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Predict class probabilities at each stage for X.</span>

<span class="sd">        This method allows monitoring (i.e. determine error on testing set)</span>
<span class="sd">        after each stage.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            The input samples. Internally, it will be converted to</span>
<span class="sd">            ``dtype=np.float32`` and if a sparse matrix is provided</span>
<span class="sd">            to a sparse ``csr_matrix``.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        y : generator of ndarray of shape (n_samples,)</span>
<span class="sd">            The predicted value of the input samples.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">raw_predictions</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_staged_raw_predict</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
                <span class="k">yield</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_</span><span class="o">.</span><span class="n">_raw_prediction_to_proba</span><span class="p">(</span><span class="n">raw_predictions</span><span class="p">)</span>
        <span class="k">except</span> <span class="n">NotFittedError</span><span class="p">:</span>
            <span class="k">raise</span>
        <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="s1">&#39;loss=</span><span class="si">%r</span><span class="s1"> does not support predict_proba&#39;</span> <span class="o">%</span>
                                 <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">GradientBoostingRegressor</span><span class="p">(</span><span class="n">RegressorMixin</span><span class="p">,</span> <span class="n">BaseGradientBoosting</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Gradient Boosting for regression.</span>

<span class="sd">    GB builds an additive model in a forward stage-wise fashion;</span>
<span class="sd">    it allows for the optimization of arbitrary differentiable loss functions.</span>
<span class="sd">    In each stage a regression tree is fit on the negative gradient of the</span>
<span class="sd">    given loss function.</span>

<span class="sd">    Read more in the :ref:`User Guide &lt;gradient_boosting&gt;`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    loss : {&#39;ls&#39;, &#39;lad&#39;, &#39;huber&#39;, &#39;quantile&#39;}, default=&#39;ls&#39;</span>
<span class="sd">        loss function to be optimized. &#39;ls&#39; refers to least squares</span>
<span class="sd">        regression. &#39;lad&#39; (least absolute deviation) is a highly robust</span>
<span class="sd">        loss function solely based on order information of the input</span>
<span class="sd">        variables. &#39;huber&#39; is a combination of the two. &#39;quantile&#39;</span>
<span class="sd">        allows quantile regression (use `alpha` to specify the quantile).</span>

<span class="sd">    learning_rate : float, default=0.1</span>
<span class="sd">        learning rate shrinks the contribution of each tree by `learning_rate`.</span>
<span class="sd">        There is a trade-off between learning_rate and n_estimators.</span>

<span class="sd">    n_estimators : int, default=100</span>
<span class="sd">        The number of boosting stages to perform. Gradient boosting</span>
<span class="sd">        is fairly robust to over-fitting so a large number usually</span>
<span class="sd">        results in better performance.</span>

<span class="sd">    subsample : float, default=1.0</span>
<span class="sd">        The fraction of samples to be used for fitting the individual base</span>
<span class="sd">        learners. If smaller than 1.0 this results in Stochastic Gradient</span>
<span class="sd">        Boosting. `subsample` interacts with the parameter `n_estimators`.</span>
<span class="sd">        Choosing `subsample &lt; 1.0` leads to a reduction of variance</span>
<span class="sd">        and an increase in bias.</span>

<span class="sd">    criterion : {&#39;friedman_mse&#39;, &#39;mse&#39;, &#39;mae&#39;}, default=&#39;friedman_mse&#39;</span>
<span class="sd">        The function to measure the quality of a split. Supported criteria</span>
<span class="sd">        are &quot;friedman_mse&quot; for the mean squared error with improvement</span>
<span class="sd">        score by Friedman, &quot;mse&quot; for mean squared error, and &quot;mae&quot; for</span>
<span class="sd">        the mean absolute error. The default value of &quot;friedman_mse&quot; is</span>
<span class="sd">        generally the best as it can provide a better approximation in</span>
<span class="sd">        some cases.</span>

<span class="sd">        .. versionadded:: 0.18</span>

<span class="sd">    min_samples_split : int or float, default=2</span>
<span class="sd">        The minimum number of samples required to split an internal node:</span>

<span class="sd">        - If int, then consider `min_samples_split` as the minimum number.</span>
<span class="sd">        - If float, then `min_samples_split` is a fraction and</span>
<span class="sd">          `ceil(min_samples_split * n_samples)` are the minimum</span>
<span class="sd">          number of samples for each split.</span>

<span class="sd">        .. versionchanged:: 0.18</span>
<span class="sd">           Added float values for fractions.</span>

<span class="sd">    min_samples_leaf : int or float, default=1</span>
<span class="sd">        The minimum number of samples required to be at a leaf node.</span>
<span class="sd">        A split point at any depth will only be considered if it leaves at</span>
<span class="sd">        least ``min_samples_leaf`` training samples in each of the left and</span>
<span class="sd">        right branches.  This may have the effect of smoothing the model,</span>
<span class="sd">        especially in regression.</span>

<span class="sd">        - If int, then consider `min_samples_leaf` as the minimum number.</span>
<span class="sd">        - If float, then `min_samples_leaf` is a fraction and</span>
<span class="sd">          `ceil(min_samples_leaf * n_samples)` are the minimum</span>
<span class="sd">          number of samples for each node.</span>

<span class="sd">        .. versionchanged:: 0.18</span>
<span class="sd">           Added float values for fractions.</span>

<span class="sd">    min_weight_fraction_leaf : float, default=0.0</span>
<span class="sd">        The minimum weighted fraction of the sum total of weights (of all</span>
<span class="sd">        the input samples) required to be at a leaf node. Samples have</span>
<span class="sd">        equal weight when sample_weight is not provided.</span>

<span class="sd">    max_depth : int, default=3</span>
<span class="sd">        maximum depth of the individual regression estimators. The maximum</span>
<span class="sd">        depth limits the number of nodes in the tree. Tune this parameter</span>
<span class="sd">        for best performance; the best value depends on the interaction</span>
<span class="sd">        of the input variables.</span>

<span class="sd">    min_impurity_decrease : float, default=0.0</span>
<span class="sd">        A node will be split if this split induces a decrease of the impurity</span>
<span class="sd">        greater than or equal to this value.</span>

<span class="sd">        The weighted impurity decrease equation is the following::</span>

<span class="sd">            N_t / N * (impurity - N_t_R / N_t * right_impurity</span>
<span class="sd">                                - N_t_L / N_t * left_impurity)</span>

<span class="sd">        where ``N`` is the total number of samples, ``N_t`` is the number of</span>
<span class="sd">        samples at the current node, ``N_t_L`` is the number of samples in the</span>
<span class="sd">        left child, and ``N_t_R`` is the number of samples in the right child.</span>

<span class="sd">        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,</span>
<span class="sd">        if ``sample_weight`` is passed.</span>

<span class="sd">        .. versionadded:: 0.19</span>

<span class="sd">    min_impurity_split : float, default=None</span>
<span class="sd">        Threshold for early stopping in tree growth. A node will split</span>
<span class="sd">        if its impurity is above the threshold, otherwise it is a leaf.</span>

<span class="sd">        .. deprecated:: 0.19</span>
<span class="sd">           ``min_impurity_split`` has been deprecated in favor of</span>
<span class="sd">           ``min_impurity_decrease`` in 0.19. The default value of</span>
<span class="sd">           ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it</span>
<span class="sd">           will be removed in 0.25. Use ``min_impurity_decrease`` instead.</span>

<span class="sd">    init : estimator or &#39;zero&#39;, default=None</span>
<span class="sd">        An estimator object that is used to compute the initial predictions.</span>
<span class="sd">        ``init`` has to provide :term:`fit` and :term:`predict`. If &#39;zero&#39;, the</span>
<span class="sd">        initial raw predictions are set to zero. By default a</span>
<span class="sd">        ``DummyEstimator`` is used, predicting either the average target value</span>
<span class="sd">        (for loss=&#39;ls&#39;), or a quantile for the other losses.</span>

<span class="sd">    random_state : int or RandomState, default=None</span>
<span class="sd">        Controls the random seed given to each Tree estimator at each</span>
<span class="sd">        boosting iteration.</span>
<span class="sd">        In addition, it controls the random permutation of the features at</span>
<span class="sd">        each split (see Notes for more details).</span>
<span class="sd">        It also controls the random spliting of the training data to obtain a</span>
<span class="sd">        validation set if `n_iter_no_change` is not None.</span>
<span class="sd">        Pass an int for reproducible output across multiple function calls.</span>
<span class="sd">        See :term:`Glossary &lt;random_state&gt;`.</span>

<span class="sd">    max_features : {&#39;auto&#39;, &#39;sqrt&#39;, &#39;log2&#39;}, int or float, default=None</span>
<span class="sd">        The number of features to consider when looking for the best split:</span>

<span class="sd">        - If int, then consider `max_features` features at each split.</span>
<span class="sd">        - If float, then `max_features` is a fraction and</span>
<span class="sd">          `int(max_features * n_features)` features are considered at each</span>
<span class="sd">          split.</span>
<span class="sd">        - If &quot;auto&quot;, then `max_features=n_features`.</span>
<span class="sd">        - If &quot;sqrt&quot;, then `max_features=sqrt(n_features)`.</span>
<span class="sd">        - If &quot;log2&quot;, then `max_features=log2(n_features)`.</span>
<span class="sd">        - If None, then `max_features=n_features`.</span>

<span class="sd">        Choosing `max_features &lt; n_features` leads to a reduction of variance</span>
<span class="sd">        and an increase in bias.</span>

<span class="sd">        Note: the search for a split does not stop until at least one</span>
<span class="sd">        valid partition of the node samples is found, even if it requires to</span>
<span class="sd">        effectively inspect more than ``max_features`` features.</span>

<span class="sd">    alpha : float, default=0.9</span>
<span class="sd">        The alpha-quantile of the huber loss function and the quantile</span>
<span class="sd">        loss function. Only if ``loss=&#39;huber&#39;`` or ``loss=&#39;quantile&#39;``.</span>

<span class="sd">    verbose : int, default=0</span>
<span class="sd">        Enable verbose output. If 1 then it prints progress and performance</span>
<span class="sd">        once in a while (the more trees the lower the frequency). If greater</span>
<span class="sd">        than 1 then it prints progress and performance for every tree.</span>

<span class="sd">    max_leaf_nodes : int, default=None</span>
<span class="sd">        Grow trees with ``max_leaf_nodes`` in best-first fashion.</span>
<span class="sd">        Best nodes are defined as relative reduction in impurity.</span>
<span class="sd">        If None then unlimited number of leaf nodes.</span>

<span class="sd">    warm_start : bool, default=False</span>
<span class="sd">        When set to ``True``, reuse the solution of the previous call to fit</span>
<span class="sd">        and add more estimators to the ensemble, otherwise, just erase the</span>
<span class="sd">        previous solution. See :term:`the Glossary &lt;warm_start&gt;`.</span>

<span class="sd">    presort : deprecated, default=&#39;deprecated&#39;</span>
<span class="sd">        This parameter is deprecated and will be removed in v0.24.</span>

<span class="sd">        .. deprecated :: 0.22</span>

<span class="sd">    validation_fraction : float, default=0.1</span>
<span class="sd">        The proportion of training data to set aside as validation set for</span>
<span class="sd">        early stopping. Must be between 0 and 1.</span>
<span class="sd">        Only used if ``n_iter_no_change`` is set to an integer.</span>

<span class="sd">        .. versionadded:: 0.20</span>

<span class="sd">    n_iter_no_change : int, default=None</span>
<span class="sd">        ``n_iter_no_change`` is used to decide if early stopping will be used</span>
<span class="sd">        to terminate training when validation score is not improving. By</span>
<span class="sd">        default it is set to None to disable early stopping. If set to a</span>
<span class="sd">        number, it will set aside ``validation_fraction`` size of the training</span>
<span class="sd">        data as validation and terminate training when validation score is not</span>
<span class="sd">        improving in all of the previous ``n_iter_no_change`` numbers of</span>
<span class="sd">        iterations.</span>

<span class="sd">        .. versionadded:: 0.20</span>

<span class="sd">    tol : float, default=1e-4</span>
<span class="sd">        Tolerance for the early stopping. When the loss is not improving</span>
<span class="sd">        by at least tol for ``n_iter_no_change`` iterations (if set to a</span>
<span class="sd">        number), the training stops.</span>

<span class="sd">        .. versionadded:: 0.20</span>

<span class="sd">    ccp_alpha : non-negative float, default=0.0</span>
<span class="sd">        Complexity parameter used for Minimal Cost-Complexity Pruning. The</span>
<span class="sd">        subtree with the largest cost complexity that is smaller than</span>
<span class="sd">        ``ccp_alpha`` will be chosen. By default, no pruning is performed. See</span>
<span class="sd">        :ref:`minimal_cost_complexity_pruning` for details.</span>

<span class="sd">        .. versionadded:: 0.22</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    feature_importances_ : ndarray of shape (n_features,)</span>
<span class="sd">        The impurity-based feature importances.</span>
<span class="sd">        The higher, the more important the feature.</span>
<span class="sd">        The importance of a feature is computed as the (normalized)</span>
<span class="sd">        total reduction of the criterion brought by that feature.  It is also</span>
<span class="sd">        known as the Gini importance.</span>

<span class="sd">        Warning: impurity-based feature importances can be misleading for</span>
<span class="sd">        high cardinality features (many unique values). See</span>
<span class="sd">        :func:`sklearn.inspection.permutation_importance` as an alternative.</span>

<span class="sd">    oob_improvement_ : ndarray of shape (n_estimators,)</span>
<span class="sd">        The improvement in loss (= deviance) on the out-of-bag samples</span>
<span class="sd">        relative to the previous iteration.</span>
<span class="sd">        ``oob_improvement_[0]`` is the improvement in</span>
<span class="sd">        loss of the first stage over the ``init`` estimator.</span>
<span class="sd">        Only available if ``subsample &lt; 1.0``</span>

<span class="sd">    train_score_ : ndarray of shape (n_estimators,)</span>
<span class="sd">        The i-th score ``train_score_[i]`` is the deviance (= loss) of the</span>
<span class="sd">        model at iteration ``i`` on the in-bag sample.</span>
<span class="sd">        If ``subsample == 1`` this is the deviance on the training data.</span>

<span class="sd">    loss_ : LossFunction</span>
<span class="sd">        The concrete ``LossFunction`` object.</span>

<span class="sd">    init_ : estimator</span>
<span class="sd">        The estimator that provides the initial predictions.</span>
<span class="sd">        Set via the ``init`` argument or ``loss.init_estimator``.</span>

<span class="sd">    estimators_ : ndarray of DecisionTreeRegressor of shape (n_estimators, 1)</span>
<span class="sd">        The collection of fitted sub-estimators.</span>

<span class="sd">    n_features_ : int</span>
<span class="sd">        The number of data features.</span>

<span class="sd">    max_features_ : int</span>
<span class="sd">        The inferred value of max_features.</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    The features are always randomly permuted at each split. Therefore,</span>
<span class="sd">    the best found split may vary, even with the same training data and</span>
<span class="sd">    ``max_features=n_features``, if the improvement of the criterion is</span>
<span class="sd">    identical for several splits enumerated during the search of the best</span>
<span class="sd">    split. To obtain a deterministic behaviour during fitting,</span>
<span class="sd">    ``random_state`` has to be fixed.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.datasets import make_regression</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.ensemble import GradientBoostingRegressor</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.model_selection import train_test_split</span>
<span class="sd">    &gt;&gt;&gt; X, y = make_regression(random_state=0)</span>
<span class="sd">    &gt;&gt;&gt; X_train, X_test, y_train, y_test = train_test_split(</span>
<span class="sd">    ...     X, y, random_state=0)</span>
<span class="sd">    &gt;&gt;&gt; reg = GradientBoostingRegressor(random_state=0)</span>
<span class="sd">    &gt;&gt;&gt; reg.fit(X_train, y_train)</span>
<span class="sd">    GradientBoostingRegressor(random_state=0)</span>
<span class="sd">    &gt;&gt;&gt; reg.predict(X_test[1:2])</span>
<span class="sd">    array([-61...])</span>
<span class="sd">    &gt;&gt;&gt; reg.score(X_test, y_test)</span>
<span class="sd">    0.4...</span>

<span class="sd">    See also</span>
<span class="sd">    --------</span>
<span class="sd">    sklearn.ensemble.HistGradientBoostingRegressor,</span>
<span class="sd">    sklearn.tree.DecisionTreeRegressor, RandomForestRegressor</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    J. Friedman, Greedy Function Approximation: A Gradient Boosting</span>
<span class="sd">    Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.</span>

<span class="sd">    J. Friedman, Stochastic Gradient Boosting, 1999</span>

<span class="sd">    T. Hastie, R. Tibshirani and J. Friedman.</span>
<span class="sd">    Elements of Statistical Learning Ed. 2, Springer, 2009.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_SUPPORTED_LOSS</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;ls&#39;</span><span class="p">,</span> <span class="s1">&#39;lad&#39;</span><span class="p">,</span> <span class="s1">&#39;huber&#39;</span><span class="p">,</span> <span class="s1">&#39;quantile&#39;</span><span class="p">)</span>

    <span class="nd">@_deprecate_positional_args</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;ls&#39;</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                 <span class="n">subsample</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">criterion</span><span class="o">=</span><span class="s1">&#39;friedman_mse&#39;</span><span class="p">,</span> <span class="n">min_samples_split</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                 <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">min_weight_fraction_leaf</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span>
                 <span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">min_impurity_decrease</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span>
                 <span class="n">min_impurity_split</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">max_features</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_leaf_nodes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">warm_start</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">presort</span><span class="o">=</span><span class="s1">&#39;deprecated&#39;</span><span class="p">,</span>
                 <span class="n">validation_fraction</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                 <span class="n">n_iter_no_change</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span> <span class="n">ccp_alpha</span><span class="o">=</span><span class="mf">0.0</span><span class="p">):</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">n_estimators</span><span class="o">=</span><span class="n">n_estimators</span><span class="p">,</span>
            <span class="n">criterion</span><span class="o">=</span><span class="n">criterion</span><span class="p">,</span> <span class="n">min_samples_split</span><span class="o">=</span><span class="n">min_samples_split</span><span class="p">,</span>
            <span class="n">min_samples_leaf</span><span class="o">=</span><span class="n">min_samples_leaf</span><span class="p">,</span>
            <span class="n">min_weight_fraction_leaf</span><span class="o">=</span><span class="n">min_weight_fraction_leaf</span><span class="p">,</span>
            <span class="n">max_depth</span><span class="o">=</span><span class="n">max_depth</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="n">init</span><span class="p">,</span> <span class="n">subsample</span><span class="o">=</span><span class="n">subsample</span><span class="p">,</span>
            <span class="n">max_features</span><span class="o">=</span><span class="n">max_features</span><span class="p">,</span>
            <span class="n">min_impurity_decrease</span><span class="o">=</span><span class="n">min_impurity_decrease</span><span class="p">,</span>
            <span class="n">min_impurity_split</span><span class="o">=</span><span class="n">min_impurity_split</span><span class="p">,</span>
            <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
            <span class="n">max_leaf_nodes</span><span class="o">=</span><span class="n">max_leaf_nodes</span><span class="p">,</span> <span class="n">warm_start</span><span class="o">=</span><span class="n">warm_start</span><span class="p">,</span>
            <span class="n">presort</span><span class="o">=</span><span class="n">presort</span><span class="p">,</span> <span class="n">validation_fraction</span><span class="o">=</span><span class="n">validation_fraction</span><span class="p">,</span>
            <span class="n">n_iter_no_change</span><span class="o">=</span><span class="n">n_iter_no_change</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="n">tol</span><span class="p">,</span> <span class="n">ccp_alpha</span><span class="o">=</span><span class="n">ccp_alpha</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Predict regression target for X.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            The input samples. Internally, it will be converted to</span>
<span class="sd">            ``dtype=np.float32`` and if a sparse matrix is provided</span>
<span class="sd">            to a sparse ``csr_matrix``.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        y : ndarray of shape (n_samples,)</span>
<span class="sd">            The predicted values.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">DTYPE</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="s2">&quot;C&quot;</span><span class="p">,</span> <span class="n">accept_sparse</span><span class="o">=</span><span class="s1">&#39;csr&#39;</span><span class="p">)</span>
        <span class="c1"># In regression we can directly return the raw value from the trees.</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_raw_predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">staged_predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Predict regression target at each stage for X.</span>

<span class="sd">        This method allows monitoring (i.e. determine error on testing set)</span>
<span class="sd">        after each stage.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            The input samples. Internally, it will be converted to</span>
<span class="sd">            ``dtype=np.float32`` and if a sparse matrix is provided</span>
<span class="sd">            to a sparse ``csr_matrix``.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        y : generator of ndarray of shape (n_samples,)</span>
<span class="sd">            The predicted value of the input samples.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">raw_predictions</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_staged_raw_predict</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
            <span class="k">yield</span> <span class="n">raw_predictions</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">apply</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Apply trees in the ensemble to X, return leaf indices.</span>

<span class="sd">        .. versionadded:: 0.17</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            The input samples. Internally, its dtype will be converted to</span>
<span class="sd">            ``dtype=np.float32``. If a sparse matrix is provided, it will</span>
<span class="sd">            be converted to a sparse ``csr_matrix``.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        X_leaves : array-like of shape (n_samples, n_estimators)</span>
<span class="sd">            For each datapoint x in X and for each tree in the ensemble,</span>
<span class="sd">            return the index of the leaf x ends up in each estimator.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">leaves</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">leaves</span> <span class="o">=</span> <span class="n">leaves</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">leaves</span>
</pre></div>

              </div>
              
              
              <div class='prev-next-bottom'>
                

              </div>
              
          </main>
          

      </div>
    </div>

    
  <script src="../../../_static/js/index.d3f166471bb80abb5163.js"></script>


    <footer class="footer mt-5 mt-md-0">
  <div class="container">
    <p>
          &copy; Copyright 2021, Boston Consulting Group (BCG).<br/>
        Created using <a href="http://sphinx-doc.org/">Sphinx</a> 3.4.3.<br/>
    </p>
  </div>
</footer>
  </body>
</html>