
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>sklearn.feature_extraction.text &#8212; sklearndf  documentation</title>
    
  <link rel="stylesheet" href="../../../_static/css/index.f658d18f9b420779cfdf24aa0a7e2d77.css">

    
  <link rel="stylesheet"
    href="../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../../../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../../../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/basic.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/gamma.css" />
    
  <link rel="preload" as="script" href="../../../_static/js/index.d3f166471bb80abb5163.js">

    <script id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/js/gamma.js"></script>
    <script src="../../../_static/js/versions.js"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main">
<div class="container-xl">

    <a class="navbar-brand" href="../../../index.html">
    
      <img src="../../../_static/gamma_sklearndf_logo.png" class="logo" alt="logo" />
    
    </a>
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-menu" aria-controls="navbar-menu" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar-menu" class="col-lg-9 collapse navbar-collapse">
      <ul id="navbar-main-elements" class="navbar-nav mr-auto">
        
        
        <li class="nav-item ">
            <a class="nav-link" href="../../../getting_started/getting_started.html">Getting started</a>
        </li>
        
        <li class="nav-item ">
            <a class="nav-link" href="../../../apidoc/sklearndf.html">API reference</a>
        </li>
        
        <li class="nav-item ">
            <a class="nav-link" href="../../../tutorials.html">Tutorials</a>
        </li>
        
        <li class="nav-item ">
            <a class="nav-link" href="../../../contribution_guide.html">Development Guidelines</a>
        </li>
        
        <li class="nav-item ">
            <a class="nav-link" href="../../../faqs.html">FAQ</a>
        </li>
        
        <li class="nav-item ">
            <a class="nav-link" href="../../../release_notes.html">Release Notes</a>
        </li>
        
        
      </ul>


      

      <ul class="navbar-nav">
        
        
      </ul>
    </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
          <div class="col-12 col-md-3 bd-sidebar"><form class="bd-search d-flex align-items-center" action="../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">

    <div class="bd-toc-item active">
    
  
    <ul class="nav bd-sidenav">
        
        
        
        
        
        
        
        
        
        
        
        
        
      </ul>
  
  </nav>
          </div>
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
              

<nav id="bd-toc-nav">
    <ul class="nav section-nav flex-column">
    
    </ul>
</nav>


              
          </div>
          

          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <h1>Source code for sklearn.feature_extraction.text</h1><div class="highlight"><pre>
<span></span><span class="c1"># -*- coding: utf-8 -*-</span>
<span class="c1"># Authors: Olivier Grisel &lt;olivier.grisel@ensta.org&gt;</span>
<span class="c1">#          Mathieu Blondel &lt;mathieu@mblondel.org&gt;</span>
<span class="c1">#          Lars Buitinck</span>
<span class="c1">#          Robert Layton &lt;robertlayton@gmail.com&gt;</span>
<span class="c1">#          Jochen Wersd√∂rfer &lt;jochen@wersdoerfer.de&gt;</span>
<span class="c1">#          Roman Sinayev &lt;roman.sinayev@gmail.com&gt;</span>
<span class="c1">#</span>
<span class="c1"># License: BSD 3 clause</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">The :mod:`sklearn.feature_extraction.text` submodule gathers utilities to</span>
<span class="sd">build feature vectors from text documents.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">import</span> <span class="nn">array</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span>
<span class="kn">from</span> <span class="nn">collections.abc</span> <span class="kn">import</span> <span class="n">Mapping</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span>
<span class="kn">import</span> <span class="nn">numbers</span>
<span class="kn">from</span> <span class="nn">operator</span> <span class="kn">import</span> <span class="n">itemgetter</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">unicodedata</span>
<span class="kn">import</span> <span class="nn">warnings</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">scipy.sparse</span> <span class="k">as</span> <span class="nn">sp</span>

<span class="kn">from</span> <span class="nn">..base</span> <span class="kn">import</span> <span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">TransformerMixin</span>
<span class="kn">from</span> <span class="nn">..preprocessing</span> <span class="kn">import</span> <span class="n">normalize</span>
<span class="kn">from</span> <span class="nn">._hash</span> <span class="kn">import</span> <span class="n">FeatureHasher</span>
<span class="kn">from</span> <span class="nn">._stop_words</span> <span class="kn">import</span> <span class="n">ENGLISH_STOP_WORDS</span>
<span class="kn">from</span> <span class="nn">..utils.validation</span> <span class="kn">import</span> <span class="n">check_is_fitted</span><span class="p">,</span> <span class="n">check_array</span><span class="p">,</span> <span class="n">FLOAT_DTYPES</span>
<span class="kn">from</span> <span class="nn">..utils</span> <span class="kn">import</span> <span class="n">_IS_32BIT</span><span class="p">,</span> <span class="n">deprecated</span>
<span class="kn">from</span> <span class="nn">..utils.fixes</span> <span class="kn">import</span> <span class="n">_astype_copy_false</span>
<span class="kn">from</span> <span class="nn">..exceptions</span> <span class="kn">import</span> <span class="n">NotFittedError</span>
<span class="kn">from</span> <span class="nn">..utils.validation</span> <span class="kn">import</span> <span class="n">_deprecate_positional_args</span>


<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;HashingVectorizer&#39;</span><span class="p">,</span>
           <span class="s1">&#39;CountVectorizer&#39;</span><span class="p">,</span>
           <span class="s1">&#39;ENGLISH_STOP_WORDS&#39;</span><span class="p">,</span>
           <span class="s1">&#39;TfidfTransformer&#39;</span><span class="p">,</span>
           <span class="s1">&#39;TfidfVectorizer&#39;</span><span class="p">,</span>
           <span class="s1">&#39;strip_accents_ascii&#39;</span><span class="p">,</span>
           <span class="s1">&#39;strip_accents_unicode&#39;</span><span class="p">,</span>
           <span class="s1">&#39;strip_tags&#39;</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">_preprocess</span><span class="p">(</span><span class="n">doc</span><span class="p">,</span> <span class="n">accent_function</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Chain together an optional series of text preprocessing steps to</span>
<span class="sd">    apply to a document.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    doc: str</span>
<span class="sd">        The string to preprocess</span>
<span class="sd">    accent_function: callable, default=None</span>
<span class="sd">        Function for handling accented characters. Common strategies include</span>
<span class="sd">        normalizing and removing.</span>
<span class="sd">    lower: bool, default=False</span>
<span class="sd">        Whether to use str.lower to lowercase all fo the text</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    doc: str</span>
<span class="sd">        preprocessed string</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">lower</span><span class="p">:</span>
        <span class="n">doc</span> <span class="o">=</span> <span class="n">doc</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">accent_function</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">doc</span> <span class="o">=</span> <span class="n">accent_function</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">doc</span>


<span class="k">def</span> <span class="nf">_analyze</span><span class="p">(</span><span class="n">doc</span><span class="p">,</span> <span class="n">analyzer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ngrams</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
             <span class="n">preprocessor</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">decoder</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stop_words</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Chain together an optional series of text processing steps to go from</span>
<span class="sd">    a single document to ngrams, with or without tokenizing or preprocessing.</span>

<span class="sd">    If analyzer is used, only the decoder argument is used, as the analyzer is</span>
<span class="sd">    intended to replace the preprocessor, tokenizer, and ngrams steps.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    analyzer: callable, default=None</span>
<span class="sd">    tokenizer: callable, default=None</span>
<span class="sd">    ngrams: callable, default=None</span>
<span class="sd">    preprocessor: callable, default=None</span>
<span class="sd">    decoder: callable, default=None</span>
<span class="sd">    stop_words: list, default=None</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    ngrams: list</span>
<span class="sd">        A sequence of tokens, possibly with pairs, triples, etc.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">decoder</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">doc</span> <span class="o">=</span> <span class="n">decoder</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">analyzer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">doc</span> <span class="o">=</span> <span class="n">analyzer</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">preprocessor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">doc</span> <span class="o">=</span> <span class="n">preprocessor</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">tokenizer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">doc</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">ngrams</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">stop_words</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">doc</span> <span class="o">=</span> <span class="n">ngrams</span><span class="p">(</span><span class="n">doc</span><span class="p">,</span> <span class="n">stop_words</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">doc</span> <span class="o">=</span> <span class="n">ngrams</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">doc</span>


<span class="k">def</span> <span class="nf">strip_accents_unicode</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Transform accentuated unicode symbols into their simple counterpart</span>

<span class="sd">    Warning: the python-level loop and join operations make this</span>
<span class="sd">    implementation 20 times slower than the strip_accents_ascii basic</span>
<span class="sd">    normalization.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    s : string</span>
<span class="sd">        The string to strip</span>

<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    strip_accents_ascii</span>
<span class="sd">        Remove accentuated char for any unicode symbol that has a direct</span>
<span class="sd">        ASCII equivalent.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="c1"># If `s` is ASCII-compatible, then it does not contain any accented</span>
        <span class="c1"># characters and we can avoid an expensive list comprehension</span>
        <span class="n">s</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s2">&quot;ASCII&quot;</span><span class="p">,</span> <span class="n">errors</span><span class="o">=</span><span class="s2">&quot;strict&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">s</span>
    <span class="k">except</span> <span class="ne">UnicodeEncodeError</span><span class="p">:</span>
        <span class="n">normalized</span> <span class="o">=</span> <span class="n">unicodedata</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="s1">&#39;NFKD&#39;</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>
        <span class="k">return</span> <span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">c</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">normalized</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">unicodedata</span><span class="o">.</span><span class="n">combining</span><span class="p">(</span><span class="n">c</span><span class="p">)])</span>


<span class="k">def</span> <span class="nf">strip_accents_ascii</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Transform accentuated unicode symbols into ascii or nothing</span>

<span class="sd">    Warning: this solution is only suited for languages that have a direct</span>
<span class="sd">    transliteration to ASCII symbols.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    s : string</span>
<span class="sd">        The string to strip</span>

<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    strip_accents_unicode</span>
<span class="sd">        Remove accentuated char for any unicode symbol.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">nkfd_form</span> <span class="o">=</span> <span class="n">unicodedata</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="s1">&#39;NFKD&#39;</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">nkfd_form</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s1">&#39;ASCII&#39;</span><span class="p">,</span> <span class="s1">&#39;ignore&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s1">&#39;ASCII&#39;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">strip_tags</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Basic regexp based HTML / XML tag stripper function</span>

<span class="sd">    For serious HTML/XML preprocessing you should rather use an external</span>
<span class="sd">    library such as lxml or BeautifulSoup.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    s : string</span>
<span class="sd">        The string to strip</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">re</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;&lt;([^&gt;]+)&gt;&quot;</span><span class="p">,</span> <span class="n">flags</span><span class="o">=</span><span class="n">re</span><span class="o">.</span><span class="n">UNICODE</span><span class="p">)</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="s2">&quot; &quot;</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_check_stop_list</span><span class="p">(</span><span class="n">stop</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">stop</span> <span class="o">==</span> <span class="s2">&quot;english&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">ENGLISH_STOP_WORDS</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">stop</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;not a built-in stop list: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">stop</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">stop</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">None</span>
    <span class="k">else</span><span class="p">:</span>  <span class="c1"># assume it&#39;s a collection</span>
        <span class="k">return</span> <span class="nb">frozenset</span><span class="p">(</span><span class="n">stop</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">_VectorizerMixin</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Provides common code for text vectorizers (tokenization logic).&quot;&quot;&quot;</span>

    <span class="n">_white_spaces</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;\s\s+&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">decode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">doc</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Decode the input into a string of unicode symbols.</span>

<span class="sd">        The decoding strategy depends on the vectorizer parameters.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        doc : str</span>
<span class="sd">            The string to decode.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        doc: str</span>
<span class="sd">            A string of unicode symbols.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">input</span> <span class="o">==</span> <span class="s1">&#39;filename&#39;</span><span class="p">:</span>
            <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">doc</span><span class="p">,</span> <span class="s1">&#39;rb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fh</span><span class="p">:</span>
                <span class="n">doc</span> <span class="o">=</span> <span class="n">fh</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>

        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">input</span> <span class="o">==</span> <span class="s1">&#39;file&#39;</span><span class="p">:</span>
            <span class="n">doc</span> <span class="o">=</span> <span class="n">doc</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">doc</span><span class="p">,</span> <span class="nb">bytes</span><span class="p">):</span>
            <span class="n">doc</span> <span class="o">=</span> <span class="n">doc</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">encoding</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">decode_error</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">doc</span> <span class="ow">is</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;np.nan is an invalid document, expected byte or &quot;</span>
                             <span class="s2">&quot;unicode string.&quot;</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">doc</span>

    <span class="k">def</span> <span class="nf">_word_ngrams</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tokens</span><span class="p">,</span> <span class="n">stop_words</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Turn tokens into a sequence of n-grams after stop words filtering&quot;&quot;&quot;</span>
        <span class="c1"># handle stop words</span>
        <span class="k">if</span> <span class="n">stop_words</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">tokens</span> <span class="k">if</span> <span class="n">w</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stop_words</span><span class="p">]</span>

        <span class="c1"># handle token n-grams</span>
        <span class="n">min_n</span><span class="p">,</span> <span class="n">max_n</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ngram_range</span>
        <span class="k">if</span> <span class="n">max_n</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">original_tokens</span> <span class="o">=</span> <span class="n">tokens</span>
            <span class="k">if</span> <span class="n">min_n</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="c1"># no need to do any slicing for unigrams</span>
                <span class="c1"># just iterate through the original tokens</span>
                <span class="n">tokens</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">original_tokens</span><span class="p">)</span>
                <span class="n">min_n</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">tokens</span> <span class="o">=</span> <span class="p">[]</span>

            <span class="n">n_original_tokens</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">original_tokens</span><span class="p">)</span>

            <span class="c1"># bind method outside of loop to reduce overhead</span>
            <span class="n">tokens_append</span> <span class="o">=</span> <span class="n">tokens</span><span class="o">.</span><span class="n">append</span>
            <span class="n">space_join</span> <span class="o">=</span> <span class="s2">&quot; &quot;</span><span class="o">.</span><span class="n">join</span>

            <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">min_n</span><span class="p">,</span>
                           <span class="nb">min</span><span class="p">(</span><span class="n">max_n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_original_tokens</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)):</span>
                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_original_tokens</span> <span class="o">-</span> <span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
                    <span class="n">tokens_append</span><span class="p">(</span><span class="n">space_join</span><span class="p">(</span><span class="n">original_tokens</span><span class="p">[</span><span class="n">i</span><span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="n">n</span><span class="p">]))</span>

        <span class="k">return</span> <span class="n">tokens</span>

    <span class="k">def</span> <span class="nf">_char_ngrams</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text_document</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Tokenize text_document into a sequence of character n-grams&quot;&quot;&quot;</span>
        <span class="c1"># normalize white spaces</span>
        <span class="n">text_document</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_white_spaces</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="s2">&quot; &quot;</span><span class="p">,</span> <span class="n">text_document</span><span class="p">)</span>

        <span class="n">text_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">text_document</span><span class="p">)</span>
        <span class="n">min_n</span><span class="p">,</span> <span class="n">max_n</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ngram_range</span>
        <span class="k">if</span> <span class="n">min_n</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="c1"># no need to do any slicing for unigrams</span>
            <span class="c1"># iterate through the string</span>
            <span class="n">ngrams</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">text_document</span><span class="p">)</span>
            <span class="n">min_n</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">ngrams</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># bind method outside of loop to reduce overhead</span>
        <span class="n">ngrams_append</span> <span class="o">=</span> <span class="n">ngrams</span><span class="o">.</span><span class="n">append</span>

        <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">min_n</span><span class="p">,</span> <span class="nb">min</span><span class="p">(</span><span class="n">max_n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">text_len</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)):</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">text_len</span> <span class="o">-</span> <span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
                <span class="n">ngrams_append</span><span class="p">(</span><span class="n">text_document</span><span class="p">[</span><span class="n">i</span><span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="n">n</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">ngrams</span>

    <span class="k">def</span> <span class="nf">_char_wb_ngrams</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text_document</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Whitespace sensitive char-n-gram tokenization.</span>

<span class="sd">        Tokenize text_document into a sequence of character n-grams</span>
<span class="sd">        operating only inside word boundaries. n-grams at the edges</span>
<span class="sd">        of words are padded with space.&quot;&quot;&quot;</span>
        <span class="c1"># normalize white spaces</span>
        <span class="n">text_document</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_white_spaces</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="s2">&quot; &quot;</span><span class="p">,</span> <span class="n">text_document</span><span class="p">)</span>

        <span class="n">min_n</span><span class="p">,</span> <span class="n">max_n</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ngram_range</span>
        <span class="n">ngrams</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># bind method outside of loop to reduce overhead</span>
        <span class="n">ngrams_append</span> <span class="o">=</span> <span class="n">ngrams</span><span class="o">.</span><span class="n">append</span>

        <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">text_document</span><span class="o">.</span><span class="n">split</span><span class="p">():</span>
            <span class="n">w</span> <span class="o">=</span> <span class="s1">&#39; &#39;</span> <span class="o">+</span> <span class="n">w</span> <span class="o">+</span> <span class="s1">&#39; &#39;</span>
            <span class="n">w_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">min_n</span><span class="p">,</span> <span class="n">max_n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
                <span class="n">offset</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="n">ngrams_append</span><span class="p">(</span><span class="n">w</span><span class="p">[</span><span class="n">offset</span><span class="p">:</span><span class="n">offset</span> <span class="o">+</span> <span class="n">n</span><span class="p">])</span>
                <span class="k">while</span> <span class="n">offset</span> <span class="o">+</span> <span class="n">n</span> <span class="o">&lt;</span> <span class="n">w_len</span><span class="p">:</span>
                    <span class="n">offset</span> <span class="o">+=</span> <span class="mi">1</span>
                    <span class="n">ngrams_append</span><span class="p">(</span><span class="n">w</span><span class="p">[</span><span class="n">offset</span><span class="p">:</span><span class="n">offset</span> <span class="o">+</span> <span class="n">n</span><span class="p">])</span>
                <span class="k">if</span> <span class="n">offset</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>   <span class="c1"># count a short word (w_len &lt; n) only once</span>
                    <span class="k">break</span>
        <span class="k">return</span> <span class="n">ngrams</span>

    <span class="k">def</span> <span class="nf">build_preprocessor</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Return a function to preprocess the text before tokenization.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        preprocessor: callable</span>
<span class="sd">              A function to preprocess the text before tokenization.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">preprocessor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">preprocessor</span>

        <span class="c1"># accent stripping</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">strip_accents</span><span class="p">:</span>
            <span class="n">strip_accents</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">elif</span> <span class="n">callable</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">strip_accents</span><span class="p">):</span>
            <span class="n">strip_accents</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">strip_accents</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">strip_accents</span> <span class="o">==</span> <span class="s1">&#39;ascii&#39;</span><span class="p">:</span>
            <span class="n">strip_accents</span> <span class="o">=</span> <span class="n">strip_accents_ascii</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">strip_accents</span> <span class="o">==</span> <span class="s1">&#39;unicode&#39;</span><span class="p">:</span>
            <span class="n">strip_accents</span> <span class="o">=</span> <span class="n">strip_accents_unicode</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Invalid value for &quot;strip_accents&quot;: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span>
                             <span class="bp">self</span><span class="o">.</span><span class="n">strip_accents</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">partial</span><span class="p">(</span>
            <span class="n">_preprocess</span><span class="p">,</span> <span class="n">accent_function</span><span class="o">=</span><span class="n">strip_accents</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">lowercase</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">build_tokenizer</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Return a function that splits a string into a sequence of tokens.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        tokenizer: callable</span>
<span class="sd">              A function to split a string into a sequence of tokens.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span>
        <span class="n">token_pattern</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">token_pattern</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">token_pattern</span><span class="o">.</span><span class="n">findall</span>

    <span class="k">def</span> <span class="nf">get_stop_words</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Build or fetch the effective stop words list.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        stop_words: list or None</span>
<span class="sd">                A list of stop words.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">_check_stop_list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">stop_words</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_check_stop_words_consistency</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">stop_words</span><span class="p">,</span> <span class="n">preprocess</span><span class="p">,</span> <span class="n">tokenize</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Check if stop words are consistent</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        is_consistent : True if stop words are consistent with the preprocessor</span>
<span class="sd">                        and tokenizer, False if they are not, None if the check</span>
<span class="sd">                        was previously performed, &quot;error&quot; if it could not be</span>
<span class="sd">                        performed (e.g. because of the use of a custom</span>
<span class="sd">                        preprocessor / tokenizer)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">id</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">stop_words</span><span class="p">)</span> <span class="o">==</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_stop_words_id&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">):</span>
            <span class="c1"># Stop words are were previously validated</span>
            <span class="k">return</span> <span class="kc">None</span>

        <span class="c1"># NB: stop_words is validated, unlike self.stop_words</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">inconsistent</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
            <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">stop_words</span> <span class="ow">or</span> <span class="p">():</span>
                <span class="n">tokens</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">tokenize</span><span class="p">(</span><span class="n">preprocess</span><span class="p">(</span><span class="n">w</span><span class="p">)))</span>
                <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">token</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stop_words</span><span class="p">:</span>
                        <span class="n">inconsistent</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_stop_words_id</span> <span class="o">=</span> <span class="nb">id</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">stop_words</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">inconsistent</span><span class="p">:</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s1">&#39;Your stop_words may be inconsistent with &#39;</span>
                              <span class="s1">&#39;your preprocessing. Tokenizing the stop &#39;</span>
                              <span class="s1">&#39;words generated tokens </span><span class="si">%r</span><span class="s1"> not in &#39;</span>
                              <span class="s1">&#39;stop_words.&#39;</span> <span class="o">%</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">inconsistent</span><span class="p">))</span>
            <span class="k">return</span> <span class="ow">not</span> <span class="n">inconsistent</span>
        <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
            <span class="c1"># Failed to check stop words consistency (e.g. because a custom</span>
            <span class="c1"># preprocessor or tokenizer was used)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_stop_words_id</span> <span class="o">=</span> <span class="nb">id</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">stop_words</span><span class="p">)</span>
            <span class="k">return</span> <span class="s1">&#39;error&#39;</span>

    <span class="k">def</span> <span class="nf">build_analyzer</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Return a callable that handles preprocessing, tokenization</span>
<span class="sd">        and n-grams generation.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        analyzer: callable</span>
<span class="sd">            A function to handle preprocessing, tokenization</span>
<span class="sd">            and n-grams generation.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="n">callable</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">analyzer</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">partial</span><span class="p">(</span>
                <span class="n">_analyze</span><span class="p">,</span> <span class="n">analyzer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">analyzer</span><span class="p">,</span> <span class="n">decoder</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">decode</span>
            <span class="p">)</span>

        <span class="n">preprocess</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">build_preprocessor</span><span class="p">()</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">analyzer</span> <span class="o">==</span> <span class="s1">&#39;char&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">partial</span><span class="p">(</span><span class="n">_analyze</span><span class="p">,</span> <span class="n">ngrams</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_char_ngrams</span><span class="p">,</span>
                           <span class="n">preprocessor</span><span class="o">=</span><span class="n">preprocess</span><span class="p">,</span> <span class="n">decoder</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">decode</span><span class="p">)</span>

        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">analyzer</span> <span class="o">==</span> <span class="s1">&#39;char_wb&#39;</span><span class="p">:</span>

            <span class="k">return</span> <span class="n">partial</span><span class="p">(</span><span class="n">_analyze</span><span class="p">,</span> <span class="n">ngrams</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_char_wb_ngrams</span><span class="p">,</span>
                           <span class="n">preprocessor</span><span class="o">=</span><span class="n">preprocess</span><span class="p">,</span> <span class="n">decoder</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">decode</span><span class="p">)</span>

        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">analyzer</span> <span class="o">==</span> <span class="s1">&#39;word&#39;</span><span class="p">:</span>
            <span class="n">stop_words</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_stop_words</span><span class="p">()</span>
            <span class="n">tokenize</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">build_tokenizer</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_check_stop_words_consistency</span><span class="p">(</span><span class="n">stop_words</span><span class="p">,</span> <span class="n">preprocess</span><span class="p">,</span>
                                               <span class="n">tokenize</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">partial</span><span class="p">(</span><span class="n">_analyze</span><span class="p">,</span> <span class="n">ngrams</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_word_ngrams</span><span class="p">,</span>
                           <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenize</span><span class="p">,</span> <span class="n">preprocessor</span><span class="o">=</span><span class="n">preprocess</span><span class="p">,</span>
                           <span class="n">decoder</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">decode</span><span class="p">,</span> <span class="n">stop_words</span><span class="o">=</span><span class="n">stop_words</span><span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">%s</span><span class="s1"> is not a valid tokenization scheme/analyzer&#39;</span> <span class="o">%</span>
                             <span class="bp">self</span><span class="o">.</span><span class="n">analyzer</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_validate_vocabulary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">vocabulary</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocabulary</span>
        <span class="k">if</span> <span class="n">vocabulary</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">vocabulary</span><span class="p">,</span> <span class="nb">set</span><span class="p">):</span>
                <span class="n">vocabulary</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">vocabulary</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">vocabulary</span><span class="p">,</span> <span class="n">Mapping</span><span class="p">):</span>
                <span class="n">vocab</span> <span class="o">=</span> <span class="p">{}</span>
                <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">vocabulary</span><span class="p">):</span>
                    <span class="k">if</span> <span class="n">vocab</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span> <span class="o">!=</span> <span class="n">i</span><span class="p">:</span>
                        <span class="n">msg</span> <span class="o">=</span> <span class="s2">&quot;Duplicate term in vocabulary: </span><span class="si">%r</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">t</span>
                        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
                <span class="n">vocabulary</span> <span class="o">=</span> <span class="n">vocab</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">indices</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">vocabulary</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">vocabulary</span><span class="p">):</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Vocabulary contains repeated indices.&quot;</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">vocabulary</span><span class="p">)):</span>
                    <span class="k">if</span> <span class="n">i</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">indices</span><span class="p">:</span>
                        <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;Vocabulary of size </span><span class="si">%d</span><span class="s2"> doesn&#39;t contain index &quot;</span>
                               <span class="s2">&quot;</span><span class="si">%d</span><span class="s2">.&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">vocabulary</span><span class="p">),</span> <span class="n">i</span><span class="p">))</span>
                        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">vocabulary</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;empty vocabulary passed to fit&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">fixed_vocabulary_</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">vocabulary_</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">vocabulary</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">fixed_vocabulary_</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">def</span> <span class="nf">_check_vocabulary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Check if vocabulary is empty or missing (not fitted)&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;vocabulary_&#39;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_validate_vocabulary</span><span class="p">()</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">fixed_vocabulary_</span><span class="p">:</span>
                <span class="k">raise</span> <span class="n">NotFittedError</span><span class="p">(</span><span class="s2">&quot;Vocabulary not fitted or provided&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocabulary_</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Vocabulary is empty&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_validate_params</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Check validity of ngram_range parameter&quot;&quot;&quot;</span>
        <span class="n">min_n</span><span class="p">,</span> <span class="n">max_m</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ngram_range</span>
        <span class="k">if</span> <span class="n">min_n</span> <span class="o">&gt;</span> <span class="n">max_m</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Invalid value for ngram_range=</span><span class="si">%s</span><span class="s2"> &quot;</span>
                <span class="s2">&quot;lower boundary larger than the upper boundary.&quot;</span>
                <span class="o">%</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ngram_range</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">_warn_for_unused_params</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">token_pattern</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;The parameter &#39;token_pattern&#39; will not be used&quot;</span>
                          <span class="s2">&quot; since &#39;tokenizer&#39; is not None&#39;&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">preprocessor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">callable</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">analyzer</span><span class="p">):</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;The parameter &#39;preprocessor&#39; will not be used&quot;</span>
                          <span class="s2">&quot; since &#39;analyzer&#39; is callable&#39;&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ngram_range</span> <span class="o">!=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">ngram_range</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                <span class="ow">and</span> <span class="n">callable</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">analyzer</span><span class="p">)):</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;The parameter &#39;ngram_range&#39; will not be used&quot;</span>
                          <span class="s2">&quot; since &#39;analyzer&#39; is callable&#39;&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">analyzer</span> <span class="o">!=</span> <span class="s1">&#39;word&#39;</span> <span class="ow">or</span> <span class="n">callable</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">analyzer</span><span class="p">):</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">stop_words</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;The parameter &#39;stop_words&#39; will not be used&quot;</span>
                              <span class="s2">&quot; since &#39;analyzer&#39; != &#39;word&#39;&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">token_pattern</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> \
               <span class="bp">self</span><span class="o">.</span><span class="n">token_pattern</span> <span class="o">!=</span> <span class="sa">r</span><span class="s2">&quot;(?u)\b\w\w+\b&quot;</span><span class="p">:</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;The parameter &#39;token_pattern&#39; will not be used&quot;</span>
                              <span class="s2">&quot; since &#39;analyzer&#39; != &#39;word&#39;&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;The parameter &#39;tokenizer&#39; will not be used&quot;</span>
                              <span class="s2">&quot; since &#39;analyzer&#39; != &#39;word&#39;&quot;</span><span class="p">)</span>


<span class="nd">@deprecated</span><span class="p">(</span><span class="s2">&quot;VectorizerMixin is deprecated in version &quot;</span>
            <span class="s2">&quot;0.22 and will be removed in version 0.24.&quot;</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">VectorizerMixin</span><span class="p">(</span><span class="n">_VectorizerMixin</span><span class="p">):</span>
    <span class="k">pass</span>


<span class="k">class</span> <span class="nc">HashingVectorizer</span><span class="p">(</span><span class="n">TransformerMixin</span><span class="p">,</span> <span class="n">_VectorizerMixin</span><span class="p">,</span> <span class="n">BaseEstimator</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Convert a collection of text documents to a matrix of token occurrences</span>

<span class="sd">    It turns a collection of text documents into a scipy.sparse matrix holding</span>
<span class="sd">    token occurrence counts (or binary occurrence information), possibly</span>
<span class="sd">    normalized as token frequencies if norm=&#39;l1&#39; or projected on the euclidean</span>
<span class="sd">    unit sphere if norm=&#39;l2&#39;.</span>

<span class="sd">    This text vectorizer implementation uses the hashing trick to find the</span>
<span class="sd">    token string name to feature integer index mapping.</span>

<span class="sd">    This strategy has several advantages:</span>

<span class="sd">    - it is very low memory scalable to large datasets as there is no need to</span>
<span class="sd">      store a vocabulary dictionary in memory</span>

<span class="sd">    - it is fast to pickle and un-pickle as it holds no state besides the</span>
<span class="sd">      constructor parameters</span>

<span class="sd">    - it can be used in a streaming (partial fit) or parallel pipeline as there</span>
<span class="sd">      is no state computed during fit.</span>

<span class="sd">    There are also a couple of cons (vs using a CountVectorizer with an</span>
<span class="sd">    in-memory vocabulary):</span>

<span class="sd">    - there is no way to compute the inverse transform (from feature indices to</span>
<span class="sd">      string feature names) which can be a problem when trying to introspect</span>
<span class="sd">      which features are most important to a model.</span>

<span class="sd">    - there can be collisions: distinct tokens can be mapped to the same</span>
<span class="sd">      feature index. However in practice this is rarely an issue if n_features</span>
<span class="sd">      is large enough (e.g. 2 ** 18 for text classification problems).</span>

<span class="sd">    - no IDF weighting as this would render the transformer stateful.</span>

<span class="sd">    The hash function employed is the signed 32-bit version of Murmurhash3.</span>

<span class="sd">    Read more in the :ref:`User Guide &lt;text_feature_extraction&gt;`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>

<span class="sd">    input : string {&#39;filename&#39;, &#39;file&#39;, &#39;content&#39;}, default=&#39;content&#39;</span>
<span class="sd">        If &#39;filename&#39;, the sequence passed as an argument to fit is</span>
<span class="sd">        expected to be a list of filenames that need reading to fetch</span>
<span class="sd">        the raw content to analyze.</span>

<span class="sd">        If &#39;file&#39;, the sequence items must have a &#39;read&#39; method (file-like</span>
<span class="sd">        object) that is called to fetch the bytes in memory.</span>

<span class="sd">        Otherwise the input is expected to be a sequence of items that</span>
<span class="sd">        can be of type string or byte.</span>

<span class="sd">    encoding : string, default=&#39;utf-8&#39;</span>
<span class="sd">        If bytes or files are given to analyze, this encoding is used to</span>
<span class="sd">        decode.</span>

<span class="sd">    decode_error : {&#39;strict&#39;, &#39;ignore&#39;, &#39;replace&#39;}, default=&#39;strict&#39;</span>
<span class="sd">        Instruction on what to do if a byte sequence is given to analyze that</span>
<span class="sd">        contains characters not of the given `encoding`. By default, it is</span>
<span class="sd">        &#39;strict&#39;, meaning that a UnicodeDecodeError will be raised. Other</span>
<span class="sd">        values are &#39;ignore&#39; and &#39;replace&#39;.</span>

<span class="sd">    strip_accents : {&#39;ascii&#39;, &#39;unicode&#39;}, default=None</span>
<span class="sd">        Remove accents and perform other character normalization</span>
<span class="sd">        during the preprocessing step.</span>
<span class="sd">        &#39;ascii&#39; is a fast method that only works on characters that have</span>
<span class="sd">        an direct ASCII mapping.</span>
<span class="sd">        &#39;unicode&#39; is a slightly slower method that works on any characters.</span>
<span class="sd">        None (default) does nothing.</span>

<span class="sd">        Both &#39;ascii&#39; and &#39;unicode&#39; use NFKD normalization from</span>
<span class="sd">        :func:`unicodedata.normalize`.</span>

<span class="sd">    lowercase : bool, default=True</span>
<span class="sd">        Convert all characters to lowercase before tokenizing.</span>

<span class="sd">    preprocessor : callable, default=None</span>
<span class="sd">        Override the preprocessing (string transformation) stage while</span>
<span class="sd">        preserving the tokenizing and n-grams generation steps.</span>
<span class="sd">        Only applies if ``analyzer is not callable``.</span>

<span class="sd">    tokenizer : callable, default=None</span>
<span class="sd">        Override the string tokenization step while preserving the</span>
<span class="sd">        preprocessing and n-grams generation steps.</span>
<span class="sd">        Only applies if ``analyzer == &#39;word&#39;``.</span>

<span class="sd">    stop_words : string {&#39;english&#39;}, list, default=None</span>
<span class="sd">        If &#39;english&#39;, a built-in stop word list for English is used.</span>
<span class="sd">        There are several known issues with &#39;english&#39; and you should</span>
<span class="sd">        consider an alternative (see :ref:`stop_words`).</span>

<span class="sd">        If a list, that list is assumed to contain stop words, all of which</span>
<span class="sd">        will be removed from the resulting tokens.</span>
<span class="sd">        Only applies if ``analyzer == &#39;word&#39;``.</span>

<span class="sd">    token_pattern : string</span>
<span class="sd">        Regular expression denoting what constitutes a &quot;token&quot;, only used</span>
<span class="sd">        if ``analyzer == &#39;word&#39;``. The default regexp selects tokens of 2</span>
<span class="sd">        or more alphanumeric characters (punctuation is completely ignored</span>
<span class="sd">        and always treated as a token separator).</span>

<span class="sd">    ngram_range : tuple (min_n, max_n), default=(1, 1)</span>
<span class="sd">        The lower and upper boundary of the range of n-values for different</span>
<span class="sd">        n-grams to be extracted. All values of n such that min_n &lt;= n &lt;= max_n</span>
<span class="sd">        will be used. For example an ``ngram_range`` of ``(1, 1)`` means only</span>
<span class="sd">        unigrams, ``(1, 2)`` means unigrams and bigrams, and ``(2, 2)`` means</span>
<span class="sd">        only bigrams.</span>
<span class="sd">        Only applies if ``analyzer is not callable``.</span>

<span class="sd">    analyzer : string, {&#39;word&#39;, &#39;char&#39;, &#39;char_wb&#39;} or callable, \</span>
<span class="sd">            default=&#39;word&#39;</span>
<span class="sd">        Whether the feature should be made of word or character n-grams.</span>
<span class="sd">        Option &#39;char_wb&#39; creates character n-grams only from text inside</span>
<span class="sd">        word boundaries; n-grams at the edges of words are padded with space.</span>

<span class="sd">        If a callable is passed it is used to extract the sequence of features</span>
<span class="sd">        out of the raw, unprocessed input.</span>

<span class="sd">        .. versionchanged:: 0.21</span>

<span class="sd">        Since v0.21, if ``input`` is ``filename`` or ``file``, the data is</span>
<span class="sd">        first read from the file and then passed to the given callable</span>
<span class="sd">        analyzer.</span>

<span class="sd">    n_features : int, default=(2 ** 20)</span>
<span class="sd">        The number of features (columns) in the output matrices. Small numbers</span>
<span class="sd">        of features are likely to cause hash collisions, but large numbers</span>
<span class="sd">        will cause larger coefficient dimensions in linear learners.</span>

<span class="sd">    binary : bool, default=False.</span>
<span class="sd">        If True, all non zero counts are set to 1. This is useful for discrete</span>
<span class="sd">        probabilistic models that model binary events rather than integer</span>
<span class="sd">        counts.</span>

<span class="sd">    norm : {&#39;l1&#39;, &#39;l2&#39;}, default=&#39;l2&#39;</span>
<span class="sd">        Norm used to normalize term vectors. None for no normalization.</span>

<span class="sd">    alternate_sign : bool, default=True</span>
<span class="sd">        When True, an alternating sign is added to the features as to</span>
<span class="sd">        approximately conserve the inner product in the hashed space even for</span>
<span class="sd">        small n_features. This approach is similar to sparse random projection.</span>

<span class="sd">        .. versionadded:: 0.19</span>

<span class="sd">    dtype : type, default=np.float64</span>
<span class="sd">        Type of the matrix returned by fit_transform() or transform().</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.feature_extraction.text import HashingVectorizer</span>
<span class="sd">    &gt;&gt;&gt; corpus = [</span>
<span class="sd">    ...     &#39;This is the first document.&#39;,</span>
<span class="sd">    ...     &#39;This document is the second document.&#39;,</span>
<span class="sd">    ...     &#39;And this is the third one.&#39;,</span>
<span class="sd">    ...     &#39;Is this the first document?&#39;,</span>
<span class="sd">    ... ]</span>
<span class="sd">    &gt;&gt;&gt; vectorizer = HashingVectorizer(n_features=2**4)</span>
<span class="sd">    &gt;&gt;&gt; X = vectorizer.fit_transform(corpus)</span>
<span class="sd">    &gt;&gt;&gt; print(X.shape)</span>
<span class="sd">    (4, 16)</span>

<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    CountVectorizer, TfidfVectorizer</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@_deprecate_positional_args</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="s1">&#39;content&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">,</span>
                 <span class="n">decode_error</span><span class="o">=</span><span class="s1">&#39;strict&#39;</span><span class="p">,</span> <span class="n">strip_accents</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">lowercase</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">preprocessor</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">stop_words</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">token_pattern</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;(?u)\b\w\w+\b&quot;</span><span class="p">,</span>
                 <span class="n">ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">analyzer</span><span class="o">=</span><span class="s1">&#39;word&#39;</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span> <span class="o">**</span> <span class="mi">20</span><span class="p">),</span>
                 <span class="n">binary</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="n">alternate_sign</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input</span> <span class="o">=</span> <span class="nb">input</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoding</span> <span class="o">=</span> <span class="n">encoding</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decode_error</span> <span class="o">=</span> <span class="n">decode_error</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">strip_accents</span> <span class="o">=</span> <span class="n">strip_accents</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">preprocessor</span> <span class="o">=</span> <span class="n">preprocessor</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tokenizer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">analyzer</span> <span class="o">=</span> <span class="n">analyzer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lowercase</span> <span class="o">=</span> <span class="n">lowercase</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">token_pattern</span> <span class="o">=</span> <span class="n">token_pattern</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stop_words</span> <span class="o">=</span> <span class="n">stop_words</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_features</span> <span class="o">=</span> <span class="n">n_features</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ngram_range</span> <span class="o">=</span> <span class="n">ngram_range</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">binary</span> <span class="o">=</span> <span class="n">binary</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">norm</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alternate_sign</span> <span class="o">=</span> <span class="n">alternate_sign</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">dtype</span>

    <span class="k">def</span> <span class="nf">partial_fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Does nothing: this transformer is stateless.</span>

<span class="sd">        This method is just there to mark the fact that this transformer</span>
<span class="sd">        can work in a streaming setup.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : ndarray of shape [n_samples, n_features]</span>
<span class="sd">            Training data.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Does nothing: this transformer is stateless.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : ndarray of shape [n_samples, n_features]</span>
<span class="sd">            Training data.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># triggers a parameter validation</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Iterable over raw text documents expected, &quot;</span>
                <span class="s2">&quot;string object received.&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_warn_for_unused_params</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_validate_params</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_get_hasher</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Transform a sequence of documents to a document-term matrix.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : iterable over raw text documents, length = n_samples</span>
<span class="sd">            Samples. Each sample must be a text document (either bytes or</span>
<span class="sd">            unicode strings, file name or file object depending on the</span>
<span class="sd">            constructor argument) which will be tokenized and hashed.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        X : sparse matrix of shape (n_samples, n_features)</span>
<span class="sd">            Document-term matrix.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Iterable over raw text documents expected, &quot;</span>
                <span class="s2">&quot;string object received.&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_validate_params</span><span class="p">()</span>

        <span class="n">analyzer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">build_analyzer</span><span class="p">()</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_hasher</span><span class="p">()</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">analyzer</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">X</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">binary</span><span class="p">:</span>
            <span class="n">X</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">fill</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">X</span> <span class="o">=</span> <span class="n">normalize</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">X</span>

    <span class="k">def</span> <span class="nf">fit_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Transform a sequence of documents to a document-term matrix.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : iterable over raw text documents, length = n_samples</span>
<span class="sd">            Samples. Each sample must be a text document (either bytes or</span>
<span class="sd">            unicode strings, file name or file object depending on the</span>
<span class="sd">            constructor argument) which will be tokenized and hashed.</span>
<span class="sd">        y : any</span>
<span class="sd">            Ignored. This parameter exists only for compatibility with</span>
<span class="sd">            sklearn.pipeline.Pipeline.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        X : sparse matrix of shape (n_samples, n_features)</span>
<span class="sd">            Document-term matrix.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_get_hasher</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">FeatureHasher</span><span class="p">(</span><span class="n">n_features</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_features</span><span class="p">,</span>
                             <span class="n">input_type</span><span class="o">=</span><span class="s1">&#39;string&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                             <span class="n">alternate_sign</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">alternate_sign</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_more_tags</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;X_types&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;string&#39;</span><span class="p">]}</span>


<span class="k">def</span> <span class="nf">_document_frequency</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Count the number of non-zero values for each feature in sparse X.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">sp</span><span class="o">.</span><span class="n">isspmatrix_csr</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">bincount</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span> <span class="n">minlength</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">diff</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">indptr</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">CountVectorizer</span><span class="p">(</span><span class="n">_VectorizerMixin</span><span class="p">,</span> <span class="n">BaseEstimator</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Convert a collection of text documents to a matrix of token counts</span>

<span class="sd">    This implementation produces a sparse representation of the counts using</span>
<span class="sd">    scipy.sparse.csr_matrix.</span>

<span class="sd">    If you do not provide an a-priori dictionary and you do not use an analyzer</span>
<span class="sd">    that does some kind of feature selection then the number of features will</span>
<span class="sd">    be equal to the vocabulary size found by analyzing the data.</span>

<span class="sd">    Read more in the :ref:`User Guide &lt;text_feature_extraction&gt;`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    input : string {&#39;filename&#39;, &#39;file&#39;, &#39;content&#39;}, default=&#39;content&#39;</span>
<span class="sd">        If &#39;filename&#39;, the sequence passed as an argument to fit is</span>
<span class="sd">        expected to be a list of filenames that need reading to fetch</span>
<span class="sd">        the raw content to analyze.</span>

<span class="sd">        If &#39;file&#39;, the sequence items must have a &#39;read&#39; method (file-like</span>
<span class="sd">        object) that is called to fetch the bytes in memory.</span>

<span class="sd">        Otherwise the input is expected to be a sequence of items that</span>
<span class="sd">        can be of type string or byte.</span>

<span class="sd">    encoding : string, default=&#39;utf-8&#39;</span>
<span class="sd">        If bytes or files are given to analyze, this encoding is used to</span>
<span class="sd">        decode.</span>

<span class="sd">    decode_error : {&#39;strict&#39;, &#39;ignore&#39;, &#39;replace&#39;}, default=&#39;strict&#39;</span>
<span class="sd">        Instruction on what to do if a byte sequence is given to analyze that</span>
<span class="sd">        contains characters not of the given `encoding`. By default, it is</span>
<span class="sd">        &#39;strict&#39;, meaning that a UnicodeDecodeError will be raised. Other</span>
<span class="sd">        values are &#39;ignore&#39; and &#39;replace&#39;.</span>

<span class="sd">    strip_accents : {&#39;ascii&#39;, &#39;unicode&#39;}, default=None</span>
<span class="sd">        Remove accents and perform other character normalization</span>
<span class="sd">        during the preprocessing step.</span>
<span class="sd">        &#39;ascii&#39; is a fast method that only works on characters that have</span>
<span class="sd">        an direct ASCII mapping.</span>
<span class="sd">        &#39;unicode&#39; is a slightly slower method that works on any characters.</span>
<span class="sd">        None (default) does nothing.</span>

<span class="sd">        Both &#39;ascii&#39; and &#39;unicode&#39; use NFKD normalization from</span>
<span class="sd">        :func:`unicodedata.normalize`.</span>

<span class="sd">    lowercase : bool, default=True</span>
<span class="sd">        Convert all characters to lowercase before tokenizing.</span>

<span class="sd">    preprocessor : callable, default=None</span>
<span class="sd">        Override the preprocessing (string transformation) stage while</span>
<span class="sd">        preserving the tokenizing and n-grams generation steps.</span>
<span class="sd">        Only applies if ``analyzer is not callable``.</span>

<span class="sd">    tokenizer : callable, default=None</span>
<span class="sd">        Override the string tokenization step while preserving the</span>
<span class="sd">        preprocessing and n-grams generation steps.</span>
<span class="sd">        Only applies if ``analyzer == &#39;word&#39;``.</span>

<span class="sd">    stop_words : string {&#39;english&#39;}, list, default=None</span>
<span class="sd">        If &#39;english&#39;, a built-in stop word list for English is used.</span>
<span class="sd">        There are several known issues with &#39;english&#39; and you should</span>
<span class="sd">        consider an alternative (see :ref:`stop_words`).</span>

<span class="sd">        If a list, that list is assumed to contain stop words, all of which</span>
<span class="sd">        will be removed from the resulting tokens.</span>
<span class="sd">        Only applies if ``analyzer == &#39;word&#39;``.</span>

<span class="sd">        If None, no stop words will be used. max_df can be set to a value</span>
<span class="sd">        in the range [0.7, 1.0) to automatically detect and filter stop</span>
<span class="sd">        words based on intra corpus document frequency of terms.</span>

<span class="sd">    token_pattern : string</span>
<span class="sd">        Regular expression denoting what constitutes a &quot;token&quot;, only used</span>
<span class="sd">        if ``analyzer == &#39;word&#39;``. The default regexp select tokens of 2</span>
<span class="sd">        or more alphanumeric characters (punctuation is completely ignored</span>
<span class="sd">        and always treated as a token separator).</span>

<span class="sd">    ngram_range : tuple (min_n, max_n), default=(1, 1)</span>
<span class="sd">        The lower and upper boundary of the range of n-values for different</span>
<span class="sd">        word n-grams or char n-grams to be extracted. All values of n such</span>
<span class="sd">        such that min_n &lt;= n &lt;= max_n will be used. For example an</span>
<span class="sd">        ``ngram_range`` of ``(1, 1)`` means only unigrams, ``(1, 2)`` means</span>
<span class="sd">        unigrams and bigrams, and ``(2, 2)`` means only bigrams.</span>
<span class="sd">        Only applies if ``analyzer is not callable``.</span>

<span class="sd">    analyzer : string, {&#39;word&#39;, &#39;char&#39;, &#39;char_wb&#39;} or callable, \</span>
<span class="sd">            default=&#39;word&#39;</span>
<span class="sd">        Whether the feature should be made of word n-gram or character</span>
<span class="sd">        n-grams.</span>
<span class="sd">        Option &#39;char_wb&#39; creates character n-grams only from text inside</span>
<span class="sd">        word boundaries; n-grams at the edges of words are padded with space.</span>

<span class="sd">        If a callable is passed it is used to extract the sequence of features</span>
<span class="sd">        out of the raw, unprocessed input.</span>

<span class="sd">        .. versionchanged:: 0.21</span>

<span class="sd">        Since v0.21, if ``input`` is ``filename`` or ``file``, the data is</span>
<span class="sd">        first read from the file and then passed to the given callable</span>
<span class="sd">        analyzer.</span>

<span class="sd">    max_df : float in range [0.0, 1.0] or int, default=1.0</span>
<span class="sd">        When building the vocabulary ignore terms that have a document</span>
<span class="sd">        frequency strictly higher than the given threshold (corpus-specific</span>
<span class="sd">        stop words).</span>
<span class="sd">        If float, the parameter represents a proportion of documents, integer</span>
<span class="sd">        absolute counts.</span>
<span class="sd">        This parameter is ignored if vocabulary is not None.</span>

<span class="sd">    min_df : float in range [0.0, 1.0] or int, default=1</span>
<span class="sd">        When building the vocabulary ignore terms that have a document</span>
<span class="sd">        frequency strictly lower than the given threshold. This value is also</span>
<span class="sd">        called cut-off in the literature.</span>
<span class="sd">        If float, the parameter represents a proportion of documents, integer</span>
<span class="sd">        absolute counts.</span>
<span class="sd">        This parameter is ignored if vocabulary is not None.</span>

<span class="sd">    max_features : int, default=None</span>
<span class="sd">        If not None, build a vocabulary that only consider the top</span>
<span class="sd">        max_features ordered by term frequency across the corpus.</span>

<span class="sd">        This parameter is ignored if vocabulary is not None.</span>

<span class="sd">    vocabulary : Mapping or iterable, default=None</span>
<span class="sd">        Either a Mapping (e.g., a dict) where keys are terms and values are</span>
<span class="sd">        indices in the feature matrix, or an iterable over terms. If not</span>
<span class="sd">        given, a vocabulary is determined from the input documents. Indices</span>
<span class="sd">        in the mapping should not be repeated and should not have any gap</span>
<span class="sd">        between 0 and the largest index.</span>

<span class="sd">    binary : bool, default=False</span>
<span class="sd">        If True, all non zero counts are set to 1. This is useful for discrete</span>
<span class="sd">        probabilistic models that model binary events rather than integer</span>
<span class="sd">        counts.</span>

<span class="sd">    dtype : type, default=np.int64</span>
<span class="sd">        Type of the matrix returned by fit_transform() or transform().</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    vocabulary_ : dict</span>
<span class="sd">        A mapping of terms to feature indices.</span>

<span class="sd">    fixed_vocabulary_: boolean</span>
<span class="sd">        True if a fixed vocabulary of term to indices mapping</span>
<span class="sd">        is provided by the user</span>

<span class="sd">    stop_words_ : set</span>
<span class="sd">        Terms that were ignored because they either:</span>

<span class="sd">          - occurred in too many documents (`max_df`)</span>
<span class="sd">          - occurred in too few documents (`min_df`)</span>
<span class="sd">          - were cut off by feature selection (`max_features`).</span>

<span class="sd">        This is only available if no vocabulary was given.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.feature_extraction.text import CountVectorizer</span>
<span class="sd">    &gt;&gt;&gt; corpus = [</span>
<span class="sd">    ...     &#39;This is the first document.&#39;,</span>
<span class="sd">    ...     &#39;This document is the second document.&#39;,</span>
<span class="sd">    ...     &#39;And this is the third one.&#39;,</span>
<span class="sd">    ...     &#39;Is this the first document?&#39;,</span>
<span class="sd">    ... ]</span>
<span class="sd">    &gt;&gt;&gt; vectorizer = CountVectorizer()</span>
<span class="sd">    &gt;&gt;&gt; X = vectorizer.fit_transform(corpus)</span>
<span class="sd">    &gt;&gt;&gt; print(vectorizer.get_feature_names())</span>
<span class="sd">    [&#39;and&#39;, &#39;document&#39;, &#39;first&#39;, &#39;is&#39;, &#39;one&#39;, &#39;second&#39;, &#39;the&#39;, &#39;third&#39;, &#39;this&#39;]</span>
<span class="sd">    &gt;&gt;&gt; print(X.toarray())</span>
<span class="sd">    [[0 1 1 1 0 0 1 0 1]</span>
<span class="sd">     [0 2 0 1 0 1 1 0 1]</span>
<span class="sd">     [1 0 0 1 1 0 1 1 1]</span>
<span class="sd">     [0 1 1 1 0 0 1 0 1]]</span>
<span class="sd">    &gt;&gt;&gt; vectorizer2 = CountVectorizer(analyzer=&#39;word&#39;, ngram_range=(2, 2))</span>
<span class="sd">    &gt;&gt;&gt; X2 = vectorizer2.fit_transform(corpus)</span>
<span class="sd">    &gt;&gt;&gt; print(vectorizer2.get_feature_names())</span>
<span class="sd">    [&#39;and this&#39;, &#39;document is&#39;, &#39;first document&#39;, &#39;is the&#39;, &#39;is this&#39;,</span>
<span class="sd">    &#39;second document&#39;, &#39;the first&#39;, &#39;the second&#39;, &#39;the third&#39;, &#39;third one&#39;,</span>
<span class="sd">     &#39;this document&#39;, &#39;this is&#39;, &#39;this the&#39;]</span>
<span class="sd">     &gt;&gt;&gt; print(X2.toarray())</span>
<span class="sd">     [[0 0 1 1 0 0 1 0 0 0 0 1 0]</span>
<span class="sd">     [0 1 0 1 0 1 0 1 0 0 1 0 0]</span>
<span class="sd">     [1 0 0 1 0 0 0 0 1 1 0 1 0]</span>
<span class="sd">     [0 0 1 0 1 0 1 0 0 0 0 0 1]]</span>

<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    HashingVectorizer, TfidfVectorizer</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    The ``stop_words_`` attribute can get large and increase the model size</span>
<span class="sd">    when pickling. This attribute is provided only for introspection and can</span>
<span class="sd">    be safely removed using delattr or set to None before pickling.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@_deprecate_positional_args</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="s1">&#39;content&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">,</span>
                 <span class="n">decode_error</span><span class="o">=</span><span class="s1">&#39;strict&#39;</span><span class="p">,</span> <span class="n">strip_accents</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">lowercase</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">preprocessor</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">stop_words</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">token_pattern</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;(?u)\b\w\w+\b&quot;</span><span class="p">,</span>
                 <span class="n">ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">analyzer</span><span class="o">=</span><span class="s1">&#39;word&#39;</span><span class="p">,</span>
                 <span class="n">max_df</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">min_df</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_features</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">vocabulary</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">binary</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input</span> <span class="o">=</span> <span class="nb">input</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoding</span> <span class="o">=</span> <span class="n">encoding</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decode_error</span> <span class="o">=</span> <span class="n">decode_error</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">strip_accents</span> <span class="o">=</span> <span class="n">strip_accents</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">preprocessor</span> <span class="o">=</span> <span class="n">preprocessor</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tokenizer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">analyzer</span> <span class="o">=</span> <span class="n">analyzer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lowercase</span> <span class="o">=</span> <span class="n">lowercase</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">token_pattern</span> <span class="o">=</span> <span class="n">token_pattern</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stop_words</span> <span class="o">=</span> <span class="n">stop_words</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_df</span> <span class="o">=</span> <span class="n">max_df</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_df</span> <span class="o">=</span> <span class="n">min_df</span>
        <span class="k">if</span> <span class="n">max_df</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">min_df</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;negative value for max_df or min_df&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_features</span> <span class="o">=</span> <span class="n">max_features</span>
        <span class="k">if</span> <span class="n">max_features</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="p">(</span><span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">max_features</span><span class="p">,</span> <span class="n">numbers</span><span class="o">.</span><span class="n">Integral</span><span class="p">)</span> <span class="ow">or</span>
                    <span class="n">max_features</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;max_features=</span><span class="si">%r</span><span class="s2">, neither a positive integer nor None&quot;</span>
                    <span class="o">%</span> <span class="n">max_features</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ngram_range</span> <span class="o">=</span> <span class="n">ngram_range</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vocabulary</span> <span class="o">=</span> <span class="n">vocabulary</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">binary</span> <span class="o">=</span> <span class="n">binary</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">dtype</span>

    <span class="k">def</span> <span class="nf">_sort_features</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">vocabulary</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sort features by name</span>

<span class="sd">        Returns a reordered matrix and modifies the vocabulary in place</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">sorted_features</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">vocabulary</span><span class="o">.</span><span class="n">items</span><span class="p">())</span>
        <span class="n">map_index</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">sorted_features</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">indices</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">new_val</span><span class="p">,</span> <span class="p">(</span><span class="n">term</span><span class="p">,</span> <span class="n">old_val</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">sorted_features</span><span class="p">):</span>
            <span class="n">vocabulary</span><span class="p">[</span><span class="n">term</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_val</span>
            <span class="n">map_index</span><span class="p">[</span><span class="n">old_val</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_val</span>

        <span class="n">X</span><span class="o">.</span><span class="n">indices</span> <span class="o">=</span> <span class="n">map_index</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;clip&#39;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">X</span>

    <span class="k">def</span> <span class="nf">_limit_features</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">vocabulary</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">low</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                        <span class="n">limit</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Remove too rare or too common features.</span>

<span class="sd">        Prune features that are non zero in more samples than high or less</span>
<span class="sd">        documents than low, modifying the vocabulary, and restricting it to</span>
<span class="sd">        at most the limit most frequent.</span>

<span class="sd">        This does not prune samples with zero features.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">high</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">low</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">limit</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="nb">set</span><span class="p">()</span>

        <span class="c1"># Calculate a mask based on document frequencies</span>
        <span class="n">dfs</span> <span class="o">=</span> <span class="n">_document_frequency</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">dfs</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">bool</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">high</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">mask</span> <span class="o">&amp;=</span> <span class="n">dfs</span> <span class="o">&lt;=</span> <span class="n">high</span>
        <span class="k">if</span> <span class="n">low</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">mask</span> <span class="o">&amp;=</span> <span class="n">dfs</span> <span class="o">&gt;=</span> <span class="n">low</span>
        <span class="k">if</span> <span class="n">limit</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">mask</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">&gt;</span> <span class="n">limit</span><span class="p">:</span>
            <span class="n">tfs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
            <span class="n">mask_inds</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="n">tfs</span><span class="p">[</span><span class="n">mask</span><span class="p">])</span><span class="o">.</span><span class="n">argsort</span><span class="p">()[:</span><span class="n">limit</span><span class="p">]</span>
            <span class="n">new_mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">dfs</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">bool</span><span class="p">)</span>
            <span class="n">new_mask</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">mask</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="n">mask_inds</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="n">new_mask</span>

        <span class="n">new_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">mask</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>  <span class="c1"># maps old indices to new</span>
        <span class="n">removed_terms</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">term</span><span class="p">,</span> <span class="n">old_index</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="n">vocabulary</span><span class="o">.</span><span class="n">items</span><span class="p">()):</span>
            <span class="k">if</span> <span class="n">mask</span><span class="p">[</span><span class="n">old_index</span><span class="p">]:</span>
                <span class="n">vocabulary</span><span class="p">[</span><span class="n">term</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_indices</span><span class="p">[</span><span class="n">old_index</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">del</span> <span class="n">vocabulary</span><span class="p">[</span><span class="n">term</span><span class="p">]</span>
                <span class="n">removed_terms</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">term</span><span class="p">)</span>
        <span class="n">kept_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">mask</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">kept_indices</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;After pruning, no terms remain. Try a lower&quot;</span>
                             <span class="s2">&quot; min_df or a higher max_df.&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">X</span><span class="p">[:,</span> <span class="n">kept_indices</span><span class="p">],</span> <span class="n">removed_terms</span>

    <span class="k">def</span> <span class="nf">_count_vocab</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">raw_documents</span><span class="p">,</span> <span class="n">fixed_vocab</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Create sparse feature matrix, and vocabulary where fixed_vocab=False</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">fixed_vocab</span><span class="p">:</span>
            <span class="n">vocabulary</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocabulary_</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Add a new value when a new vocabulary item is seen</span>
            <span class="n">vocabulary</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">()</span>
            <span class="n">vocabulary</span><span class="o">.</span><span class="n">default_factory</span> <span class="o">=</span> <span class="n">vocabulary</span><span class="o">.</span><span class="fm">__len__</span>

        <span class="n">analyze</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">build_analyzer</span><span class="p">()</span>
        <span class="n">j_indices</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">indptr</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="n">values</span> <span class="o">=</span> <span class="n">_make_int_array</span><span class="p">()</span>
        <span class="n">indptr</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">raw_documents</span><span class="p">:</span>
            <span class="n">feature_counter</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="k">for</span> <span class="n">feature</span> <span class="ow">in</span> <span class="n">analyze</span><span class="p">(</span><span class="n">doc</span><span class="p">):</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="n">feature_idx</span> <span class="o">=</span> <span class="n">vocabulary</span><span class="p">[</span><span class="n">feature</span><span class="p">]</span>
                    <span class="k">if</span> <span class="n">feature_idx</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">feature_counter</span><span class="p">:</span>
                        <span class="n">feature_counter</span><span class="p">[</span><span class="n">feature_idx</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">feature_counter</span><span class="p">[</span><span class="n">feature_idx</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
                    <span class="c1"># Ignore out-of-vocabulary items for fixed_vocab=True</span>
                    <span class="k">continue</span>

            <span class="n">j_indices</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">feature_counter</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
            <span class="n">values</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">feature_counter</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>
            <span class="n">indptr</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">j_indices</span><span class="p">))</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">fixed_vocab</span><span class="p">:</span>
            <span class="c1"># disable defaultdict behaviour</span>
            <span class="n">vocabulary</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">vocabulary</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">vocabulary</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;empty vocabulary; perhaps the documents only&quot;</span>
                                 <span class="s2">&quot; contain stop words&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">indptr</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">iinfo</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">:</span>  <span class="c1"># = 2**31 - 1</span>
            <span class="k">if</span> <span class="n">_IS_32BIT</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">((</span><span class="s1">&#39;sparse CSR array has </span><span class="si">{}</span><span class="s1"> non-zero &#39;</span>
                                  <span class="s1">&#39;elements and requires 64 bit indexing, &#39;</span>
                                  <span class="s1">&#39;which is unsupported with 32 bit Python.&#39;</span><span class="p">)</span>
                                 <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">indptr</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
            <span class="n">indices_dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">int64</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="n">indices_dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">int32</span>
        <span class="n">j_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">j_indices</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">indices_dtype</span><span class="p">)</span>
        <span class="n">indptr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">indptr</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">indices_dtype</span><span class="p">)</span>
        <span class="n">values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">frombuffer</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">intc</span><span class="p">)</span>

        <span class="n">X</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">csr_matrix</span><span class="p">((</span><span class="n">values</span><span class="p">,</span> <span class="n">j_indices</span><span class="p">,</span> <span class="n">indptr</span><span class="p">),</span>
                          <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">indptr</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">vocabulary</span><span class="p">)),</span>
                          <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">X</span><span class="o">.</span><span class="n">sort_indices</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">vocabulary</span><span class="p">,</span> <span class="n">X</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">raw_documents</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Learn a vocabulary dictionary of all tokens in the raw documents.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        raw_documents : iterable</span>
<span class="sd">            An iterable which yields either str, unicode or file objects.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        self</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_warn_for_unused_params</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">raw_documents</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">fit_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">raw_documents</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Learn the vocabulary dictionary and return document-term matrix.</span>

<span class="sd">        This is equivalent to fit followed by transform, but more efficiently</span>
<span class="sd">        implemented.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        raw_documents : iterable</span>
<span class="sd">            An iterable which yields either str, unicode or file objects.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        X : array of shape (n_samples, n_features)</span>
<span class="sd">            Document-term matrix.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># We intentionally don&#39;t call the transform method to make</span>
        <span class="c1"># fit_transform overridable without unwanted side effects in</span>
        <span class="c1"># TfidfVectorizer.</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">raw_documents</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Iterable over raw text documents expected, &quot;</span>
                <span class="s2">&quot;string object received.&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_validate_params</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_validate_vocabulary</span><span class="p">()</span>
        <span class="n">max_df</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_df</span>
        <span class="n">min_df</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_df</span>
        <span class="n">max_features</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_features</span>

        <span class="n">vocabulary</span><span class="p">,</span> <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_count_vocab</span><span class="p">(</span><span class="n">raw_documents</span><span class="p">,</span>
                                          <span class="bp">self</span><span class="o">.</span><span class="n">fixed_vocabulary_</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">binary</span><span class="p">:</span>
            <span class="n">X</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">fill</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">fixed_vocabulary_</span><span class="p">:</span>
            <span class="n">n_doc</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">max_doc_count</span> <span class="o">=</span> <span class="p">(</span><span class="n">max_df</span>
                             <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">max_df</span><span class="p">,</span> <span class="n">numbers</span><span class="o">.</span><span class="n">Integral</span><span class="p">)</span>
                             <span class="k">else</span> <span class="n">max_df</span> <span class="o">*</span> <span class="n">n_doc</span><span class="p">)</span>
            <span class="n">min_doc_count</span> <span class="o">=</span> <span class="p">(</span><span class="n">min_df</span>
                             <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">min_df</span><span class="p">,</span> <span class="n">numbers</span><span class="o">.</span><span class="n">Integral</span><span class="p">)</span>
                             <span class="k">else</span> <span class="n">min_df</span> <span class="o">*</span> <span class="n">n_doc</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">max_doc_count</span> <span class="o">&lt;</span> <span class="n">min_doc_count</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;max_df corresponds to &lt; documents than min_df&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">max_features</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sort_features</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">vocabulary</span><span class="p">)</span>
            <span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stop_words_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_limit_features</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">vocabulary</span><span class="p">,</span>
                                                       <span class="n">max_doc_count</span><span class="p">,</span>
                                                       <span class="n">min_doc_count</span><span class="p">,</span>
                                                       <span class="n">max_features</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">max_features</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sort_features</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">vocabulary</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">vocabulary_</span> <span class="o">=</span> <span class="n">vocabulary</span>

        <span class="k">return</span> <span class="n">X</span>

    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">raw_documents</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Transform documents to document-term matrix.</span>

<span class="sd">        Extract token counts out of raw text documents using the vocabulary</span>
<span class="sd">        fitted with fit or the one provided to the constructor.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        raw_documents : iterable</span>
<span class="sd">            An iterable which yields either str, unicode or file objects.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        X : sparse matrix of shape (n_samples, n_features)</span>
<span class="sd">            Document-term matrix.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">raw_documents</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Iterable over raw text documents expected, &quot;</span>
                <span class="s2">&quot;string object received.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_vocabulary</span><span class="p">()</span>

        <span class="c1"># use the same matrix-building strategy as fit_transform</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_count_vocab</span><span class="p">(</span><span class="n">raw_documents</span><span class="p">,</span> <span class="n">fixed_vocab</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">binary</span><span class="p">:</span>
            <span class="n">X</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">fill</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">X</span>

    <span class="k">def</span> <span class="nf">inverse_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Return terms per document with nonzero entries in X.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            Document-term matrix.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        X_inv : list of arrays of shape (n_samples,)</span>
<span class="sd">            List of arrays of terms.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_vocabulary</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">sp</span><span class="o">.</span><span class="n">issparse</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
            <span class="c1"># We need CSR format for fast row manipulations.</span>
            <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">tocsr</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># We need to convert X to a matrix, so that the indexing</span>
            <span class="c1"># returns 2D objects</span>
            <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asmatrix</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">n_samples</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="n">terms</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocabulary_</span><span class="o">.</span><span class="n">keys</span><span class="p">()))</span>
        <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocabulary_</span><span class="o">.</span><span class="n">values</span><span class="p">()))</span>
        <span class="n">inverse_vocabulary</span> <span class="o">=</span> <span class="n">terms</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">indices</span><span class="p">)]</span>

        <span class="k">return</span> <span class="p">[</span><span class="n">inverse_vocabulary</span><span class="p">[</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">nonzero</span><span class="p">()[</span><span class="mi">1</span><span class="p">]]</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)]</span>

    <span class="k">def</span> <span class="nf">get_feature_names</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Array mapping from feature integer indices to feature name.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        feature_names : list</span>
<span class="sd">            A list of feature names.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_check_vocabulary</span><span class="p">()</span>

        <span class="k">return</span> <span class="p">[</span><span class="n">t</span> <span class="k">for</span> <span class="n">t</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocabulary_</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span>
                                     <span class="n">key</span><span class="o">=</span><span class="n">itemgetter</span><span class="p">(</span><span class="mi">1</span><span class="p">))]</span>

    <span class="k">def</span> <span class="nf">_more_tags</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;X_types&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;string&#39;</span><span class="p">]}</span>


<span class="k">def</span> <span class="nf">_make_int_array</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot;Construct an array.array of a type suitable for scipy.sparse indices.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">array</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="s2">&quot;i&quot;</span><span class="p">))</span>


<span class="k">class</span> <span class="nc">TfidfTransformer</span><span class="p">(</span><span class="n">TransformerMixin</span><span class="p">,</span> <span class="n">BaseEstimator</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Transform a count matrix to a normalized tf or tf-idf representation</span>

<span class="sd">    Tf means term-frequency while tf-idf means term-frequency times inverse</span>
<span class="sd">    document-frequency. This is a common term weighting scheme in information</span>
<span class="sd">    retrieval, that has also found good use in document classification.</span>

<span class="sd">    The goal of using tf-idf instead of the raw frequencies of occurrence of a</span>
<span class="sd">    token in a given document is to scale down the impact of tokens that occur</span>
<span class="sd">    very frequently in a given corpus and that are hence empirically less</span>
<span class="sd">    informative than features that occur in a small fraction of the training</span>
<span class="sd">    corpus.</span>

<span class="sd">    The formula that is used to compute the tf-idf for a term t of a document d</span>
<span class="sd">    in a document set is tf-idf(t, d) = tf(t, d) * idf(t), and the idf is</span>
<span class="sd">    computed as idf(t) = log [ n / df(t) ] + 1 (if ``smooth_idf=False``), where</span>
<span class="sd">    n is the total number of documents in the document set and df(t) is the</span>
<span class="sd">    document frequency of t; the document frequency is the number of documents</span>
<span class="sd">    in the document set that contain the term t. The effect of adding &quot;1&quot; to</span>
<span class="sd">    the idf in the equation above is that terms with zero idf, i.e., terms</span>
<span class="sd">    that occur in all documents in a training set, will not be entirely</span>
<span class="sd">    ignored.</span>
<span class="sd">    (Note that the idf formula above differs from the standard textbook</span>
<span class="sd">    notation that defines the idf as</span>
<span class="sd">    idf(t) = log [ n / (df(t) + 1) ]).</span>

<span class="sd">    If ``smooth_idf=True`` (the default), the constant &quot;1&quot; is added to the</span>
<span class="sd">    numerator and denominator of the idf as if an extra document was seen</span>
<span class="sd">    containing every term in the collection exactly once, which prevents</span>
<span class="sd">    zero divisions: idf(t) = log [ (1 + n) / (1 + df(t)) ] + 1.</span>

<span class="sd">    Furthermore, the formulas used to compute tf and idf depend</span>
<span class="sd">    on parameter settings that correspond to the SMART notation used in IR</span>
<span class="sd">    as follows:</span>

<span class="sd">    Tf is &quot;n&quot; (natural) by default, &quot;l&quot; (logarithmic) when</span>
<span class="sd">    ``sublinear_tf=True``.</span>
<span class="sd">    Idf is &quot;t&quot; when use_idf is given, &quot;n&quot; (none) otherwise.</span>
<span class="sd">    Normalization is &quot;c&quot; (cosine) when ``norm=&#39;l2&#39;``, &quot;n&quot; (none)</span>
<span class="sd">    when ``norm=None``.</span>

<span class="sd">    Read more in the :ref:`User Guide &lt;text_feature_extraction&gt;`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    norm : {&#39;l1&#39;, &#39;l2&#39;}, default=&#39;l2&#39;</span>
<span class="sd">        Each output row will have unit norm, either:</span>
<span class="sd">        * &#39;l2&#39;: Sum of squares of vector elements is 1. The cosine</span>
<span class="sd">        similarity between two vectors is their dot product when l2 norm has</span>
<span class="sd">        been applied.</span>
<span class="sd">        * &#39;l1&#39;: Sum of absolute values of vector elements is 1.</span>
<span class="sd">        See :func:`preprocessing.normalize`</span>

<span class="sd">    use_idf : bool, default=True</span>
<span class="sd">        Enable inverse-document-frequency reweighting.</span>

<span class="sd">    smooth_idf : bool, default=True</span>
<span class="sd">        Smooth idf weights by adding one to document frequencies, as if an</span>
<span class="sd">        extra document was seen containing every term in the collection</span>
<span class="sd">        exactly once. Prevents zero divisions.</span>

<span class="sd">    sublinear_tf : bool, default=False</span>
<span class="sd">        Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    idf_ : array of shape (n_features)</span>
<span class="sd">        The inverse document frequency (IDF) vector; only defined</span>
<span class="sd">        if  ``use_idf`` is True.</span>

<span class="sd">        .. versionadded:: 0.20</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.feature_extraction.text import TfidfTransformer</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.feature_extraction.text import CountVectorizer</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.pipeline import Pipeline</span>
<span class="sd">    &gt;&gt;&gt; import numpy as np</span>
<span class="sd">    &gt;&gt;&gt; corpus = [&#39;this is the first document&#39;,</span>
<span class="sd">    ...           &#39;this document is the second document&#39;,</span>
<span class="sd">    ...           &#39;and this is the third one&#39;,</span>
<span class="sd">    ...           &#39;is this the first document&#39;]</span>
<span class="sd">    &gt;&gt;&gt; vocabulary = [&#39;this&#39;, &#39;document&#39;, &#39;first&#39;, &#39;is&#39;, &#39;second&#39;, &#39;the&#39;,</span>
<span class="sd">    ...               &#39;and&#39;, &#39;one&#39;]</span>
<span class="sd">    &gt;&gt;&gt; pipe = Pipeline([(&#39;count&#39;, CountVectorizer(vocabulary=vocabulary)),</span>
<span class="sd">    ...                  (&#39;tfid&#39;, TfidfTransformer())]).fit(corpus)</span>
<span class="sd">    &gt;&gt;&gt; pipe[&#39;count&#39;].transform(corpus).toarray()</span>
<span class="sd">    array([[1, 1, 1, 1, 0, 1, 0, 0],</span>
<span class="sd">           [1, 2, 0, 1, 1, 1, 0, 0],</span>
<span class="sd">           [1, 0, 0, 1, 0, 1, 1, 1],</span>
<span class="sd">           [1, 1, 1, 1, 0, 1, 0, 0]])</span>
<span class="sd">    &gt;&gt;&gt; pipe[&#39;tfid&#39;].idf_</span>
<span class="sd">    array([1.        , 1.22314355, 1.51082562, 1.        , 1.91629073,</span>
<span class="sd">           1.        , 1.91629073, 1.91629073])</span>
<span class="sd">    &gt;&gt;&gt; pipe.transform(corpus).shape</span>
<span class="sd">    (4, 8)</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>

<span class="sd">    .. [Yates2011] R. Baeza-Yates and B. Ribeiro-Neto (2011). Modern</span>
<span class="sd">                   Information Retrieval. Addison Wesley, pp. 68-74.</span>

<span class="sd">    .. [MRS2008] C.D. Manning, P. Raghavan and H. Sch√ºtze  (2008).</span>
<span class="sd">                   Introduction to Information Retrieval. Cambridge University</span>
<span class="sd">                   Press, pp. 118-120.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@_deprecate_positional_args</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="n">use_idf</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">smooth_idf</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">sublinear_tf</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">norm</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_idf</span> <span class="o">=</span> <span class="n">use_idf</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">smooth_idf</span> <span class="o">=</span> <span class="n">smooth_idf</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sublinear_tf</span> <span class="o">=</span> <span class="n">sublinear_tf</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Learn the idf vector (global term weights).</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : sparse matrix of shape n_samples, n_features)</span>
<span class="sd">            A matrix of term/token counts.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">accept_sparse</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;csr&#39;</span><span class="p">,</span> <span class="s1">&#39;csc&#39;</span><span class="p">))</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">sp</span><span class="o">.</span><span class="n">issparse</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
            <span class="n">X</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">csr_matrix</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">dtype</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">dtype</span> <span class="k">if</span> <span class="n">X</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">in</span> <span class="n">FLOAT_DTYPES</span> <span class="k">else</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_idf</span><span class="p">:</span>
            <span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
            <span class="n">df</span> <span class="o">=</span> <span class="n">_document_frequency</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
            <span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">dtype</span><span class="p">,</span> <span class="o">**</span><span class="n">_astype_copy_false</span><span class="p">(</span><span class="n">df</span><span class="p">))</span>

            <span class="c1"># perform idf smoothing if required</span>
            <span class="n">df</span> <span class="o">+=</span> <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">smooth_idf</span><span class="p">)</span>
            <span class="n">n_samples</span> <span class="o">+=</span> <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">smooth_idf</span><span class="p">)</span>

            <span class="c1"># log+1 instead of log makes sure terms with zero idf don&#39;t get</span>
            <span class="c1"># suppressed entirely.</span>
            <span class="n">idf</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">n_samples</span> <span class="o">/</span> <span class="n">df</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_idf_diag</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">diags</span><span class="p">(</span><span class="n">idf</span><span class="p">,</span> <span class="n">offsets</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                                      <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">n_features</span><span class="p">,</span> <span class="n">n_features</span><span class="p">),</span>
                                      <span class="nb">format</span><span class="o">=</span><span class="s1">&#39;csr&#39;</span><span class="p">,</span>
                                      <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Transform a count matrix to a tf or tf-idf representation</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : sparse matrix of (n_samples, n_features)</span>
<span class="sd">            a matrix of term/token counts</span>

<span class="sd">        copy : bool, default=True</span>
<span class="sd">            Whether to copy X and operate on the copy or perform in-place</span>
<span class="sd">            operations.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        vectors : sparse matrix of shape (n_samples, n_features)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">accept_sparse</span><span class="o">=</span><span class="s1">&#39;csr&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">FLOAT_DTYPES</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="n">copy</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">sp</span><span class="o">.</span><span class="n">issparse</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
            <span class="n">X</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">csr_matrix</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>

        <span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">sublinear_tf</span><span class="p">:</span>
            <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
            <span class="n">X</span><span class="o">.</span><span class="n">data</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_idf</span><span class="p">:</span>
            <span class="c1"># idf_ being a property, the automatic attributes detection</span>
            <span class="c1"># does not work as usual and we need to specify the attribute</span>
            <span class="c1"># name:</span>
            <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">attributes</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;idf_&quot;</span><span class="p">],</span>
                            <span class="n">msg</span><span class="o">=</span><span class="s1">&#39;idf vector is not fitted&#39;</span><span class="p">)</span>

            <span class="n">expected_n_features</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_idf_diag</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">n_features</span> <span class="o">!=</span> <span class="n">expected_n_features</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Input has n_features=</span><span class="si">%d</span><span class="s2"> while the model&quot;</span>
                                 <span class="s2">&quot; has been trained with n_features=</span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span>
                                     <span class="n">n_features</span><span class="p">,</span> <span class="n">expected_n_features</span><span class="p">))</span>
            <span class="c1"># *= doesn&#39;t work</span>
            <span class="n">X</span> <span class="o">=</span> <span class="n">X</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_idf_diag</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">:</span>
            <span class="n">X</span> <span class="o">=</span> <span class="n">normalize</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">X</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">idf_</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># if _idf_diag is not set, this will raise an attribute error,</span>
        <span class="c1"># which means hasattr(self, &quot;idf_&quot;) is False</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">ravel</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_idf_diag</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>

    <span class="nd">@idf_</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">idf_</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
        <span class="n">n_features</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_idf_diag</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">spdiags</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">diags</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">m</span><span class="o">=</span><span class="n">n_features</span><span class="p">,</span>
                                    <span class="n">n</span><span class="o">=</span><span class="n">n_features</span><span class="p">,</span> <span class="nb">format</span><span class="o">=</span><span class="s1">&#39;csr&#39;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_more_tags</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;X_types&#39;</span><span class="p">:</span> <span class="s1">&#39;sparse&#39;</span><span class="p">}</span>


<span class="k">class</span> <span class="nc">TfidfVectorizer</span><span class="p">(</span><span class="n">CountVectorizer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Convert a collection of raw documents to a matrix of TF-IDF features.</span>

<span class="sd">    Equivalent to :class:`CountVectorizer` followed by</span>
<span class="sd">    :class:`TfidfTransformer`.</span>

<span class="sd">    Read more in the :ref:`User Guide &lt;text_feature_extraction&gt;`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    input : {&#39;filename&#39;, &#39;file&#39;, &#39;content&#39;}, default=&#39;content&#39;</span>
<span class="sd">        If &#39;filename&#39;, the sequence passed as an argument to fit is</span>
<span class="sd">        expected to be a list of filenames that need reading to fetch</span>
<span class="sd">        the raw content to analyze.</span>

<span class="sd">        If &#39;file&#39;, the sequence items must have a &#39;read&#39; method (file-like</span>
<span class="sd">        object) that is called to fetch the bytes in memory.</span>

<span class="sd">        Otherwise the input is expected to be a sequence of items that</span>
<span class="sd">        can be of type string or byte.</span>

<span class="sd">    encoding : str, default=&#39;utf-8&#39;</span>
<span class="sd">        If bytes or files are given to analyze, this encoding is used to</span>
<span class="sd">        decode.</span>

<span class="sd">    decode_error : {&#39;strict&#39;, &#39;ignore&#39;, &#39;replace&#39;}, default=&#39;strict&#39;</span>
<span class="sd">        Instruction on what to do if a byte sequence is given to analyze that</span>
<span class="sd">        contains characters not of the given `encoding`. By default, it is</span>
<span class="sd">        &#39;strict&#39;, meaning that a UnicodeDecodeError will be raised. Other</span>
<span class="sd">        values are &#39;ignore&#39; and &#39;replace&#39;.</span>

<span class="sd">    strip_accents : {&#39;ascii&#39;, &#39;unicode&#39;}, default=None</span>
<span class="sd">        Remove accents and perform other character normalization</span>
<span class="sd">        during the preprocessing step.</span>
<span class="sd">        &#39;ascii&#39; is a fast method that only works on characters that have</span>
<span class="sd">        an direct ASCII mapping.</span>
<span class="sd">        &#39;unicode&#39; is a slightly slower method that works on any characters.</span>
<span class="sd">        None (default) does nothing.</span>

<span class="sd">        Both &#39;ascii&#39; and &#39;unicode&#39; use NFKD normalization from</span>
<span class="sd">        :func:`unicodedata.normalize`.</span>

<span class="sd">    lowercase : bool, default=True</span>
<span class="sd">        Convert all characters to lowercase before tokenizing.</span>

<span class="sd">    preprocessor : callable, default=None</span>
<span class="sd">        Override the preprocessing (string transformation) stage while</span>
<span class="sd">        preserving the tokenizing and n-grams generation steps.</span>
<span class="sd">        Only applies if ``analyzer is not callable``.</span>

<span class="sd">    tokenizer : callable, default=None</span>
<span class="sd">        Override the string tokenization step while preserving the</span>
<span class="sd">        preprocessing and n-grams generation steps.</span>
<span class="sd">        Only applies if ``analyzer == &#39;word&#39;``.</span>

<span class="sd">    analyzer : {&#39;word&#39;, &#39;char&#39;, &#39;char_wb&#39;} or callable, default=&#39;word&#39;</span>
<span class="sd">        Whether the feature should be made of word or character n-grams.</span>
<span class="sd">        Option &#39;char_wb&#39; creates character n-grams only from text inside</span>
<span class="sd">        word boundaries; n-grams at the edges of words are padded with space.</span>

<span class="sd">        If a callable is passed it is used to extract the sequence of features</span>
<span class="sd">        out of the raw, unprocessed input.</span>

<span class="sd">        .. versionchanged:: 0.21</span>

<span class="sd">        Since v0.21, if ``input`` is ``filename`` or ``file``, the data is</span>
<span class="sd">        first read from the file and then passed to the given callable</span>
<span class="sd">        analyzer.</span>

<span class="sd">    stop_words : {&#39;english&#39;}, list, default=None</span>
<span class="sd">        If a string, it is passed to _check_stop_list and the appropriate stop</span>
<span class="sd">        list is returned. &#39;english&#39; is currently the only supported string</span>
<span class="sd">        value.</span>
<span class="sd">        There are several known issues with &#39;english&#39; and you should</span>
<span class="sd">        consider an alternative (see :ref:`stop_words`).</span>

<span class="sd">        If a list, that list is assumed to contain stop words, all of which</span>
<span class="sd">        will be removed from the resulting tokens.</span>
<span class="sd">        Only applies if ``analyzer == &#39;word&#39;``.</span>

<span class="sd">        If None, no stop words will be used. max_df can be set to a value</span>
<span class="sd">        in the range [0.7, 1.0) to automatically detect and filter stop</span>
<span class="sd">        words based on intra corpus document frequency of terms.</span>

<span class="sd">    token_pattern : str</span>
<span class="sd">        Regular expression denoting what constitutes a &quot;token&quot;, only used</span>
<span class="sd">        if ``analyzer == &#39;word&#39;``. The default regexp selects tokens of 2</span>
<span class="sd">        or more alphanumeric characters (punctuation is completely ignored</span>
<span class="sd">        and always treated as a token separator).</span>

<span class="sd">    ngram_range : tuple (min_n, max_n), default=(1, 1)</span>
<span class="sd">        The lower and upper boundary of the range of n-values for different</span>
<span class="sd">        n-grams to be extracted. All values of n such that min_n &lt;= n &lt;= max_n</span>
<span class="sd">        will be used. For example an ``ngram_range`` of ``(1, 1)`` means only</span>
<span class="sd">        unigrams, ``(1, 2)`` means unigrams and bigrams, and ``(2, 2)`` means</span>
<span class="sd">        only bigrams.</span>
<span class="sd">        Only applies if ``analyzer is not callable``.</span>

<span class="sd">    max_df : float or int, default=1.0</span>
<span class="sd">        When building the vocabulary ignore terms that have a document</span>
<span class="sd">        frequency strictly higher than the given threshold (corpus-specific</span>
<span class="sd">        stop words).</span>
<span class="sd">        If float in range [0.0, 1.0], the parameter represents a proportion of</span>
<span class="sd">        documents, integer absolute counts.</span>
<span class="sd">        This parameter is ignored if vocabulary is not None.</span>

<span class="sd">    min_df : float or int, default=1</span>
<span class="sd">        When building the vocabulary ignore terms that have a document</span>
<span class="sd">        frequency strictly lower than the given threshold. This value is also</span>
<span class="sd">        called cut-off in the literature.</span>
<span class="sd">        If float in range of [0.0, 1.0], the parameter represents a proportion</span>
<span class="sd">        of documents, integer absolute counts.</span>
<span class="sd">        This parameter is ignored if vocabulary is not None.</span>

<span class="sd">    max_features : int, default=None</span>
<span class="sd">        If not None, build a vocabulary that only consider the top</span>
<span class="sd">        max_features ordered by term frequency across the corpus.</span>

<span class="sd">        This parameter is ignored if vocabulary is not None.</span>

<span class="sd">    vocabulary : Mapping or iterable, default=None</span>
<span class="sd">        Either a Mapping (e.g., a dict) where keys are terms and values are</span>
<span class="sd">        indices in the feature matrix, or an iterable over terms. If not</span>
<span class="sd">        given, a vocabulary is determined from the input documents.</span>

<span class="sd">    binary : bool, default=False</span>
<span class="sd">        If True, all non-zero term counts are set to 1. This does not mean</span>
<span class="sd">        outputs will have only 0/1 values, only that the tf term in tf-idf</span>
<span class="sd">        is binary. (Set idf and normalization to False to get 0/1 outputs).</span>

<span class="sd">    dtype : dtype, default=float64</span>
<span class="sd">        Type of the matrix returned by fit_transform() or transform().</span>

<span class="sd">    norm : {&#39;l1&#39;, &#39;l2&#39;}, default=&#39;l2&#39;</span>
<span class="sd">        Each output row will have unit norm, either:</span>
<span class="sd">        * &#39;l2&#39;: Sum of squares of vector elements is 1. The cosine</span>
<span class="sd">        similarity between two vectors is their dot product when l2 norm has</span>
<span class="sd">        been applied.</span>
<span class="sd">        * &#39;l1&#39;: Sum of absolute values of vector elements is 1.</span>
<span class="sd">        See :func:`preprocessing.normalize`.</span>

<span class="sd">    use_idf : bool, default=True</span>
<span class="sd">        Enable inverse-document-frequency reweighting.</span>

<span class="sd">    smooth_idf : bool, default=True</span>
<span class="sd">        Smooth idf weights by adding one to document frequencies, as if an</span>
<span class="sd">        extra document was seen containing every term in the collection</span>
<span class="sd">        exactly once. Prevents zero divisions.</span>

<span class="sd">    sublinear_tf : bool, default=False</span>
<span class="sd">        Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    vocabulary_ : dict</span>
<span class="sd">        A mapping of terms to feature indices.</span>

<span class="sd">    fixed_vocabulary_: bool</span>
<span class="sd">        True if a fixed vocabulary of term to indices mapping</span>
<span class="sd">        is provided by the user</span>

<span class="sd">    idf_ : array of shape (n_features,)</span>
<span class="sd">        The inverse document frequency (IDF) vector; only defined</span>
<span class="sd">        if ``use_idf`` is True.</span>

<span class="sd">    stop_words_ : set</span>
<span class="sd">        Terms that were ignored because they either:</span>

<span class="sd">          - occurred in too many documents (`max_df`)</span>
<span class="sd">          - occurred in too few documents (`min_df`)</span>
<span class="sd">          - were cut off by feature selection (`max_features`).</span>

<span class="sd">        This is only available if no vocabulary was given.</span>

<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    CountVectorizer : Transforms text into a sparse matrix of n-gram counts.</span>

<span class="sd">    TfidfTransformer : Performs the TF-IDF transformation from a provided</span>
<span class="sd">        matrix of counts.</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    The ``stop_words_`` attribute can get large and increase the model size</span>
<span class="sd">    when pickling. This attribute is provided only for introspection and can</span>
<span class="sd">    be safely removed using delattr or set to None before pickling.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.feature_extraction.text import TfidfVectorizer</span>
<span class="sd">    &gt;&gt;&gt; corpus = [</span>
<span class="sd">    ...     &#39;This is the first document.&#39;,</span>
<span class="sd">    ...     &#39;This document is the second document.&#39;,</span>
<span class="sd">    ...     &#39;And this is the third one.&#39;,</span>
<span class="sd">    ...     &#39;Is this the first document?&#39;,</span>
<span class="sd">    ... ]</span>
<span class="sd">    &gt;&gt;&gt; vectorizer = TfidfVectorizer()</span>
<span class="sd">    &gt;&gt;&gt; X = vectorizer.fit_transform(corpus)</span>
<span class="sd">    &gt;&gt;&gt; print(vectorizer.get_feature_names())</span>
<span class="sd">    [&#39;and&#39;, &#39;document&#39;, &#39;first&#39;, &#39;is&#39;, &#39;one&#39;, &#39;second&#39;, &#39;the&#39;, &#39;third&#39;, &#39;this&#39;]</span>
<span class="sd">    &gt;&gt;&gt; print(X.shape)</span>
<span class="sd">    (4, 9)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@_deprecate_positional_args</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="s1">&#39;content&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">,</span>
                 <span class="n">decode_error</span><span class="o">=</span><span class="s1">&#39;strict&#39;</span><span class="p">,</span> <span class="n">strip_accents</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">lowercase</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">preprocessor</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">analyzer</span><span class="o">=</span><span class="s1">&#39;word&#39;</span><span class="p">,</span>
                 <span class="n">stop_words</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">token_pattern</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;(?u)\b\w\w+\b&quot;</span><span class="p">,</span>
                 <span class="n">ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">max_df</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">min_df</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">max_features</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">vocabulary</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">binary</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="n">use_idf</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">smooth_idf</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">sublinear_tf</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="nb">input</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="n">encoding</span><span class="p">,</span> <span class="n">decode_error</span><span class="o">=</span><span class="n">decode_error</span><span class="p">,</span>
            <span class="n">strip_accents</span><span class="o">=</span><span class="n">strip_accents</span><span class="p">,</span> <span class="n">lowercase</span><span class="o">=</span><span class="n">lowercase</span><span class="p">,</span>
            <span class="n">preprocessor</span><span class="o">=</span><span class="n">preprocessor</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">analyzer</span><span class="o">=</span><span class="n">analyzer</span><span class="p">,</span>
            <span class="n">stop_words</span><span class="o">=</span><span class="n">stop_words</span><span class="p">,</span> <span class="n">token_pattern</span><span class="o">=</span><span class="n">token_pattern</span><span class="p">,</span>
            <span class="n">ngram_range</span><span class="o">=</span><span class="n">ngram_range</span><span class="p">,</span> <span class="n">max_df</span><span class="o">=</span><span class="n">max_df</span><span class="p">,</span> <span class="n">min_df</span><span class="o">=</span><span class="n">min_df</span><span class="p">,</span>
            <span class="n">max_features</span><span class="o">=</span><span class="n">max_features</span><span class="p">,</span> <span class="n">vocabulary</span><span class="o">=</span><span class="n">vocabulary</span><span class="p">,</span> <span class="n">binary</span><span class="o">=</span><span class="n">binary</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_tfidf</span> <span class="o">=</span> <span class="n">TfidfTransformer</span><span class="p">(</span><span class="n">norm</span><span class="o">=</span><span class="n">norm</span><span class="p">,</span> <span class="n">use_idf</span><span class="o">=</span><span class="n">use_idf</span><span class="p">,</span>
                                       <span class="n">smooth_idf</span><span class="o">=</span><span class="n">smooth_idf</span><span class="p">,</span>
                                       <span class="n">sublinear_tf</span><span class="o">=</span><span class="n">sublinear_tf</span><span class="p">)</span>

    <span class="c1"># Broadcast the TF-IDF parameters to the underlying transformer instance</span>
    <span class="c1"># for easy grid search and repr</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">norm</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tfidf</span><span class="o">.</span><span class="n">norm</span>

    <span class="nd">@norm</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">norm</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_tfidf</span><span class="o">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">value</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">use_idf</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tfidf</span><span class="o">.</span><span class="n">use_idf</span>

    <span class="nd">@use_idf</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">use_idf</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_tfidf</span><span class="o">.</span><span class="n">use_idf</span> <span class="o">=</span> <span class="n">value</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">smooth_idf</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tfidf</span><span class="o">.</span><span class="n">smooth_idf</span>

    <span class="nd">@smooth_idf</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">smooth_idf</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_tfidf</span><span class="o">.</span><span class="n">smooth_idf</span> <span class="o">=</span> <span class="n">value</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">sublinear_tf</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tfidf</span><span class="o">.</span><span class="n">sublinear_tf</span>

    <span class="nd">@sublinear_tf</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">sublinear_tf</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_tfidf</span><span class="o">.</span><span class="n">sublinear_tf</span> <span class="o">=</span> <span class="n">value</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">idf_</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tfidf</span><span class="o">.</span><span class="n">idf_</span>

    <span class="nd">@idf_</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">idf_</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_validate_vocabulary</span><span class="p">()</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;vocabulary_&#39;</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocabulary_</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">value</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;idf length = </span><span class="si">%d</span><span class="s2"> must be equal &quot;</span>
                                 <span class="s2">&quot;to vocabulary size = </span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span>
                                 <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">value</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocabulary</span><span class="p">)))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_tfidf</span><span class="o">.</span><span class="n">idf_</span> <span class="o">=</span> <span class="n">value</span>

    <span class="k">def</span> <span class="nf">_check_params</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">FLOAT_DTYPES</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Only </span><span class="si">{}</span><span class="s2"> &#39;dtype&#39; should be used. </span><span class="si">{}</span><span class="s2"> &#39;dtype&#39; will &quot;</span>
                          <span class="s2">&quot;be converted to np.float64.&quot;</span>
                          <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">FLOAT_DTYPES</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span>
                          <span class="ne">UserWarning</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">raw_documents</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Learn vocabulary and idf from training set.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        raw_documents : iterable</span>
<span class="sd">            An iterable which yields either str, unicode or file objects.</span>
<span class="sd">        y : None</span>
<span class="sd">            This parameter is not needed to compute tfidf.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        self : object</span>
<span class="sd">            Fitted vectorizer.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_params</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_warn_for_unused_params</span><span class="p">()</span>
        <span class="n">X</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">raw_documents</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_tfidf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">fit_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">raw_documents</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Learn vocabulary and idf, return document-term matrix.</span>

<span class="sd">        This is equivalent to fit followed by transform, but more efficiently</span>
<span class="sd">        implemented.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        raw_documents : iterable</span>
<span class="sd">            An iterable which yields either str, unicode or file objects.</span>
<span class="sd">        y : None</span>
<span class="sd">            This parameter is ignored.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        X : sparse matrix of (n_samples, n_features)</span>
<span class="sd">            Tf-idf-weighted document-term matrix.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_params</span><span class="p">()</span>
        <span class="n">X</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">raw_documents</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_tfidf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="c1"># X is already a transformed view of raw_documents so</span>
        <span class="c1"># we set copy to False</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tfidf</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">raw_documents</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="s2">&quot;deprecated&quot;</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Transform documents to document-term matrix.</span>

<span class="sd">        Uses the vocabulary and document frequencies (df) learned by fit (or</span>
<span class="sd">        fit_transform).</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        raw_documents : iterable</span>
<span class="sd">            An iterable which yields either str, unicode or file objects.</span>

<span class="sd">        copy : bool, default=True</span>
<span class="sd">            Whether to copy X and operate on the copy or perform in-place</span>
<span class="sd">            operations.</span>

<span class="sd">            .. deprecated:: 0.22</span>
<span class="sd">               The `copy` parameter is unused and was deprecated in version</span>
<span class="sd">               0.22 and will be removed in 0.24. This parameter will be</span>
<span class="sd">               ignored.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        X : sparse matrix of (n_samples, n_features)</span>
<span class="sd">            Tf-idf-weighted document-term matrix.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">msg</span><span class="o">=</span><span class="s1">&#39;The TF-IDF vectorizer is not fitted&#39;</span><span class="p">)</span>

        <span class="c1"># FIXME Remove copy parameter support in 0.24</span>
        <span class="k">if</span> <span class="n">copy</span> <span class="o">!=</span> <span class="s2">&quot;deprecated&quot;</span><span class="p">:</span>
            <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;&#39;copy&#39; param is unused and has been deprecated since &quot;</span>
                   <span class="s2">&quot;version 0.22. Backward compatibility for &#39;copy&#39; will &quot;</span>
                   <span class="s2">&quot;be removed in 0.24.&quot;</span><span class="p">)</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="n">msg</span><span class="p">,</span> <span class="ne">FutureWarning</span><span class="p">)</span>
        <span class="n">X</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">raw_documents</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tfidf</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_more_tags</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;X_types&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;string&#39;</span><span class="p">],</span> <span class="s1">&#39;_skip_test&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">}</span>
</pre></div>

              </div>
              
              
              <div class='prev-next-bottom'>
                

              </div>
              
          </main>
          

      </div>
    </div>

    
  <script src="../../../_static/js/index.d3f166471bb80abb5163.js"></script>


    <footer class="footer mt-5 mt-md-0">
  <div class="container">
    <p>
          &copy; Copyright 2021, Boston Consulting Group (BCG).<br/>
        Created using <a href="http://sphinx-doc.org/">Sphinx</a> 3.4.3.<br/>
    </p>
  </div>
</footer>
  </body>
</html>